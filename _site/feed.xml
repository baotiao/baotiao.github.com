<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>baotiao</title>
    <description>做有积累的事情</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>长路漫漫, 从Blink-tree 到Bw-tree (上)</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;天不生我bw-tree, 索引万古如长夜&lt;/p&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;背景&quot;&gt;背景&lt;/h3&gt;

&lt;p&gt;在前面的文章 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/374000358&quot;&gt;路在脚下, 从BTree 到Polar Index&lt;/a&gt;中提到, 我们已经将InnoDB 里面Btree 替换成Blink Tree, 高并发压力下, 在标准的TPCC 场景中最高能够有239%的性能提升, 然后我们对InnoDB 的file space模块也进行了优化, 在分配新page 的时候, 可以允许不进行填0 操作, 从而尽可能的减少fsp-&amp;gt;mutex 的开销, 在这之后我们发现瓶颈卡在了page latch 上, 而且越是在多核的场景, page latch 的开销也越大.&lt;/p&gt;

&lt;p&gt;目前latch 冲突的主要场景&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;读写冲突场景, 典型场景对Page 进行write 操作的时候, 另外一个read 操作进行要读取page.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;写写冲突场景, 典型 autoinc,  或者update_index 对同一个page 进行更新场景.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“读读冲突”场景. 频繁的对btree root 附近level 进行s lcok 和 unlock 操作.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;前面两个场景比较常见, 为什么会有读读冲突这样的问题?&lt;/p&gt;

&lt;p&gt;现有 blink-tree + lock coupling 的实现中, 我们加锁的顺序是自上而下的, 每一次访问page 都需要lock, 然后unlock.&lt;/p&gt;

&lt;p&gt;在InnoDB 自己实现的rw_lock中(大部分的rw_lock 实现也都类似), 每一个latch 都有lock_word 用于记录持有 s lock thread 数目, 所以即使是read 操作加s lock 的场景, 也是需要修改内存里面的 rw_lock-&amp;gt;lock_word, 然后释放的时候继续对 lock_word 进行减1 操作.&lt;/p&gt;

&lt;p&gt;这样带来的问题是在multi-core 场景中, 需要频繁的修改一个share memory(rw_lock-&amp;gt;lock_word). 而对于share memory 的频繁修改会大量增加CPU cache coherence protocol.&lt;/p&gt;

&lt;p&gt;即使 read only 的场景, 对于Blink-tree 的root page 依然有大量的s lock, s unlock 操作, 从而造成瓶颈.&lt;/p&gt;

&lt;p&gt;在PolarDB 中, 这样的场景对我们的性能有多大的影响呢?&lt;/p&gt;

&lt;p&gt;我们修改了InnoDB 代码, 在sysbench read_only 的场景下, 将所有的lock 都去掉 vs 仅仅持有s lock 的场景.&lt;/p&gt;

&lt;p&gt;before 表示的是s lock 场景&lt;/p&gt;

&lt;p&gt;after 表示的是不持有Lock 场景&lt;/p&gt;

&lt;p&gt;因为是read_only 场景, 不会有任何数据写入, 所以不加锁也就不会出错&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/1657541081763-abaac431-71d3-41a8-a144-9004abb2ab8d.png&quot; alt=&quot;img&quot; style=&quot;zoom: 67%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/1657541115808-aa2781a3-792e-40d5-bbfb-5cd95ffd2ddf.png&quot; alt=&quot;img&quot; style=&quot;zoom:67%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上面两个图可以看到, 在Intel 128 core 的场景下, 不持有lock 的性能对比持有s lock 有10% 的提升, 在AMD 256 core 的场景下, 这个提升更加明显, 有20% 左右的提升.&lt;/p&gt;

&lt;p&gt;所以我们认为这里在read only的场景中, 因为cache coherence protocol 引入的开销差不多有10%~20% 之间.&lt;/p&gt;

&lt;p&gt;但是, 其实Btree 是一个非常扁平的tree, 绝大多数访问的是并不冲突的leaf page. 能否避免大家都去加锁访问root page 呢?&lt;/p&gt;

&lt;p&gt;学术界如何处理Page 冲突的问题呢?&lt;/p&gt;

&lt;p&gt;笔者主要参看&lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2020/&quot;&gt;CMU Advanced Database Systems&lt;/a&gt; 里面推荐的OLTP Indexex(B+Tree Data Structures) 和 OLTP Indexes (Trie Data Structures) 对这一块进行了解.&lt;/p&gt;

&lt;p&gt;其实早在2001 年的时候, 在论文&lt;a href=&quot;http://www.vldb.org/conf/2001/P181.pdf&quot;&gt;OLFIT&lt;/a&gt; 里面, 作者Cha 第一个提出简单的lock coupling 是无法在现代的multi-core CPU 进行scability 的. 即使在没有冲突的场景, 也会因为cache coherence protocol 的开销, 导致性能下降.&lt;/p&gt;

&lt;p&gt;在OLFIT 文章里面, 介绍了这样的场景&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/image-20220518165111054.png&quot; alt=&quot;image-20220518165111054&quot; style=&quot;zoom:40%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图中, 在Main-Memory 里面有一个包含7个node btree, 为了简化问题, 有4个core, p1, p2, p3, p4. 有各种的cache block.&lt;/p&gt;

&lt;p&gt;在初始的时刻, 所有core 里面的cacheline 都是空的, P1 访问的路径是 n1-&amp;gt;n2-&amp;gt;n4. 访问完以后, 会把n1, n2, n4 放在自己的cache block 中, 也会把对应的latch 放在自己的cache block 中. P2 访问的路径是n1-&amp;gt;n2-&amp;gt;n5. 在P2 访问n1, n2 的时候, 由于修改了n1 和 n2 的latch, 因此就必须增加了 Invalidate 在P1 里面的n1, n2 的latch 的开销, 同时如果P1 再次访问n1 的时候, 依然要重新冲main-memory 中去获得对应的latch. 并且这里可以看到c1 的cache block 即使有足够多的空间, 也依然会被Invalidate 掉的.&lt;/p&gt;

&lt;p&gt;整体而言OLFIT 这个文章描述了Btree 在Multi-core 场景下的性能问题, 给出了一个较为简单的乐观加锁解决方案OLFIT.&lt;/p&gt;

&lt;h3 id=&quot;olfitoptimistic-latch-free-index-traversal-cc&quot;&gt;OLFIT(Optimistic, Latch-Free Index Traversal CC)&lt;/h3&gt;

&lt;p&gt;OLFIT 实现读操作不需要加锁, 只有写操作需要加锁, 但是很重要的一个前提是, write 操作的修改一定要是原子性的.&lt;/p&gt;

&lt;p&gt;OLFIT 大致流程:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Algorithm UpdateNode
Update 的算法:
U1. Acquire latch.
U2. Update the content. 
U3. Increment version. 
U4. Release latch.

Algorithm ReadNode
Read 的算法:
R1. Copy the value of version into a register R. 
R2. Read the content of the node.
R3. If latch is locked, go to R1.
R4. If the current value of version is different fromthe copied value in R, go to R1.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Update 的算法:&lt;/p&gt;

&lt;p&gt;操作的时候, 修改之前对page 进行lock, 然后修改完成以后增加版本号, 最后放锁.&lt;/p&gt;

&lt;p&gt;Read 的算法:&lt;/p&gt;

&lt;p&gt;整体Read 过程是不需要加锁的.&lt;/p&gt;

&lt;p&gt;R1 操作将page 内容拷贝到本地, R2 读取内容.&lt;/p&gt;

&lt;p&gt;由于OLFIT不对page 进行lock 操作, Read 进行R2 操作的时候, 有可能有新的Update 操作进来, 进行U2 操作, 也就是修改了page 里面的内容. 因此OLFIT需要在读操作完成以后, 通过R3/R4 操作, 判断Page 版本号是否被修改以及Page 是否被持有lock 来确认读取过程中该Page 是否被修改了, 如果被修改那么就需要发起重试操作了.&lt;/p&gt;

&lt;p&gt;但是, 在工程上, 虽然最后R3/R4 会保证如果U2 进行了修改, Read 操作会进行重试, 但是如果这个U2 update非atomic, 会导致R2 读取出来的内容是corrupt 的. 所以在工程上必须保证U2 操作还应该是atomic 的. 但是目前看来在工程上是无法实现的.&lt;/p&gt;

&lt;p&gt;比如现有 InnoDB 的 update the content 就是非atomic 的, 因为需要修改的是16kb 大小page 的内容. 那么读取操作有可能读取到了corrupt page, 那么这里就无法判断page corrupt 是由于真的page 出错导致corrupt, 还是由于读取非atomic 导致的corrupt. 当然可以通过重试来读取Page 来进一步验证, 但还是无法准确确定Page 的正确性.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;所以这么看来在实际的工程实现中OLFIT 似乎是不可能实现的, OLFIT 很大的一个假设前提read content 是atomic 操作.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;OLFIT 如何解决SMO 的问题?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Insert and delete operations can also initially acquire read locks before upgrading them to write locks if necessary.&lt;/p&gt;

&lt;p&gt;并且由于read 操作可以不进行加s lock操作, 那么在btree 中进行x lock 加锁操作的时候自然可以自下而上了. 因为不存在之前search 自上而下的Lock, 而modify自下而上的Lock 导致的死锁问题了.&lt;/p&gt;

&lt;p&gt;但是由于read 操作不再进行加锁操作, 那么可能在上述Algorithm ReadNode R2 操作中, Page 进行了split 操作. 即使我们能够保证Read Content of node 是原子操作, 也有可能该Page 发生了SMO 操作, 要访问的key 已经不在当前Page 了. 那么该如何处理?&lt;/p&gt;

&lt;p&gt;目前OLFIT 通过和blink-tree 类似方法, 增加high key, 并且保证所有的SMO 操作是自左向右, 那么如果访问的page 发生了SMO 操作, 那么可以查看一下当前page 是否有high key, 如果有的话, 那么说明正在发生SMO 操作, 那么如果要访问的key 超过了high key, 那么就直接去访问next node 即可.&lt;/p&gt;

&lt;p&gt;对于删除操作, 和大部分的无锁算法有点类似, Page 被删除以后无法确认没有其他的读取操作正在读取该Page, 因此目前的删除操作是将当前的Page 从当前Btree 中删除, 然后添加到garbage collector list 中. 后续有专门的garbage collector 线程进行garbage collector 操作.&lt;/p&gt;

&lt;p&gt;在OLFIT 之后, Btree 在multi-core 场景下的性能问题也越来越热门, 并且随着现代CPU 技术的发展, CPU 上的core 数会越来越多, 因此该问题会随着core 数的增长愈发严重. 因此后面又有很多文章对该问题进行了探索, 典型的代表是 Bw-Tree 和 ART Tree.&lt;/p&gt;

&lt;p&gt;我们先来看看ART Tree 怎么解决这样的问题.&lt;/p&gt;

&lt;h3 id=&quot;art-tree&quot;&gt;ART Tree&lt;/h3&gt;

&lt;p&gt;ART Tree 其实提出了两个方案&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Optimistic Lock Coupling (OLC)&lt;/li&gt;
  &lt;li&gt;Read-Optimized Write Exclusion (ROWEX)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;OLC&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;OLC 的思想就是从 OLFIT 里面出来的, 但是OLFIT 最大的问题在于page version 一旦进行了修改就需要进行重试, 但是很多时候如果page 里面只是增加了数据, 并没有发生SMO, 其实是可以不用retry 的. 所以后续的masstree, ART tree 都是类似的思路.&lt;/p&gt;

&lt;p&gt;另外一个优化是, OLC 可以在同一时刻snapshot 多个节点, 而OLFIT 可以认为同一时刻只能snapshot 1个节点的.&lt;/p&gt;

&lt;p&gt;比如下图中, 右边是OLC 的流程, OLC 的流程是持有父节点的version, 然后去遍历子节点, 此刻最多持有2个节点的version, 当然也可以继续持有父节点的version 去遍历下一个子节点, 这样持有的snapshot 就是3个, 但是这样的并发就降低了.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/image-20220814042951936.png&quot; alt=&quot;image-20220814042951936&quot; /&gt;&lt;/p&gt;

&lt;p&gt;但是这样OLC 还是有一个前提, 需要对page 的修改是atomic 的, 工程上依然是无法实现的.&lt;/p&gt;

&lt;p&gt;工程上无法实现对page 的atomic 修改本质原因是当前的修改方案是Inplace update, 如果可以做non-inplace update修改, 那么就可以做到atomic 了. bw-tree 和 ROWEX 对Page 的修改就是这样的思路, 但是还是有细微的区别. 下面会讲到.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ROWEX&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ROWEX 称为Read-Optimized Write EXclusion, 也就是Read 操作不需要加锁, 甚至不需要读取版本号, 但是写操作需要加锁.&lt;/p&gt;

&lt;p&gt;ROWEX 和RCU 非常像, 读操作不需要任何加锁, 写操作需要加锁. 由于写操作不会对老page 进行修改, 所以读操作很容易实现 atomic, 但是读操作需要保证读取到的是旧版本的数据也能够处理. 对比OLC, 读取到旧版本就要retry 的行为, 性能肯定好很多.&lt;/p&gt;

&lt;p&gt;ROWEX 改动肯定就比OLC 大很多了, 并且并不是所有的数据结构都能改成ROWEX, 因为需要保证读操作读取到老的版本程序的正确性.&lt;/p&gt;

&lt;p&gt;那么ROWEX 如何解决工程上提到的page 修改non-atomic 的问题呢?&lt;/p&gt;

&lt;p&gt;通过Node replacement 的方法, 具体 ROWEX Node replacement 的流程如下:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Both the node and its parent are locked.&lt;/li&gt;
  &lt;li&gt;A new node of the appropriate type is created and initialized by copying over all entries from the old node.&lt;/li&gt;
  &lt;li&gt;The location within the parent pointing to the old node is changed to the new node using an atomic store.&lt;/li&gt;
  &lt;li&gt;The old node is unlocked and marked as obsolete. The parent node is unlocked.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这里最大的区别是写入的时候将node 拷贝出来, 然后在新的node 上进行改动, parent node 指向 old node 和new node 的指针通过atomic cas (这里保证了修改content 是atomic) 进行切换即可. 然后old node 被标记成unlocked and obsolete.&lt;/p&gt;

&lt;p&gt;与OLC 区别在于, OLC 一个Page 只有一个版本, 因此所有的请求都访问该版本. 而ROWEX 有多个版本, 在修改new page 过程中, 老的请求继续访问old page, 等这次修改完成以后, 把new page cas 到parent node 之下,替换old page. 之后再进来的新请求就可以访问new page 了.&lt;/p&gt;

&lt;p&gt;最后等这些老的请求都结束了以后, old node 才会被真的物理删除.&lt;/p&gt;

&lt;p&gt;那么我们能直接将ROWEX 用于InnoDB 中么?&lt;/p&gt;

&lt;p&gt;也很难, 因为这里每一次修改都需要copy node, 开销还是非常大.&lt;/p&gt;

&lt;h3 id=&quot;bw-tree&quot;&gt;Bw-tree&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/img/20210321073458.png&quot; style=&quot;zoom: 40%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;bw-tree 做法整体而言是page 多版本, 多个版本通过delta-chain 连接到一起, 由于page 存在的多个版本, 每次读取操作访问的是不同版本Page, 那么自然就需要引入了mapping table. 所以可以说 delta-chain + mapping table 实现无锁化.&lt;/p&gt;

&lt;p&gt;对比ROWEX, ROWEX 将node replacement 的方法常态化, 但是对于page 中每一行的修改都使用copy node 的做法对内存的开销太大, 工程中很难采用.&lt;/p&gt;

&lt;p&gt;具体做法是将修改的内容append 到page delta-chain, 然后通过cas 的方式原子的添加到page 的deltai-chain 中, 从而老版本page 不会被修改, 那么读操作自然就可以不需要加锁. 由于读操作不需要加锁, 那么自然就不存在读写冲突. 同样由于读操作不需要加锁, 那么”读读” 操作自然也就不会有冲突.&lt;/p&gt;

&lt;p&gt;在之前的 OLC/ROWEX 实现中, 依然还是对page 进行加锁操作, 因此 write-write 还是冲突的, 而在 bw-tree 通过引入delta-chain, 改动都是追加到page delta-chain 中, 因此不同的写入操作可以也可以通过cas 的操作顺序添加到delta-chain, 从而也可以将这些操作无锁化, 所以保证write-write 操作也是无冲突的.&lt;/p&gt;

&lt;p&gt;其实可以看到这里对于避免inplace update 的做法, 只有ROWEX 和 bw-tree 这两种做法了.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;每做一次修改都copy node, 然后进行修改, 然后再进行cas 回去. =&amp;gt; ROWEX&lt;/li&gt;
  &lt;li&gt;通过delta-chain 的方式连接到之前的node, 然后再进行cas 回去. =&amp;gt; bw-tree&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;bw-tree 具体实现, 主要两个主要流程:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;正常的数据append 过程&lt;/p&gt;

    &lt;p&gt;在append page 过程, 增加delta chain 的过程, mapping table 里面page id =&amp;gt; memory 中parse buffer 地址&lt;/p&gt;

    &lt;p&gt;当比如超过16个, 进行一次consolidate 以后, 但是该page 并没有进行smo, 那么我们的做法是尽可能不去修改Parent node, 那么会纯生存new page, 该page 同样和Delta chain 连在一起. 就变成这样的结构&lt;/p&gt;

    &lt;p&gt;page id =&amp;gt; new page -&amp;gt; delta1 -&amp;gt; delta2 -&amp;gt; delta3 -&amp;gt; oldpage&lt;/p&gt;

    &lt;p&gt;这里new page = delt1 + delt2 + delta3 + oldpage 的内容.&lt;/p&gt;

    &lt;p&gt;如果进行consolidate 需要发生smo 操作, 那么就不得不修改parent node, 这个时候会给page 增加一个split deltas, 然后合适的实际进行SMO 操作&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;smo 过程&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/image-20220427201403469.png&quot; alt=&quot;image-20220427201403469&quot; style=&quot;zoom: 33%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;bw-tree 的smo 流程是这样, 整体而言和blink-tree 类似, 分成了2个阶段. 上图的 a + b 是阶段1(处理子节点), c 是阶段2(添加到父节点).&lt;/p&gt;

&lt;p&gt;图a 需要计算出seperate key Kp, 增加Page Q, 然后将Page P 上面 &amp;gt;=Kp 的keys 拷贝到Page Q, Q 指向Page R.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意: 图a 阶段就已经确定了 seperate key Kp&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;图b 需要增加split delta, 包含&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;增加seperate key Kp, 用来记录哪些record 迁移到Q上&lt;/li&gt;
  &lt;li&gt;增加logic point 指向Page Q.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;最后通过CAS 操作, 将Page P 指向split delta.&lt;/p&gt;

&lt;p&gt;阶段1 只处理了child node, 并没有把新增加的Page Q添加的Parent node O 上, 和Blink-tree 类似, 访问的时候有可能一半的内容已经迁移到Page Q 了, 但Page O 上面记录的范围还在Page P里面, (因此Page P 需要有一个信息, 这个信息可以记录在Split delta 记录中), 所以和Blink tree 类似, 先访问Page P, 知道Page P 处在SMO 状态中, 然后通过Page P 的link page 到 Page Q.&lt;/p&gt;

&lt;p&gt;和Blink-tree 类似, 阶段1 和 阶段2 之前不需要在同一个mtr 里面完成, 到阶段1 完成以后, 是一个合法的Btree. 只不过查询的时候需要额外多走一个Page.&lt;/p&gt;

&lt;p&gt;阶段2 做的事情就是把Page Q 添加到父节点Page O 中.&lt;/p&gt;

&lt;p&gt;这里Index entry delta 做3个事情&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Page O 需要增加一个seperate key, 访问从[P, R] 变成[P, Q] + [Q, R] 两个, Page Q 负责[Q, R] 的区间&lt;/li&gt;
  &lt;li&gt;Page O 需要增加一个point 指向Page Q&lt;/li&gt;
  &lt;li&gt;Page Q 和 Page R 的seperate key(其实可以复用Page P 和 Page R, 这个可以忽略)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;最后通过CAS 操作, 将Page O 指向这个Index entry delta.&lt;/p&gt;

&lt;p&gt;我们来看看这个时候有Read 操作如何保证正确性, 在图a 中时候, 只是增加了Page Q 对任何操作都是不可见的, 因此没有问题.&lt;/p&gt;

&lt;p&gt;图b 增加了Split delta, Page P + Delta 是最新版本, 这是一次cas 操作, 完成以后P -&amp;gt;next = Q. Q-&amp;gt;next = R(图a中修改), 那么此时的访问依然没有正确性问题.&lt;/p&gt;

&lt;p&gt;如果此刻有一个mtr 访问的是Old Page P(不包含Delta), 然后访问next page, 依然是正确的.&lt;/p&gt;

&lt;p&gt;图c 同样通过一个cas 操作将Page Q 添加到Page O中, 依然没有正确性问题.&lt;/p&gt;

&lt;p&gt;如果这里Page P 已经把一半的内容迁移到Page Q 了, 那么这个时候Page P 还有写入, 这个时候该如何处理?&lt;/p&gt;

&lt;p&gt;也就是SMO 和正常写入的冲突, 以及SMO 和SMO 的冲突.&lt;/p&gt;

&lt;p&gt;bw-tree 文章在 &lt;em&gt;C. Serializing Structure Modifications and Updates&lt;/em&gt; 讲的过程&lt;/p&gt;

&lt;p&gt;文章里面说的是, 对于同一个row 的修改, 通过事务锁去保证. 目前InnoDB 也是这么做的. 在lock 模块就已经冲突了, 就不可能拿到page latch.&lt;/p&gt;

&lt;p&gt;这里需要处理SMO 和 SMO 的关系, 以及SMO 和正常写入的关系.&lt;/p&gt;

&lt;p&gt;因为bw-tree 是无锁的. 所以SMO 过程中(阶段1 和 阶段2中间) 比如会穿插正常的查询和写入操作, 甚至SMO 操作.&lt;/p&gt;

&lt;p&gt;bw-tree 选择的操作是, 如果候选的查询或者写入遇到了处在smo 中间状态的节点, 那么该操作会帮助完成这次smo 操作.&lt;/p&gt;

&lt;p&gt;原因是如果Page P 正在做SMO 期间, 又有写入操作, 假设写入的内容应该写入到迁移完成的Page Q上, 因为SMO 操作还未完成, 也就是已经完成了图a, 确定了seperate Key Kp, 这个时候其实应该把新的内容写入到Page Q 的delta chain 上, 但是此时该操作是不知道的.&lt;/p&gt;

&lt;p&gt;因此bw-tree 选择的是Page P 上标记正在进行smo 的状态, 那么写入操作帮忙Page P 完成这次SMO, 然后等SMO 结束以后已经知道Page Q了, 再把要写入的内容写入到Page Q上.&lt;/p&gt;

&lt;p&gt;bw-tree 为了解决write SMO 的时候同时有新的write 进来的时候, Bw-tree 让新的write 完成前面write SMO 操作, 但是具体工程实现上, 这样的操作想想就过于复杂, InnoDB 的btree 需要同时承担事务锁相关能力, 更增加了复杂度.&lt;/p&gt;

&lt;h3 id=&quot;结论&quot;&gt;结论&lt;/h3&gt;

&lt;p&gt;笔者主要站在工业界的视角看学术界如何解决btree 在multi-core 场景下的性能问题, 了解这些学术界的实现, 看是否对工业界有帮助.&lt;/p&gt;

&lt;p&gt;目前看来, 学术界这些新型的数据结构在工程上落地都有一定的难度.&lt;/p&gt;

&lt;p&gt;比如OLFIT假设读取一个page 是atomic 的, 这个在工程上是无法实现的. 也就是读取的时候如果有写入, 虽然后面可以通过版本号判断是旧的,但是可能存在的问题是读取了一半的内容, 那么使用这一半内容的MySQL程序本身就会corrupt.&lt;/p&gt;

&lt;p&gt;比如ART Tree 通过copy node 的方式实现Atomic 修改, 但是这样拷贝的开销工程上是无法接受的.&lt;/p&gt;

&lt;p&gt;比如Bw-tree 处理SMO 的时候如果有write 操作, SMO 和 SMO 的冲突的解决方案过于复杂等等.&lt;/p&gt;

&lt;p&gt;有些方案是latch-based, 有些方案是latch-free,  有些是hybrid, 在工程上我们一般如何考虑呢?&lt;/p&gt;

&lt;p&gt;在文章 “Latch-free Synchronization in Database Systems: Silver Bullet or Fool’s Gold?” 给出了比较有意义的结论:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Latch free Algorithm 通常在线程数超过CPU 核数的场景下, 比 latch-based 的算法要来的好.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this traditional database system architecture, the progress guarantees of latch-free algorithms are extremely valuable because a preempted context will never block or delay the execution of other contexts in the system (Section 2).&lt;/p&gt;

&lt;p&gt;因为在latch-based 做法里面, 当核数比thread 数多的时候, 有可能当前持有latch 的thread 被CPU 调度出去, 而其他线程被调度执行, 但是因为在等锁, 也无法执行, 持有latch 的thread 又得不到cpu, 陷入了死锁.&lt;/p&gt;

&lt;p&gt;在这种场景里面. latch free 算法有优势, 因为latch free 并没有一个thread 持有latch, 所有thread 都尝试乐观获得old value, 然后尝试更新, 最后再commit, 如果commit 之前被cpu 调度出去, 那么切回来以后可能old value 已经被修改了, 仅仅是commit 失败而已, 不会影响其他thread. 所以cpu 的调度对latch free的影响是不大的.&lt;/p&gt;

&lt;p&gt;当然这里的本质原因是操作系统的调度不感知用户层的信息, 也不可能感知. 持有Latch thread 和waiting latch thread 在操作系统眼里是一样的.&lt;/p&gt;

&lt;p&gt;现在一些新的im-memory database 一般都才去latch based 的方法, 尽可能不依赖操作系统的调度.&lt;/p&gt;

&lt;p&gt;而MySQL 正好属于这种场景, 在大量的并发场景下, 活跃连接远远高于cpu 核数. 所以latch free 是有意义的.&lt;/p&gt;

&lt;p&gt;但是现在新的Database 比如in-memory database 都是和core 绑定, 比如redis 这种单线程的. 包括polarstore 也是这样的设计, 那么这种场景下, 由于冲突不明显, 本身设计就考虑了尽可能减少contention, 所以latch-based 就更好了. 因为latch-based 更简单, lock-free 需要一些复杂的内存管理解决 ABA, garbage collect 等等问题.&lt;/p&gt;

&lt;p&gt;所以作者不建议database 里面建议使用latch-free algorithm, 更建议好好优化Latch-based algorithm.&lt;/p&gt;

&lt;p&gt;最后强调multi-core 上的scalability 重点在于减少在single shared meomry 上面的频繁操作, 也就是contention. 而不是在于选择latch-free 和 latch-based 的算法. 其实这个观点也是能理解的, 本质是能通过业务逻辑的设计避免contention 肯定是最好的.&lt;/p&gt;

&lt;p&gt;但是MySQL 内部其实有一些逻辑避免不了contention, 比如lock_sys-&amp;gt;mutex, trx_sys-&amp;gt;mutx 等等.&lt;/p&gt;

&lt;p&gt;另外可以看到实现一个bw-tree, blink-tree 在学术界的论文里面是非常简单的一个事情, 但是InnoDB 里面Page 里面结合了事务锁, 并发控制, undo log 等等一系列内容, 所以将InnoDB 的btree 修改并稳定到线上其实是比较大工程量的事情, 比如为了推动Blink-tree 上线, 笔者就写了Inno_space 这样的工具, 用于如果真的有数据损坏的情况, 对数据进行修复, 为了前期的灰度, blink-tree 先上线到二级索引等等这样的工作.&lt;/p&gt;

&lt;p&gt;那么PolarDB 是如何处理这个问题呢?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1]  Z. Wang, et al., &lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2020/papers/06-oltpindexes1/mod342-wangA.pdf&quot;&gt;Building A Bw-Tree Takes More Than Just Buzz Words&lt;/a&gt;, in &lt;em&gt;SIGMOD&lt;/em&gt;, 2018 
[2] S.K. Cha, et al., &lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2020/papers/06-oltpindexes1/cha-vldb2001.pdf&quot;&gt;Cache-Conscious Concurrency Control of Main-Memory Indexes on Shared-Memory Multiprocessor Systems&lt;/a&gt;, in &lt;em&gt;VLDB&lt;/em&gt;, 2001 &lt;em&gt;(Optional)&lt;/em&gt;
[3] G. Graefe, &lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2020/papers/06-oltpindexes1/a16-graefe.pdf&quot;&gt;A Survey of B-Tree Locking Techniques&lt;/a&gt;, in &lt;em&gt;TODS&lt;/em&gt;, 2010 &lt;em&gt;(Optional)&lt;/em&gt;
[4]  J. Faleiro, et al., &lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2020/papers/06-oltpindexes1/faleiro-cidr17.pdf&quot;&gt;Latch-free Synchronization in Database Systems: Silver Bullet or Fool’s Gold?&lt;/a&gt;, in &lt;em&gt;CIDR&lt;/em&gt;, 2017 &lt;em&gt;(Optional)&lt;/em&gt;
[5]  J. Levandoski, et al., &lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2020/papers/06-oltpindexes1/bwtree-icde2013.pdf&quot;&gt;The Bw-Tree: A B-tree for New Hardware&lt;/a&gt;, in &lt;em&gt;ICDE&lt;/em&gt;, 2013 &lt;em&gt;(Optional)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;[6]  V. Leis, et al., &lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2020/papers/07-oltpindexes2/leis-damon2016.pdf&quot;&gt;The ART of Practical Synchronization&lt;/a&gt;, in &lt;em&gt;DaMoN&lt;/em&gt;, 2016 &lt;em&gt;(Optional)&lt;/em&gt;
[7]  V. Leis, et al., &lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2020/papers/07-oltpindexes2/leis-icde2013.pdf&quot;&gt;The Adaptive Radix Tree: ARTful Indexing for Main-Memory Databases&lt;/a&gt;, in &lt;em&gt;ICDE&lt;/em&gt;, 2013 &lt;em&gt;(Optional)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;[8] https://15721.courses.cs.cmu.edu/spring2020/schedule.html&lt;/p&gt;

&lt;p&gt;[9] https://zhuanlan.zhihu.com/p/374000358&lt;/p&gt;
</description>
        <pubDate>Sun, 21 Aug 2022 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2022/08/21/polardb-bw-tree/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/08/21/polardb-bw-tree/</guid>
      </item>
    
      <item>
        <title>为什么游戏行业喜欢用PolarDB</title>
        <description>&lt;p&gt;&lt;strong&gt;游戏行业痛点&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在我看来, 不同行业对数据库使用有巨大的差别. 比如游戏行业没有复杂的事务交易场景, 他有一个非常大的blob 字段用于存储角色的装备信息, 那么大Blob 字段的更新就会成为数据库的瓶颈, 比如在线教育行业需要有抢课的需求, 因此会有热点行更新的场景, 对热点行如何处理会成为数据库的瓶颈, 比如SaaS 行业, 每一个客户有一个Database, 因此会有非常多的Table, 那么数据库就需要对多表有很好的支持能力.&lt;/p&gt;

&lt;p&gt;游戏行业和其他行业对数据库的使用要求是不一样的.&lt;/p&gt;

&lt;p&gt;所以在支撑了大量游戏业务之后, 我理解游戏行业在使用自建MySQL 的时候有3个比较大的痛点&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;对备份恢复的需求&lt;/li&gt;
  &lt;li&gt;对写入性能的要求&lt;/li&gt;
  &lt;li&gt;对跨region 容灾的需求&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;接下来会分别讲述这三个痛点PolarDB 是如何解决的.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;备份恢复&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;笔者和大量游戏开发者沟通中, 游戏行业对备份恢复的需求是极其强烈的. 比如在电商行业, 是不可能存在将整个数据库实例进行回滚到一天之前的数据, 这样所有的用户的购买交易信息都丢失了.&lt;/p&gt;

&lt;p&gt;但是, 在游戏行业中, 这种场景确实存在的, 比如在发版的时候, 游戏行业是有可能发版失败, 这个在其他行业出现概率非常低, 如果发版失败, 那么整个实例就需要回滚到版本之前. 因此每次发版的时候都需要对数据库实例进行备份. 因此当我们玩游戏的时候, 看到大版本需要停服更新, 那么就有可能是因为后台需要备份数据等等一系列操作了.&lt;/p&gt;

&lt;p&gt;还有一种场景, 当发生因为外挂, 漏洞, 参数配置错误等等场景下, 这种紧急情况游戏就需要回滚到出问题前的版本, 这样就需要对整个实例进行回滚.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/image-20220717042019107.png&quot; alt=&quot;image-20220717042019107&quot; /&gt;&lt;/p&gt;

&lt;p&gt;官方MySQL 由于是单机架构, 那么常见的备份方法是通过Xtrabackup 工具, 将数据备份到本地以后, 如果本地空间不够, 就需要上传到OSS 等远端存储中. 通常通过Xtrabackup 备份工具都需要1h 左右, 如果需要将数据上传到远端那么时间就更长了.&lt;/p&gt;

&lt;p&gt;PolarDB 是天然的计存分离的架构, 那么备份的时候通过底下分布式存储的快照能力, 备份可以不超过30s, 将备份时间大大缩短了.&lt;/p&gt;

&lt;p&gt;核心思路是采用Redirect-on-Write 机制, 每次创建快照并没有真正Copy数据, 只有建立快照索引, 当数据块后期有修改(Write)时才把历史版本保留给Snapshot, 然后生成新的数据块, 被原数据引用(Redirect).&lt;/p&gt;

&lt;p&gt;另外一种场景是, 在游戏行业中, 有可能某一个玩家的装备被盗号了, 那么玩家就会找游戏的运营人员投诉, 运营人员会找到游戏运维人员, 帮忙查询玩家的历史数据.&lt;/p&gt;

&lt;p&gt;笔者之前就遇到某著名游戏多个玩家被盗号, 然后运维人员经常需要通过PolarDB 按时间的还原的能力恢复出某多个不同时间点的实例, 用来查询这个玩家的具体装备信息, 同时由于玩家对盗号的时间也不准确, 经常有时候需要还原出多个实例才可以.&lt;/p&gt;

&lt;p&gt;针对这样的场景, PolarDB 推出了Flashback Query, 就可以在当前实例查询出任意时间点的历史数据. 具体原理见文章 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/434466612&quot;&gt;Flashback Query&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/image-20220717043717181.png&quot; alt=&quot;image-20220717043717181&quot; /&gt;&lt;/p&gt;

&lt;p&gt;整体而言, PolarDB 建立了一套非常完善的备份恢复能力, 从库=&amp;gt;表=&amp;gt;行三个维度满足的游戏行业对备份恢复的需求.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/image-20220717043059363.png&quot; alt=&quot;image-20220717043059363&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;写入性能&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;游戏行业使用数据库的方式也与其他行业有较大区别, 是一种非常弱Schema 的使用方式, 其他行业通常对业务经常抽象, 建立表结构, 每个字段尽可能小, 不建议有大字段, 有大字段尽可能进行拆封等等.&lt;/p&gt;

&lt;p&gt;但是在游戏行业中, 由于需要满足游戏快速迭代发展的需求, 玩家的装备信息结构非常复杂, 因此常见的做法是将玩家装备等级信息保存在一个大的blob字段中, 这个blob字段通过proto_buf 或者 json 进行编解码, 每次在获得装备或者升级以后, 就进行整个字段更新,  在游戏开服初期玩家数据长度较短, 而随着游戏版本更新版本, 游戏剧情, 运营活动的增多, 相对于游戏开服初期的数KB, blob字段的长度可能会膨胀到数百KB, 甚至达到MB级别, 因此可能只是获得一个装备, 就需要向数据库写入数百KB 大小的数据, 这样的写放大其实非常不合理.&lt;/p&gt;

&lt;p&gt;目前也有像MongoDB 这样的文档数据库, 让用户写入的时候仅仅更新某个字段, 从而减少写放大. 但是这样影响了用户的使用习惯, 需要用户在业务逻辑上进行修改, 这是快速发展的游戏行业所不能接受的, 所以笔者看到尽管有客户因为写入问题转向了MongoDB, 但是其实不多.&lt;/p&gt;

&lt;p&gt;PolarDB 针对这样的情况尽可能满足用户的使用习惯, 在数据库内核层面优化数据库的写入能力. 通过 partition redo log, redo log cache, undo log readahead,  early lock release, no blob latch 等等能力将写入能力充分优化. 具体原理可以参考我们&lt;a href=&quot;http://mysql.taobao.org/monthly/&quot;&gt;内核月报&lt;/a&gt; 和之前的文章&lt;a href=&quot;https://zhuanlan.zhihu.com/p/535426034&quot;&gt;PolarDB-cloudjump&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;针对游戏场景, 我们修改了 sysbench 工具, 模拟游戏行业中大Blob 更新的workload, 放在 &lt;a href=&quot;https://github.com/baotiao/game-sysbench&quot;&gt;game-sysbench&lt;/a&gt; 工具中, 后续我们还会将更多行业比如Saas, 电商等等行业的workload 放在这个工具中.&lt;/p&gt;

&lt;p&gt;在game_blob_update workload 中, 如果玩家的平均装备信息是 300kb, 我们对比了PolarDB VS aurora VS 自建MySQL 的数据&lt;/p&gt;

&lt;p&gt;PolarDB 8.0 相对有最高的QPS 1877.44, 峰值QPS最高可以到2000, CPU bound场景PolarDB的性能大概是Aurora的5.7倍, 是自建 MySQL 本地盘的3倍. IO bound场景PolarDB的性能是Aurora的15倍.&lt;/p&gt;

&lt;p&gt;CPU bound场景：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;DB&lt;/th&gt;
      &lt;th&gt;并发数据&lt;/th&gt;
      &lt;th&gt;QPS&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;PolarDB 8.0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1877.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MySQL 8.0 本地盘&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;600.22&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Aurora 8.0&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;328.47&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;IO bound场景：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;DB&lt;/th&gt;
      &lt;th&gt;并发数据&lt;/th&gt;
      &lt;th&gt;QPS&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;PolarDB 8.0&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;1035.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MySQL 8.0 本地盘&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;610&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Aurora 8.0&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;69.15&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;跨region 容灾&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;目前游戏行业纷纷出海, 包含了游戏服和平台服. 用户在自建MySQL/RDS 的场景中,  用户可能需要在另外一个region 建立一个新的实例, 然后通过同步工具或者DTS 进行跨region 备份. 用户需要处理region 错误场景如何进行切换等等.&lt;/p&gt;

&lt;p&gt;笔者认为对数据库而言, 稳定性 &amp;gt; 易用性 &amp;gt; 性能.&lt;/p&gt;

&lt;p&gt;在这个场景中, 用户如果使用云厂商的话, 使用的是云厂商提供的原子能力, 自己通过组装这些原子能力实现容灾的需求, 而PolarDB 针对这样场景提出来PolarDB GlobalDataba 的解决方案, 将跨region 的容灾放在解决方案中, 提供了一个更加易容的解决方案, 从而用户可以关注自身的业务逻辑, 而不需要处理这些容灾的场景.&lt;/p&gt;

&lt;p&gt;在具体跨region 的同步场景方案中, PolarDB 是通过多通道物理复制能力, 从而保证跨region 的容灾在1s 以内.&lt;/p&gt;

</description>
        <pubDate>Tue, 19 Jul 2022 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2022/07/19/polardb-game/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/07/19/polardb-game/</guid>
      </item>
    
      <item>
        <title>通过performance_schema 定量分析系统瓶颈</title>
        <description>&lt;p&gt;目前在系统里面, 我们可以通过perf 或者 pt-pmp 汇总堆栈的方式来查看系统存在的热点, 但是我们仅仅能够知道哪些地方是热点, 却无法定量的说这个热点到底有多热, 这个热点占整个访问请求的百分比是多少? 是10%, 还是40%, 还是80%?&lt;/p&gt;

&lt;p&gt;所以我们需要一个定量分析系统瓶颈的方法以便于我们进行系统优化.&lt;/p&gt;

&lt;p&gt;本文通过Performance_schema 来进行定量的分析系统性能瓶颈.&lt;/p&gt;

&lt;p&gt;原理如下:&lt;/p&gt;

&lt;p&gt;performance_schema.events_waits_summary_global_by_event_name  这里event_name 值得是具体的mutex/sx lock, 比如trx_sys-&amp;gt;mutex, lock_sys-&amp;gt;mutex 等等,  这个table 保存的是汇总信息.&lt;/p&gt;

&lt;p&gt;具体performance_schema 信息在这里 https://dev.mysql.com/doc/mysql-perfschema-excerpt/8.0/en/performance-schema-wait-summary-tables.html&lt;/p&gt;

&lt;p&gt;通过两次调用具体的timer wait 可以算出具体某一个mutex/sx lock 等待的时间.&lt;/p&gt;

&lt;p&gt;如果这个时间再除以每一个线程就可以算出每一个线程在这个Lock 上大概的等待时间, 然后就可以算出平均1s 内等在该mutex/sx lock 的占比.&lt;/p&gt;

&lt;p&gt;比如我们知道在sysbench oltp_read_write 的小表测试中, 通过pstack 可以看到主要卡在page latch 上, 那么我们需要分析等待patch latch 占用了整个路径的时间大概是多长.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/image-20220701045526838.png&quot; alt=&quot;image-20220701045526838&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里使用256 thread 进行压测, 计算出来等待的时间大概是&lt;/p&gt;

&lt;p&gt;buf_block_lock = (122103591705572800-121158362355835200)/5/207/1000000000 = 913ms&lt;/p&gt;

&lt;p&gt;也就是平均 1s 里面, 每一个thread 有913ms 等待在page lock 上, 占比90%. 这个信息和多次pstack 的信息也基本吻合.&lt;/p&gt;

&lt;p&gt;fil_system_mutex = (3045412747942400-3044314172171200)/5/207 = 1ms&lt;/p&gt;

&lt;p&gt;也就是平均1s 里面等待在fil_system_mutex 只有1ms, 占比0.1%&lt;/p&gt;

&lt;p&gt;比如我们最常见的 oltp_insert 非 auto_inc insert 的场景中, 通过pstack 可以看到主要卡在trx_sys-&amp;gt;mutex, 那么这个trx_sys-&amp;gt;mutex 具体有多热呢?&lt;/p&gt;

&lt;p&gt;以下是perf 相关信息.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/image-20220701065441299.png&quot; alt=&quot;image-20220701065441299&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上面红框下主要的热点都是需要去获得trx_sys-&amp;gt;mutex, 从而可以操作全局活跃事务数组.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/image-20220701070450831.png&quot; alt=&quot;image-20220701070450831&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里使用256 thread 进行压测, 计算出来等待的时间大概是&lt;/p&gt;

&lt;p&gt;trx_sys_mutex =(19702987247840000-19258717650739200)/5/250/1000000000 = 355 ms&lt;/p&gt;

&lt;p&gt;那么等待trx_sys-&amp;gt;mutex 上占比大概是35%.&lt;/p&gt;

&lt;p&gt;上面还有一个看过去大头的btree 上面的 index_tree_rw_lock 占比呢&lt;/p&gt;

&lt;p&gt;index_tree_rw_lock  = (471944089179312000-471896220032430400)/5/250/1000000000 = 38ms&lt;/p&gt;

&lt;p&gt;虽然数据大, 因为跑的久, 但是其实这里只有3% 的占比&lt;/p&gt;

&lt;p&gt;tips:&lt;/p&gt;

&lt;p&gt;对比来说 perf 看到的信息是on-cpu 信息, 但是因为MySQL 的mutex/sxlock 都是通过backoff 机制进行, 在每一次线程切换出去之前都进行一段时间的spin, 所以mysql 的on-cpu 信息可以一定程度反应off-cpu 的结果.&lt;/p&gt;

&lt;p&gt;pstack 更体现的是某一时刻off-cpu 的信息&lt;/p&gt;

&lt;p&gt;performance_schame wait_event 也体现的是off-cpu 的信息.&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jul 2022 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2022/07/05/mysql-performance_schema/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/07/05/mysql-performance_schema/</guid>
      </item>
    
      <item>
        <title>PolarDB-CloudJump：优化基于云存储服务的云数据库(发表于VLDB 2022)</title>
        <description>&lt;p&gt;云数据库实现计算存储分离，支持计算与存储的独立扩展，其用户还可以享受按量付费等特性。这使得基于云数据库的系统更加高效、灵活。因此，构建并使用云原生数据库的势头愈演愈烈。另一方面，云化存储服务已经是云的标准能力，存储侧提供兼容通用的文件接口，并且不对外暴露持久化、容错处理等复杂细节，其易用性和规模化带来的高性价比使得云存储成为了云上系统的第一选择。在通用云存储服务上构建云数据库，无疑是一种既能够享受规模化云存储红利，又能够通过可靠云存储服务实现降低维护成本、加速数据库开发周期的方案。&lt;/p&gt;

&lt;p&gt;然而，考虑到云存储和本地存储之间的特性差异，在将本地数据库迁移到云上构建云数据库时，如何有效使用云存储面临了许多挑战。对此，我们在论文里分析了基于B-tree和LSM-tree的存储引擎在云存储上部署时面临的挑战，并提出了一个优化框架CloudJump，以希望能够帮助数据库开发人员在基于云存储构建数据库时使系统更为高效。我们以云原生数据库PolarDB为案例，展示了一系列针对性优化，并将部分工作扩展应用到基于云存储的RocksDB上，以此来演示CloudJump的可用性。&lt;/p&gt;

&lt;h3 id=&quot;背景&quot;&gt;背景&lt;/h3&gt;

&lt;p&gt;我们讨论的云存储主要基于弹性分布式块存储，云中其他类型的存储服务，例如基于对象的存储，不在本文的讨论范围内。共享云存储（如分布式块存储服务加分布式文件系统）可以作为多个计算节点的共享存储层，提供QoS（服务质量）保证、大容量、弹性和按量付费定价模型。对于大多数云厂商和云用户来说，拥有云存储服务比构建和维护裸机SSD集群更有吸引力。因此，与其为云本机数据库构建和优化专用存储服务，不如利用现有云存储服务构建云本机数据库，这是一种非常可行的选择。此外，随着云存储服务几乎实现了标准化，相应的开发、迁移变得更加快速。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/f1.png&quot; alt=&quot;f1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图1展示了本地数据库（不含备份）与shared-storage云原生数据库的系统结构，AWS Aurora首先引导了这种从本地数据库向shared-storage云原生数据库的迁移。它将数据库分为存储层和计算层，并可以独立扩展每一层。为了消除了传输数据页中产生的沉重的网络开销，它进一步定制了存储层，在数据页上应用重做日志，从而不再需要在两层之间传输数据页。无疑这种设计在云中提供了一种非标准存储服务，只能由Aurora的计算层使用。&lt;/p&gt;

&lt;p&gt;另一种方案是依赖标准化接口的云存储服务迁移或构建获得云数据库，这也是本文的研究目标。前面已经提到过，这样做的优势主要在于的可以实现系统的快速开发、平滑迁移、收纳标准化规模化存储服务的原有优势等。此外，特别是在我们项目（PolarDB）的硬件环境、已有背景下，兼顾服务可靠性和开发迭代需求，针对进行云存储服务特性进行性能优化是最迫切的第一步。&lt;/p&gt;

&lt;h3 id=&quot;挑战与分析&quot;&gt;挑战与分析&lt;/h3&gt;

&lt;p&gt;云存储和本地SSD存储在带宽、延迟、弹性、容量等方面存在巨大差异，例如图2展示了在稳态条件下本地SSD与云存储I/O延时、带宽与工作线程关系，它们对数据库等设计有着巨大影响。此外，共享存储的架构特性也会对云存储带来影响，如多个节点之间的数据一致性增加了维护cache一致性开销&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/f2.png&quot; alt=&quot;f2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;通过系统实验、总结分析等，我们发现CloudJump面临以下技术挑战：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;远程分布式存储集群的访问导致云存储服务的I/O延迟高；&lt;/li&gt;
  &lt;li&gt;通常聚合I/O带宽未被充分利用；&lt;/li&gt;
  &lt;li&gt;在具有本地存储的单机上运行良好但需要适应云存储而导致特性改变的传统设计，例如文件cache缓存；&lt;/li&gt;
  &lt;li&gt;长链路导致各种数据库I/O操作之间的隔离度较低（例如，日志刷写与大量数据I/O的竞争）；&lt;/li&gt;
  &lt;li&gt;云用户允许且可能使用非常大的单表文件（例如数十TB）而不进行数据切分，这加剧了I/O问题的影响。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;针对不同的数据存储引擎，如基于B-tree和LSM-tree的存储引擎，这些特性差异会带来不同的性能差异，表1归纳总结了这些挑战及其对数据库设计的影响。其中有共性问题，如WAL路径写入变慢、共享存储（分布式文件系统）cache一致性代价等；也有个性问题，如B-tree结构在独占资源情况下做远程I/O、远程加剧I/OLSM-tree读放大影响等。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/f3.png&quot; alt=&quot;f3&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;优化原则&quot;&gt;优化原则&lt;/h3&gt;

&lt;p&gt;CloudJump针对上述挑战，提出7条优化准则：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Thread-level Parallelism&lt;/strong&gt;：例如依据I/O特性实验，采用（更）多线程的日志、数据I/O线程及异步I/O模型，将数据充分打散到多个存储节点上。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Task-level Parallelism&lt;/strong&gt;：例如对集中Log buffer按Page Partition分片，实现并行写入并基于分片进行并行Recovery。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reduce remote read and Prefetching&lt;/strong&gt;：例如通过收集并聚合原分散meta至统一的superblock，将多个I/O合一实现fast validating；通过预读利用聚合读带宽、减少读任务延时；通过压缩、filter过滤减少读取数据量。与本地SSD上相比，这些技术在云存储上更能获得收益。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fine-grained Locking and Lock-free Data Structures&lt;/strong&gt;：云存储中较长的I/O延迟放大了同步开销，主要针对Update-in-place系统，实现无锁刷脏、无锁SMO等。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scattering among Distributed Nodes&lt;/strong&gt;：在云存储中，多个节点之间的分散访问可以利用更多的硬件资源，例如将单个大I/O并发分散至不同存储节点 ，充分利用聚合带宽。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bypassing Caches&lt;/strong&gt;：通过Bypassing Caches来避免分布式文件系统的cache coherence，并在DB层面优化I/O格式匹配存储最佳request格式。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scheduling Prioritized I/O Tasks&lt;/strong&gt;：由于访问链路更长（如路径中存在更多的排队情况），不填I/O请求间的隔离性相对本地存储更低，因此需要在DB层面对不同I/O进行打标、调度优先级，例：优先WAL、预读分级。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;实践案例&quot;&gt;实践案例&lt;/h3&gt;
&lt;h4 id=&quot;实践案例polardb&quot;&gt;实践案例：PolarDB&lt;/h4&gt;
&lt;p&gt;PolarDB构建基于具有兼容Posix接口的分布式文件系统PolarFS，与Aurora一样采用计算存储分离架构，借助高速RDMA网络以及最新的块存储技术实现超低延迟和高可用能力能力。在PolarDB上，我们做了许多适配于分布式存储特性、符合CloudJump准则的性能优化，大幅提升了云原生数据库PolarDB的性能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/f4.png&quot; alt=&quot;f4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. WAL写入优化&lt;/strong&gt;
WAL（Write ahead log）写入是用于一致性和持久性的关键路径，事务的写入性能对log I/O的延迟非常敏感。原生InnoDB以MTR（Mini-Transaction）的粒度组织日志，并保有一个全局redo日志缓冲区。 当一个MTR被提交时，它缓存的所有日志记录被追加到全局日志缓冲区，然后集中的顺序刷盘以保证持久化特性。这一传统集中日志模式在本地盘上工作良好，但使用云存储时，集中式日志的写入性能随着远程I/O时延变高而下降，进而影响事务写入性能。基于云存储的特性，我们提出了两个优化来提升WAL的写入性能：日志分片和（大）I/O任务并行打散。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/f5.png&quot; alt=&quot;f5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Redo日志分片&lt;/em&gt;：InnoDB的redo采用的是Physiological Logging的方式，大部分MTR针对单个的数据页（除部分特殊），页之间基本相互独立。如图5（左），我们将redo日志、redo缓冲区等按其修改的page进行分片（partition），分别写入不同的文件中，来支持并发写log（以及并发Recovery，并发物理复制等），从而在并发写友好的分布式文件系统上的获得写入性能优势。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I/O任务并行打散&lt;/em&gt;：在云存储中，一个文件由多个chunk组成，根据chunk的分配策略，不同chunk很可能位于不同的存储节点中。我们将每个redo分片（partition）的文件进一步拆分为多个物理分片（split），如图5（右）所示，对于单个大log I/O任务（如group commit、BLOB record等），log writer会将I/O按lsn切片并且并发的分发I/O请求至不同split。通过这种方式，可以将大延时的log I/O任务拆分，并利用分布式存储高分布写特性来减少整体I/O时间。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. 快速恢复&lt;/strong&gt;
为了实现快速恢复，我们提出了两个优化：快速（启动）验证和全并行恢复。
&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/f6.png&quot; alt=&quot;f6&quot; /&gt;
在InnoDB的原有恢复过程中，InnoDB首先在启动期间会打开所有文件读取元信息（如文件名、表ID等）并验证正确性，然后通过ARIES风格的恢复算法重做未checkpoint的数据。为了加速启动，快速验证方法不会扫描所有文件，而是在数据库的生命周期中记录和集中必要的元信息，并在创建、修改文件时将必要的元信息集中记录在一个superblock中，在启动时仅扫描元数据块文件。因此，减少了启动扫描过程中的远程I/O访问开销。其次，依赖于Redo日志分片，我们将log file按page拆分成多个文件，在恢复阶段（可以进一步划分为parse、redo、undo三个阶段），可以天然的支持并发parse和redo（undo阶段在后台进行），通过并发任务充分调动CPU和I/O资源加速恢复。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. 预读取&lt;/strong&gt;
在云存储环境下，读I/O延时大大增加，当用户任务访问数据发生cache miss的情况下，而有效的预读取能够充分利用聚合读带宽来减少读任务延时。InnoDB中有线性预读和非线性预读两种原生的物理预读方法，我们进一步引入了逻辑预读策略（由于无序的插入和更新，索引在物理上不一定是顺序的）。例如对于主索引扫描，当任务线程从起始键顺序扫描索引超过一定阈值时，逻辑预读会在主索引上按逻辑顺序触发异步预读，提前读取一定量的顺序页。又如对于具有二级索引和非索引列回表操作的扫描，在对二级索引进行扫描同时批量收集相关命中的主键，积累一定批数据后触发异步任务预读对应主索引数据（此时剩余的二级索引扫描可能仍在进行中）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. 同步（锁）优化&lt;/strong&gt;
相关背景可以先查阅&lt;a href=&quot;https://zhuanlan.zhihu.com/p/151397269&quot;&gt;《InnoDB btree latch 优化历程》&lt;/a&gt;这篇文章。
&lt;em&gt;无锁刷脏&lt;/em&gt;：原生InnoDB在刷脏时需要持有当前page的sx锁，导致I/O期间当前page的写入被完全阻塞。而在云存储上I/O延迟更高，阻塞时间更久。我们采用shadow page的方式，首先对当前page构建内存副本，构建好内存副本后原有page的sx锁被释放，然后用这个shadow page内容去做刷脏及相关刷写信息更新。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;SMO加锁优化&lt;/em&gt;：在InnoDB 里面, 依然有一个全局的index latch, 由于全局的index latch 存在会导致同一时刻在Btree 中只有一个SMO 能够发生, index latch 依然会成为全局的瓶颈点。上述index latch不仅是计算瓶颈，而从另一方面考虑，锁同步期间index上其他可能I/O操作无法并行，存储带宽利用率较低。相关实现可以参考文章&lt;a href=&quot;https://zhuanlan.zhihu.com/p/374000358&quot;&gt;《路在脚下, 从BTree 到Polar Index》&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5. 多I/O任务队列适配&lt;/strong&gt;
针对云存储具有I/O隔离性低的挑战，同时为了避免云存储无法识别DB层存储内核的I/O语义，而造成优先级低的I/O请求（如page刷脏、低优先级预读）影响关键I/O路径的性能，在数据库内核中提供合理的I/O调度模型是很重要的。在 PolarDB 中，我们在数据库内核层为不同类型的I/O请求进行调度，实现根据当前I/O压力实现数据库最优的性能，每个I/O请求都具有 DB 层的语义标签，如 WAL 读/写，Page 读/写。
&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/f7.png&quot; alt=&quot;f7&quot; /&gt;
我们为数据库的异步I/O请求建立了多个支持并发写入的生产者 / 消费者队列，并且其存在三种不同特性的队列，分别为 Private 队列，Priority 队列，以及 General 队列，不同队列的数量是根据当前云存储的I/O能力决定的。&lt;/p&gt;

&lt;p&gt;正常情况下，WAL 的写入只通过其 Private 队列，当写入量增大时，其I/O请求也会转发至 Priority 队列，此时 Priority 队列会优先执行 WAL 的写入，并且后续Page写入的I/O不会进入 Priority 队列。基于这种I/O模型，我们保证了一定部分的I/O资源时预留给WAL写入，保证事务提交的写入性能，充分利用云存储的高聚合带宽。此外，I/O任务队列的长度和数目也进行了拓展以一步匹配云存储高吞吐、大带宽但时延较高且波动大的特性。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6. 格式化I/O请求&lt;/strong&gt;
云存储和本地存储在I/O格式上具有显著的不同，例如 Block 大小，I/O请求的发起方式。在大多数分布式的云存储中，在实现多个计算节点的共享时，为了避免维护计算节点 cache 一致性的问题，不存在 page cache，此时采用原先本地存储的I/O格式在云上会造成例如 read-on-write 和逻辑与物理位置映射的问题，造成性能下降。在 PolarDB中，我们为WAL I/O和 Page I/O匹配了适应云存储的请求I/O格式以尽可能降低单个I/O的延时。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;WAL I/O对齐&lt;/em&gt;：文件是通过固定大小的 block 进行读写的。云存储具有更大的 block size （4-128 KB），传统的 log 对齐策略不适合云存储上的 stripe boundary。我们在 log 数据进行提交的时候，将I/O请求的长度和偏移与云存储的 block size进行对齐。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Data I/O对齐&lt;/em&gt;：例如当前存在两种类型的数据页：常规页和压缩页，常规页为16 KB，可以很容易与云存储的 Block size 进行对齐，但是压缩页会造成后续大量的不对齐I/O。以PolarDB 中对于压缩页的对齐为例。首先，我们读取时保证以最小单位（如PolarFS的4 KB）读取。而在写入时，对于所有小于最小访问单元的压缩页数据，我们会拓展到最小单位再进行写入，以保证存储上的页数据都是最小单位对齐的。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;去除 Data I/O合并&lt;/em&gt;：在本地数据库中，数据页的I/O会被合并来形成大的I/O实现连续地顺序写入。在云存储中，并发地向不同存储节点写入具有更高的性能，因此在云存储的数据库上，可以无需数据页的I/O合并操作。&lt;/p&gt;

&lt;p&gt;受篇幅所限，我们在本文中只简单介绍所提优化方法的大致实现逻辑，具体实现细节请读者查阅论文及相关文章。&lt;/p&gt;

&lt;p&gt;为了验证我们的优化效果, 我们对比了为针对云存储优化的MySQL 分别运行在PolarStore 和 Local Disk, 以及我们优化以后的PolarDB, 从下图可以看到PolarDB 在CPU-bound, IO-bound sysbench, TPCC 等各个场景下都表现除了明显的性能优势.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/image-20220623203947797.png&quot; alt=&quot;image-20220623203947797&quot; /&gt;&lt;/p&gt;

&lt;p&gt;同时, 为了证明我们的优化效果不仅仅对于我们自己的云存储PolarStore 有收益, 对于所有的云存储应该都有收益, 因此我们将针对云存储优化的PolarDB 运行在 StorageX, ESSD 等其他云存储上,  我们发现均能获得非常好的性能提升, 从而说明我们的优化对于大部分云存储都是有非常大的收益&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/image-20220623204635224.png&quot; alt=&quot;image-20220623204635224&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;实践案例rocksdb&quot;&gt;实践案例：RocksDB&lt;/h4&gt;

&lt;p&gt;我们还将CloudJump的分析框架和部分优化方法拓展到基于云存储的RocksDB上，同样获得了预计的性能收益。
&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/f8.png&quot; alt=&quot;f8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Log I/O任务并行打散&lt;/strong&gt;
RocksDB同样使用集中WAL来保证进程崩溃的一致性，集中日志收集多个column family的日志记录并持久化至单个日志文件。考虑到LSM-Tree只需要恢复尾部append-only的数据块，我们采用在上一个案例中提到的&lt;em&gt;log I/O并行打散&lt;/em&gt;的方法在log writer中切分批日志并且并行分发到不同文件分片中。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. 数据访问加速&lt;/strong&gt;
在RocksDB中有许多加速数据访问的技术，主要有prefetching, filtering 和compression机制。考虑到云存储的特性，这些技术（经过适当改造）在云存储环境中更有价值。经过分析和实验，我们提出了以下建议：1）预读机制能加速部分查询和compaction操作，建议compaction操作开启预读并设定合理的预读I/O任务优先级，并将单个预读操作的大小对齐存储粒度，对于查询操作预读应由用户场景确定；2）在云存储上建议开启bloom filters，并且将filter的meta和常规数据分离，将filter信息并集中管理；3）采用块压缩来减小数据访问的整体用时，如下表展示了数据量和PolarFS访问延时，表中存储基于RDMA，在延迟更高的存储环境中，压缩收益更高，引入压缩后数据访问的整体延时（特别是读延时）下降。
&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/f9.png&quot; alt=&quot;f9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. 多I/O任务队列及适配&lt;/strong&gt;
在多核硬件环境中，我们引入了一个多队列I/O模型并在RocksDB中拆分I/O任务和工作任务（例如压缩作业和刷新作业）。这是因为我们通过调整I/O线程的数量来控制较好吞吐和延迟关系。由于将I/O任务与后台刷写作业分离，因此无需进一步增加刷写线程的数量，刷写线程只会对齐I/O请求并进行调度分发。RocksDB本身提供了基于线程角色的优先级调度方案，而我们的调度方法这里是基于I/O标签。&lt;/p&gt;

&lt;p&gt;我们根据云存储调整I/O请求和数据组织（例如block和SST）的大小，并进行更精确的控制，以使SST文件过滤器的块大小也正确对齐。以PolarFS为例，存储的最小请求大小为4 KB（表示最小的处理单元），理想的请求大小为16 KB的倍数（不造成read-on-write），元数据存储粒度为4 MB。SST大小和块大小分别严格对齐存储粒度和理想请求大小的倍数。原生RocksDB也有对齐策略，我们在此需要进行存储参数适配并且对压缩数据块也进行对齐。&lt;/p&gt;

&lt;p&gt;我们不会向多队列I/O模型传递小于最小请求大小的I/O请求，而是对齐最小I/O大小，并将未对齐的后缀缓存在内存中以供后续对齐使用。其次，我们不会下发单个大于存储粒度的I/O请求，而是通过多队列I/O模型执行并行任务（例如一个6 MB的I/O会分散成4 MB加1 MB的两个任务）。这不仅可以将数据尽可能分散在不同的存储节点上，还可以最大限度地提高并行性以充分利用带宽。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. I/O对齐&lt;/strong&gt;
在所有日志和数据I/O请求排入队列前，对其的大小和起始offset进行对齐。对于WAL写入路径，类似于PolarDB的log I/O对齐。对于数据写入路径，在采用数据压缩时，LSM树结构可能会有大量未对齐的数据块。例如要刷写从1 KB开始的2 KB日志数据时，它将从内存缓存的数据中填充前1 KB（对于append-only结构通过保存尾部数据缓存实现，这是与update-in-place结构直接拓展原生页至最小单位的不同之处），并在3-4 KB中附加零，然后从0 KB起始发送一个4 KB的I/O。&lt;/p&gt;

&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;

&lt;p&gt;在这项论文工作中，我们分析了云存储的性能特征，将它们与本地SSD存储进行了比较，总结了它们对B-tree和LSM-tree类数据库存储引擎设计的影响，并推导出了一个框架CloudJump来指导本地存储引擎迁移到云存储的适配和优化。 并通过PolarDB, RocksDB 两个具体Case 展示优化带来的收益.&lt;/p&gt;

&lt;p&gt;更详细的内容请参阅论文《CloudJump: Optimizing Cloud Database For Cloud Storage》。&lt;/p&gt;

&lt;p&gt;感谢数据库产品事业部架构师团队, 感谢 POLARDB 团队全体同学.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 Jul 2022 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2022/07/04/polardb-innodb/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/07/04/polardb-innodb/</guid>
      </item>
    
      <item>
        <title>MySQL unique key check issue</title>
        <description>&lt;h3 id=&quot;innodb-unique-check-的问题&quot;&gt;InnoDB unique check 的问题&lt;/h3&gt;

&lt;p&gt;unique secondary index 是客户经常使用的场景, 用来保证index 上的record 的唯一性. 但是大量的客户在使用unique secondary index 以后会发现偶尔会有死锁或者不应该锁等待的时候却发生锁等待的情况. 也有很多客户来问我们这个问题.&lt;/p&gt;

&lt;p&gt;理论上PolarDB 默认使用read-commit isolation level,  在rc 隔离级别下绝大部分场景不会使用GAP lock, 因此死锁的概率应该是比较低的. 这又是为什么呢?&lt;/p&gt;

&lt;p&gt;关于InnoDB 事务锁介绍可以看这个&lt;a href=&quot;http://mysql.taobao.org/monthly/2016/01/01/&quot;&gt;InnoDB lock sys&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;其实这个问题是已经有十年历史的老问题, 也是官方一直没解决的问题.&lt;/p&gt;

&lt;p&gt;https://bugs.mysql.com/bug.php?id=68021&lt;/p&gt;

&lt;p&gt;我们用这个bug issue 里面的case 描述一下这个问题&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;-- Prepare test data&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;`ti`&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;nv&quot;&gt;`session_ref_id`&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;bigint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AUTO_INCREMENT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;nv&quot;&gt;`customer_id`&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;bigint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DEFAULT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;nv&quot;&gt;`client_id`&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DEFAULT&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;7&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;nv&quot;&gt;`app_id`&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;smallint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DEFAULT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;PRIMARY&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;KEY&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;`session_ref_id`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;UNIQUE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;KEY&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;`uk1`&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;`customer_id`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;`client_id`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;`app_id`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ENGINE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;InnoDB&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DEFAULT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CHARSET&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utf8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;INSERT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;INTO&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ti&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;session_ref_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;customer_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;app_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;VALUES&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;INSERT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;INTO&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ti&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;session_ref_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;customer_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;app_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;VALUES&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4090&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;INSERT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;INTO&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ti&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;session_ref_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;customer_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;app_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;VALUES&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;INSERT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;INTO&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ti&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;session_ref_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;customer_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;app_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;VALUES&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;14000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;session 1 删除这一行(4090, 9000, 10, 5); 然后再insert 一个二级索引一样的一行 (5000, 9000, 10, 5);&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mysql&quot;&gt;-- session 1
session1 &amp;gt; start transaction;
Query OK, 0 rows affected (0.00 sec)

session1 &amp;gt; DELETE FROM ti WHERE session_ref_id = 4090;
Query OK, 1 row affected (0.00 sec)

session1 &amp;gt; INSERT INTO ti (session_ref_id, customer_id, client_id, app_id) VALUES (5000, 9000, 10, 5);
Query OK, 1 row affected (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来问题出现了.&lt;/p&gt;

&lt;p&gt;可以看到插入 (NULL, 8001, 10, 5) 这一行的时候出现了锁等待, 理论上不应该有锁等待的, 因为pk 是自增, 而二级索引(8001, 10, 5) 并没有和任何record 冲突, 为什么会这样呢?&lt;/p&gt;

&lt;p&gt;而插入 (NULL, 7999, 10, 5) 却没有问题, 二级索引(7999, 10, 5) 同样也没有和任何二级索引冲突&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mysql&quot;&gt;-- session 2
session2 &amp;gt; set innodb_lock_wait_timeout=1;
Query OK, 0 rows affected (0.00 sec)

session2 &amp;gt; start transaction;
Query OK, 0 rows affected (0.00 sec)

session2 &amp;gt; INSERT INTO ti (session_ref_id, customer_id, client_id, app_id) VALUES (NULL, 8001, 10, 5);
ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction

session2 &amp;gt; INSERT INTO ti (session_ref_id, customer_id, client_id, app_id) VALUES (NULL, 7999, 10, 5);
Query OK, 1 row affected (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看事务锁信息可以看到&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mysql&quot;&gt;mysql&amp;gt; select ENGINE_TRANSACTION_ID, index_name, lock_type, lock_mode, LOCK_STATUS, lock_data  from performance_schema.data_locks;
+-----------------------+------------+-----------+------------------------+-------------+--------------+
| ENGINE_TRANSACTION_ID | index_name | lock_type | lock_mode              | LOCK_STATUS | lock_data    |
+-----------------------+------------+-----------+------------------------+-------------+--------------+
|              99537179 | NULL       | TABLE     | IX                     | GRANTED     | NULL         |
|              99537179 | uk1        | RECORD    | X,GAP,INSERT_INTENTION | WAITING     | 9000, 10, 5  |
|              99537176 | NULL       | TABLE     | IX                     | GRANTED     | NULL         |
|              99537176 | PRIMARY    | RECORD    | X,REC_NOT_GAP          | GRANTED     | 4090         |
|              99537176 | uk1        | RECORD    | X,REC_NOT_GAP          | GRANTED     | 9000, 10, 5  |
|              99537176 | uk1        | RECORD    | S                      | GRANTED     | 9000, 10, 5  |
|              99537176 | uk1        | RECORD    | S                      | GRANTED     | 10000, 10, 5 |
|              99537176 | uk1        | RECORD    | S,GAP                  | GRANTED     | 9000, 10, 5  |
+-----------------------+------------+-----------+------------------------+-------------+--------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;session1 在uk1 上持有(10000, 10, 5), (9000, 10, 5) 上面的next-key lock.&lt;/p&gt;

&lt;p&gt;session2 插入(8001, 10, 5) 这一行记录的时候, 走的是正常的insert 逻辑, 最后在插入的时候需要申请insert record 的下一个key 上面的GAP| insert_intention lock.  和trx1 上面持有的(9000, 10, 5) next-key lock 冲突了, 所以需要等待.&lt;/p&gt;

&lt;p&gt;而如果插入的记录是(7999, 10, 5) 需要申请的insert record 下一个key 是(8000, 10, 5) 的 GAP | insert_intention lock, 那么自然没有冲突, 那么就能够插入成功.&lt;/p&gt;

&lt;p&gt;那么为什么session1 需要持有 next-key lock, 我们需要先了解二级索引的unique check 的流程是怎样的?&lt;/p&gt;

&lt;p&gt;以下是 pseudocode&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    find the B-tree page in the secondary index you want to insert the value to
    assert the B-tree page is latched
    equal-range = the range of records in the secondary index which conflict with your value 
    if(equal-range is not empty){
      release the latches on the B-tree and start a new mini-transaction
      for each record in equal-range
        lock gap before it, and the record itself (this is what LOCK_S does)
      also lock the gap after the last(equal-range)
      also (before Bug #32617942 was fixed) lock the record after last(equal-range)
      once you are done with all of the above, find the B-tree page again and latch it again
    }
    insert the record into the page and release the latch on the B-tree page.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;可以看到在二级唯一索引插入record 的时候, 分成了两个阶段&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;判断当前的物理记录上是否有冲突的record(delete-marked 是不冲突)&lt;/li&gt;
  &lt;li&gt;如果没有冲突, 那么可以执行插入操作&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这里在阶段1 和 阶段2 之间必须有锁来保证(可以是lock, 也可以是latch), 否则阶段1 判断没有冲突可以插入的时候, 但是在阶段1和阶段2 之间另外一个事务插入了一个冲突的record, 那么阶段2 再插入的时候其实是冲突了.&lt;/p&gt;

&lt;p&gt;所以当前的实现如果gap 上存在至少一个相同的record, 大概率是delete-marked record, 那么需要给整个range 都加上gap X lock, 加了gap X lock 以后就可以禁止其他事务在这个gap 区间插入数据, 也就是通过lock 来保证阶段1和阶段2的原子性.&lt;/p&gt;

&lt;p&gt;如果gap 上没有相同的record, 那么就不需要进任何gap lock.&lt;/p&gt;

&lt;p&gt;比如只包含pk, sk 的一个table.&lt;/p&gt;

&lt;p&gt;已经存在的二级索引记录 &amp;lt;1, 1&amp;gt;, &amp;lt;4, 2&amp;gt;, &amp;lt;10(delete-mark), 3&amp;gt;, &amp;lt;10(d), 8&amp;gt;, &amp;lt;10(d), 11&amp;gt;, &amp;lt;10(d), 21&amp;gt;, &amp;lt;15, 9&amp;gt;  需要插入二级索引&amp;lt;10, 6&amp;gt;, 那么就需要给&amp;lt;10, 3&amp;gt;, &amp;lt;10, 8&amp;gt;,&amp;lt;10,11&amp;gt;,&amp;lt;10,21&amp;gt;, &amp;lt;15, 9&amp;gt; 都加上next-key lock.&lt;/p&gt;

&lt;p&gt;注意: 这里&amp;lt;15, 9&amp;gt;也需要加上next-key lock, 为的是保证像&amp;lt;10, 100&amp;gt; 这样的record 也不允许插入的. 但是如果这里&amp;lt;15, 9&amp;gt; 是&amp;lt;15000, 9&amp;gt; 那么这里被锁住的gap 区间就非常非常大了, 也是上述issue 遇到的问题.&lt;/p&gt;

&lt;p&gt;具体实现在row_ins_scan_sec_index_for_duplicate() 中.&lt;/p&gt;

&lt;p&gt;如果把这个next-key lock 去掉会有什么问题?&lt;/p&gt;

&lt;p&gt;其实官方做过这个改动, 但是这个改动带来了严重的 &lt;a href=&quot;https://bugs.mysql.com/bug.php?id=73170&quot;&gt;bug#73170&lt;/a&gt;, 会导致二级索引的唯一性约束有问题, 出现unique-index 上面出现了相同的record. 所以官方后来快速把这个fix 又revert掉了, 这个问题也就一直没解决了. 为什么会这样呢?&lt;/p&gt;

&lt;p&gt;我们简化一下上述的二级索引, 把(9000, 10, 5) 简化成9000, 因为(10, 5) 都是一样的. 下图是二级索引在page 上的一个简化结构.&lt;/p&gt;

&lt;p&gt;红色表示record 已经被删除, 蓝色表示未被删除.&lt;/p&gt;

&lt;p&gt;那么如果像官方一样把next-key lock 改成 record lock 以后, 如果这个时候插入两个record (99, 13000), (120, 13000).&lt;/p&gt;

&lt;p&gt;第一个record 在unique check 的时候对 (13000, 100), (13000, 102), (13000, 108)..(13000, 112) 所有的二级索引加record S lock, insert 的时候对 (13000, 100) 加GAP | insert_intention lock.&lt;/p&gt;

&lt;p&gt;第二个 record 在unique check 的时候对(13000, 100), (13000, 102), (13000, 108)..(13000, 112) 所有的二级索引加record S lock. insert 的时候对 (13000, 112)加 GAP | inser_intention lock.&lt;/p&gt;

&lt;p&gt;那么这时候这两个record 都可以同时插入成功, 就造成了unique key 约束失效了.&lt;/p&gt;

&lt;p&gt;具体的mtr case 可以看&lt;a href=&quot;https://bugs.mysql.com/bug.php?id=68021&quot;&gt;bug#68021&lt;/a&gt; 上面我写的mtr.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/uPic/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E4%BB%B6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那官方打算如何修复这个问题呢?&lt;/p&gt;

&lt;p&gt;主要是两个思路&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;我们知道InnoDB 的lock 必须遵守2PL 的原则, 也就是这个原因这里next-key lock 用于做Unique check 判断完成以后不能马上释放, 必须等到事务结束才能够释放. 因此官方希望区分真正的用于事务的lock 和 用于Unique check 的lock, 这两种类型的lock 的生命周期应该是不一样的, 前者需要等到事务结束才能够释放, 后者可以在当前STATEMENT 结束以后就可以释放, 当然就像issue 里面Fungo 提出理论上应该昨晚unique check 判断以后就马上释放, STATEMENT这个生命周期还是太大了, 如果在insert into values 场景, 前面的insert 还是影响到后面的insert 了.&lt;/p&gt;

    &lt;p&gt;官方已经在一些地方增加了 lock_duation_t::AT_LEAST_STATEMENT 这个类型, 但是这里麻烦的地方在于InnoDB 的lock 还存在锁继承和锁复用, 比如当前需要申请一个GAP lock 的时候, 当前事务因为unique check 已经有了该GAP lock, 那么这次申请直接返回ture 了, 因为当前的实现默认是所有的lock 都在事务提交的时候一起释放. 但是现在如果unique check 申请的GAP lock 提前释放了, 那么这里就冲突了, 因此锁复用的时候就也需要考虑声明周期了.&lt;/p&gt;

    &lt;p&gt;另外就是锁继承, 如果在gap 中间有record 被purge 或者插入了一个新的record, 那么就继承了一个生命周期是STATEMENT 的场景, unique check 引入的GAP lock 释放的时候该lock 也要释放.&lt;/p&gt;

    &lt;p&gt;这些问题都非常的细碎, 所以官方也在慢慢的修复之中&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;另外一个思路是通过latch 来做unique check 而不是lock. 我们知道latch 的生命周期远远小于lock, 通常来说latch=short-lived, lock=long-lived, 可以在mtr 提交的时候就可以释放.&lt;/p&gt;

    &lt;p&gt;但是带来的问题是, 如果有大量的delete-marked record, 那么就会覆盖到多个page, 那么mtr 持有的latch 就会很多, 我们知道mtr 是InnoDB Btree 修改的最小单位, 如果mtr 持有的page latch 过多, 那么Btree 的并发性能是必然下降的.&lt;/p&gt;

    &lt;p&gt;另外因为Undo purge 等等操作需要持有page latch 进行, 那么可能造成持有Page latch 的过程中是进行IO 操作, 那么持有latch 的时间肯定较长, 造成unique-check 判断时间过长. 对于latch 的冲突和lock 的冲突处理方式完全不一样, latch 冲突是当前线程等待的方式, lock 冲突以后, 当前事务会进入到事务锁等待, 等冲突的lock 释放以后重新唤醒的过程. 具体可以看&lt;a href=&quot;https://15721.courses.cs.cmu.edu/spring2019/papers/06-indexes/a16-graefe.pdf&quot;&gt;Goetz 的文章&lt;/a&gt;.&lt;/p&gt;

    &lt;p&gt;其实这也是Fungo 在issue 里面回复的PostgreSQL 的做法, PostgreSQL 在做unique check 的时候对于第一个page 是X latch, 后面的page 通过latch coupling 进行page latch 的持有和释放.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;我在 issue 里面也提出我的改法.&lt;/p&gt;

&lt;p&gt;在row_ins_scan_sec_index_for_duplicate() 函数里面将next_key lock 改成record lock, 然后在insert 阶段, 通过将 insert 时候申请的&lt;/p&gt;

&lt;p&gt;LOCK_X | LOCK_GAP | LOCK_INSERT_INTENTION;  改成 =&amp;gt;&lt;/p&gt;

&lt;p&gt;LOCK_X | LOCK_ORDINARY | LOCK_INSERT_INTENTION;&lt;/p&gt;

&lt;p&gt;那么就变成持有record lock, 等待LOCK_ORDINARY | LOCK_INSERT_INTENTION, 那么session2/session3 就会互相冲突, 那么就无法同时插入..&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;insert 的时候为什么要持有LOCK_GAP 而不是 LOCK_ORDINARY ?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;比如原有record 1, 4, 10 需要插入record 6, 7&lt;/p&gt;

&lt;p&gt;那么trx 去抢的都是record 10 的lock, 因为此时6, 7 都还未在btree 中, record 10 是next record..如果加的是10 上面的 LOCK_ORDINARY, 那么两个非常简单的insert 6, 7 就会互相等待死锁了..&lt;/p&gt;

&lt;p&gt;因此只能加LOCK_GAP.&lt;/p&gt;

&lt;p&gt;但是这里对于有可能冲突的SK, 互相死锁则是想要的, 比如如果现有的record&lt;/p&gt;

&lt;p&gt;&amp;lt;1, 1&amp;gt;, &amp;lt;4, 2&amp;gt;, &amp;lt;10(delete-mark), 3&amp;gt;, &amp;lt;10(d), 8&amp;gt;, &amp;lt;10(d), 11&amp;gt;, &amp;lt;10(d), 21&amp;gt;, &amp;lt;15, 9&amp;gt;  需要插入&lt;/p&gt;

&lt;p&gt;trx1: &amp;lt;10, 6&amp;gt;,  trx2: &amp;lt;10,7&amp;gt;&lt;/p&gt;

&lt;p&gt;trx1 先插入成功, 然后是trx2.&lt;/p&gt;

&lt;p&gt;第一步的时候给 &amp;lt;10, 3&amp;gt;&amp;lt;10,8&amp;gt;&amp;lt;10,11&amp;gt;&amp;lt;10,21&amp;gt; 加record s lock.&lt;/p&gt;

&lt;p&gt;插入的时候判断 插入的位置在&amp;lt;10,3&amp;gt;&amp;lt;10,8&amp;gt; 之间, 有10, 那么就可以申请的时候 &amp;lt;10, 8&amp;gt; 的 LOCK_X | LOCK_ORDINARY | insert_intention,   和已经持有record s lock 互相冲突, 已经是死锁了&lt;/p&gt;

&lt;p&gt;如果插入&amp;lt;10,6&amp;gt;&amp;lt;10,9&amp;gt; 也一样&lt;/p&gt;

&lt;p&gt;第一步给所有&amp;lt;10, x&amp;gt; 都加record s lock&lt;/p&gt;

&lt;p&gt;插入的时候,  trx1 申请&amp;lt;10, 8&amp;gt; LOCK_ORDINARY, 持有trx2 想要的&amp;lt;10, 11&amp;gt; record s lock, trx 申请&amp;lt;10, 11&amp;gt; LOCK_X | LOCK_ORDINARY, 持有trx1 想要的&amp;lt;10, 8&amp;gt; record s lock 因此也是互相死锁冲突的.&lt;/p&gt;

&lt;p&gt;最后再拓展一下, primary key 也是unique key index, 为什么primary key 没有这个问题?&lt;/p&gt;

&lt;p&gt;本质原因是在secondary index 里面, 由于mvcc 的存在, 当删除了一个record 以后, 只是把对应的record delete marked, 在插入一个新的record 的时候, delete marked record 是保留的.&lt;/p&gt;

&lt;p&gt;在primary index 里面, 在delete 之后又insert 一个数据, 会将该record delete marked 标记改成non-delete marked, 然后记录一个delete marked 的record 在undo log 里面, 这样如果有历史版本的查询, 会通过mvcc 从undo log 中恢复该数据. 因此不会出现相同的delete mark record 跨多个page 的情况, 也就不会出现上述case 里面(13000, 100) 在page1, (13000, 112) 在page3.&lt;/p&gt;

&lt;p&gt;那么在insert 的时候, 和上面的二级索引插入2阶段类似, 需要有latch 或者lock 进行保护, 这里primary index 通过持有page X latch 就可以保证两个阶段的原子性, 从而两次的insert 不可能同时插入成功, 进而避免了这个问题.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在delete + insert, insert … on duplicate key update, replace into 等场景中, 为了实现判断插入记录与现有物理记录是否冲突和插入记录这两个阶段的原子, unique check 的时候会给所有的相同的record 和下一个record 加上next-key lock. 导致后续insert record 虽然没有冲突, 但是还是会被Block 住, 进而有可能造成死锁的问题.&lt;/p&gt;

</description>
        <pubDate>Fri, 22 Apr 2022 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2022/04/22/unique-key-check/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/04/22/unique-key-check/</guid>
      </item>
    
      <item>
        <title>Innodb Physiological Logging</title>
        <description>&lt;p&gt;为什么InnoDB 的redo log 是&lt;strong&gt;Physiological logging&lt;/strong&gt;?&lt;/p&gt;

&lt;p&gt;有一个存储的同学来问, 如果redo log 是纯physical log 的话, 那么就可以省去double write buffer 的开销, 保证每一次修改都是在4kb以内(由操作系统保证4kb以内的原子操作), 那么就不存在应用redo 到不新不旧的page 上的问题, 就不需要double write buffer.&lt;/p&gt;

&lt;p&gt;目前主要有两种Logical logging and Physical logging.&lt;/p&gt;

&lt;p&gt;Logical logging 像Binlog 这种, 记录的是操作, 跟物理格式无关, 所以通过binlog 可以对接不同的存储引擎.&lt;/p&gt;

&lt;p&gt;Physical logging 是纯物理格式, byte for byte 的记录数据的改动, 比如 [start, end, ‘xxxxx’] 这样的格式内容改动.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Physical logging&lt;/strong&gt; 的优点是高效率, 并且可以直接修改物理格式, 任何操作都不需要重新遍历btree 到指定page.&lt;/p&gt;

&lt;p&gt;但是缺点也很明显, 记录的内容非常冗余, 比如一次delete 操作, logical logging 只需要记录MLOG_COMP_REC_DELETE offset 就可以, 实际执行的过程中会修改prev record-&amp;gt;next_record, next_record-&amp;gt;prev_record, checksum, PAGE_DIR_SLOT_MIN_N_OWNED, 可能还需要更新dir slot 信息等等. 如果改成physical logging 那么这些信息涉及到的内容在page 不同位置, 那么需要记录的日志就非常多了.&lt;/p&gt;

&lt;p&gt;另外在page reorgnize 或者 SMO 场景需要记录大量的无用日志, 比如当一个page 内部有过大量的删除, 有碎片需要整理的时候, 因为需要重新组织page结构, 如果physical logging 那么就需要一个一个record 重新insert 到当前page, offset 需要重新记录, 而logical logging 就只需要记录MLOG_PAGE_REORGANIZE 就可以了. 对比一下16kb 的page 只需要记录几个字节, 而physical logging 需要写差不多16kb 的内容了.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Logical logging&lt;/strong&gt; 的优点是记录非常高效, 如上面说的delete 操作, 只需要记录几个字节, 在SMO, page reorgnize 等场景更加明显.&lt;/p&gt;

&lt;p&gt;但是最大的缺点也很明显, 因为记录的是record_id, 那么所有改动就需要重新遍历btree, 因为都需要对btree 进行修改, 那么就得走加index lock, 串行修改的逻辑. 而物理日志因为page 之前完全没有依赖, 那么就可以并行回放, 这样crash recovery 的效率最高的.&lt;/p&gt;

&lt;p&gt;在&lt;a href=&quot;./https://cs.stanford.edu/people/chrismre/cs345/rl/aries.pdf&quot;&gt;ARIES&lt;/a&gt; 文章之后, 大部分的商业数据库选择的是”Physiological Logging”,  也就是”physical to a page, logical within a page.”, InnoDB 也是这样, 尽可能将Logocal logging 和 Physical logging 的优点结合在一起.&lt;/p&gt;

&lt;p&gt;记录的redo log 的格式是操作类型, 有些操作类型需要修改record 的话会记录offset. 大量的操作是一些逻辑操作, 比如 MLOG_1BYTE/MLOG_2BYTE/MLOG_INIT_FILE_PAGE 等等.&lt;/p&gt;

&lt;p&gt;对于insert/update/delete 等等操作可以保证到记录到page level, 那么在crash recovery 的时候, 就可以并行的回放日志不需要重新执行btree 遍历找到page逻辑, 从而加快crash recovery.&lt;/p&gt;

&lt;p&gt;当然现在InnoDB 的日志还有一些冗余的地方, PolarDB 也做了一些改进, 比如增加了record 长度信息, 减少了连续mtr 里面page id 记录等等, MariaDB Marko 也一直在优化这块 &lt;a href=&quot;./https://jira.mariadb.org/browse/MDEV-12353&quot;&gt;MDEV-12353&lt;/a&gt;. 整体而言都是为了在page 内部的Logical redo 尽可能高效并且减少冗余.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;C. Mohan, Don Handerle&lt;strong&gt;.&lt;/strong&gt; ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging.&lt;/li&gt;
  &lt;li&gt;C. Mohan, Frank Levine. ARIES/lM: An Efficient and High Concurrency index Management Method Using Write-Ahead Logging.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 10 Apr 2022 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2022/04/10/physiological-log/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/04/10/physiological-log/</guid>
      </item>
    
      <item>
        <title>MySQL InnoDB space file</title>
        <description>&lt;p&gt;InnoDB 最后的数据都会落到文件中.&lt;/p&gt;

&lt;p&gt;整体而言InnoDB 里面除了redo log 以外都使用统一的结构进行管理, 包括system tablespace(ibdata1), user tablespace(用户表空间), undo log, temp tablespace. 这个结构我们统称space file.&lt;/p&gt;

&lt;p&gt;接下来会4篇文章介绍InnoDB 主要的从文件, page, index, record 在具体文件里面是如何分布的, 这里大量引用了Jeremy Cole 里面的图片和文章的内容.&lt;/p&gt;

&lt;p&gt;同时介绍的过程会结合inno_space 工具直观的打印出文件的内部结构.&lt;/p&gt;

&lt;p&gt;什么是inno_space?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;./https://github.com/baotiao/inno_space&quot;&gt;inno_space &lt;/a&gt; 是一个可以直接访问InnoDB 内部文件的命令行工具,  可以打印出文件的内部结构.&lt;/p&gt;

&lt;p&gt;Jeremy Cole 用ruby 写了一个类似的工具, 不过不支持MySQL 8.0, 并且ruby 编译以及改动起来特别麻烦, 所以用cpp 重写了一个. inno_space 做到不依赖任何外部文件, 只需要make, 就可以得到可执行文件, 做到开箱即用.&lt;/p&gt;

&lt;p&gt;inno_space 除了支持打印出文件的具体结构之外, 同时还支持修复 corrupt page 功能, 如果遇到InnoDB 表文件中的page 损坏, 实例无法启动的情况, 如果损坏的只是leaf page, inno_space 可以将corrupt page 跳过, 从而保证实例能够启动, 并且将绝大部分的数据找回.&lt;/p&gt;

&lt;p&gt;inno_space 还提供分析表文件中的数据情况, 是否有过多的free page, 从而给用户建议是否需要执行 optimize table 等等&lt;/p&gt;

&lt;p&gt;具体可以看代码, 在github 上面开源: https://github.com/baotiao/inno_space/commits/main&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;InnoDB space file 也就是整个InnoDB 文件系统的管理, 介绍.ibd 文件的基础结构. &lt;a href=&quot;./InnoDB space file.md&quot;&gt;InnoDB space file&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;InnoDB page management  具体的在InnoDB file space 这些16kb 大小的page 是如何管理的 &lt;a href=&quot;./InnoDB page management.md&quot;&gt;Page management&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;InnoDB Index page 上面讲了这16kb 的page 如何管理, 那么我们细看一下最常见的page 类型, Index Page 存的是用户表空间的数据,  这些Index Page 是如何维护成一个table 的数据 &lt;a href=&quot;./InnoDB Index page.md&quot;&gt;Index page&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;InnoDB record 是具体在InnoDB page 里面, Mysql 里面的record 是如何保存在InnoDB page 里面的 &lt;a href=&quot;./InnoDB record.md&quot;&gt;InnoDB record&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这篇文章只描述InnoDB file space, 接下来会有文章介绍InnoDB page management,  InnoDB page, InnoDB record&lt;/p&gt;

&lt;h4 id=&quot;1-innodb-space-file-基本结构&quot;&gt;1. InnoDB space file 基本结构&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Page&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在InnoDB 里面, 16kb 大小的page 是最小的原子单元&lt;/p&gt;

&lt;p&gt;其他的大小都是在page 之上, 因此有:&lt;/p&gt;

&lt;p&gt;1 page = 16kB = 16384 bytes&lt;/p&gt;

&lt;p&gt;1 extent = 64 pages = 1 MB&lt;/p&gt;

&lt;p&gt;FSP_HDR  page = 256 extents = 16384 pages = 256 MB&lt;/p&gt;

&lt;p&gt;page 有最基础的38字节的 FIL Header, 8字节的FIL Trailer&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/NxR6eb3.jpg&quot; alt=&quot;Imgur&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;主要的内容包括:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Checksum: 这个page 的checksum, 用来判断page 是否有corrupt&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Page Number: Page Number 可以计算出在文件上的偏移量, 一个page 是否初始化了, 也可以看这个page number 是否设置对了, 这个值其实是冗余的, 根据file offset 可以算出来, 所以这个值是否正确, 就可以知道这个page 是否被初始化了&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Previous Page/Next Page: 这个只有在Index page 的时候才有用, 而且只有leaf page 的时候才有用, non-leaf page 是没用的, 大部分类型的page 并没有使用这个字段.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LSN for last page modification: 刷脏的时候, 写入这个page 的 newest_modification_lsn&lt;/p&gt;

    &lt;p&gt;​	mach_write_to_8(page + FIL_PAGE_LSN, newest_lsn);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Page Type: 这个page 具体的类型, 比如是btree index leaf-page, undo log page,  btree index non-leaf page, insert buffer, fresh allocated page, 属于ibdata1 的system page 等等. Page Type 最重要, 决定这个page 的用途类型, 里面很多字段就不一样了&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Flush LSN:  保存的是已经flush 到磁盘的page 的最大lsn 信息. 只有在space 0 page 0 这个page 里面有用, 其他地方都没用.. 什么用途?什么时候写入? 什么时候读取?&lt;/p&gt;

    &lt;p&gt;在进行shutdown 的时候, 或者执行force checkpoint的时候通过 fil_write_flushed_lsn_to_data_files 写入.&lt;/p&gt;

    &lt;p&gt;用途是在启动的时候, 读取这个flush lsn, 可以确保这个lsn 之前的page 已经刷到磁盘了, 从这个flush lsn 之后的redo log 才是uncheckpoint redo log, 但是其实redo log 里面已经有了 checkpoint 的信息了, 为何还需要这个字段?&lt;/p&gt;

    &lt;p&gt;logs_empty_and_mark_files_at_shutdown =&amp;gt;&lt;/p&gt;

    &lt;p&gt;在实例启动的时候, innobase_start_or_create_for_mysql =&amp;gt; open_or_create_data_files =&amp;gt; fil_read_first_page&lt;/p&gt;

    &lt;p&gt;fil_read_first_page 里面会读取出这个lsn 信息, 用于更新启动的时候的 min_flushed_lsn, max_flushed_lsn. 因为这个时候redo log 模块还没有初始化,  可以拿这个两个Lsn 做一些简单的判断&lt;/p&gt;

    &lt;p&gt;整体来看, 这个字段目前已经没啥用了, 但是每一个page 都占用了8字节的空间, 还是比较浪费, 可以充分复用&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Space ID: 当前Page 所属space ID (8.0 里面已经将该字段删除了)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;通过inno_space 可以看到相应的结构:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./inno -f ~/git/primary/dbs2250/sbtest/sbtest1.ibd -p 10

==========================block==========================
FIL Header:
CheckSum: 2065869235
Page number: 10
Previous Page: 9
Next Page: 11
Page LSN: 554513658770
Page Type: 17855
Flush LSN: 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Space file&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;一个space file 就是2^32 个page 的合集, 连续64个page 叫做extent, 256个连续的extent 会有一个XDES(extent descriptor) 进行管理, 第一个XDES 又叫做FSP_HDR, 还有一些额外的信息.&lt;/p&gt;

&lt;p&gt;下图就是这个基本文件组织结构的描述, 无论是undo space, system space, 用户的table space 都是这样结构&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../Library/Application Support/typora-user-images/image-20211118052832966.png&quot; alt=&quot;image-20211118052832966&quot; style=&quot;zoom:40%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;所有的space file 前3个page 都是一样.&lt;/p&gt;

&lt;p&gt;page 0 是 FSP_HDR(file space header)&lt;/p&gt;

&lt;p&gt;page 1 是 insert buffer bitmap&lt;/p&gt;

&lt;p&gt;page 2 是 inode page, 下一节会介绍&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The system space&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;system space 的space id = 0, 文件名叫 ibdata1, 也就是系统文件.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/l3UHMqR.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;page 0, 1, 2 这3个page 所有的space file 都一样&lt;/p&gt;

&lt;p&gt;在system space 里面接下来的3, 4, 5 等等page 也都是有指定的用途&lt;/p&gt;

&lt;p&gt;page 3 存放的是insert buffer 相关信息&lt;/p&gt;

&lt;p&gt;page 4 存放的是insert buffer tree 的root page&lt;/p&gt;

&lt;p&gt;page 5 存放的是trx_sys 模块相关信息, 比如最新的trx id, binlog 信息等等.&lt;/p&gt;

&lt;p&gt;page 6 存放的是FSP_FIRST_RSEG_PAGE_NO, 也就是undo log rollback segment的header page. 其他的undo log rollback segment 都在不同的undo log 文件中&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/ScLs3Oj.jpg&quot; alt=&quot;Imgur&quot; style=&quot;zoom:40%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;page 7 存放的是 FSP_DICT_HDR_PAGE_NO, 存放的是DD 相关的信息&lt;/p&gt;

&lt;p&gt;page 64-127 是first 64 个double write buffer 的位置&lt;/p&gt;

&lt;p&gt;page 128-191 是second 64个double write buffer 的位置&lt;/p&gt;

&lt;p&gt;剩下的其他page 就有可能被申请成Undo log page 等等了&lt;/p&gt;

&lt;p&gt;通过inno_space 打开 ibdata1文件可以观察到如下的信息&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;File path /home/zongzhi.czz/git/primary/log2250/ibdata1 path
File size 209715200
start           end             count           type
0               0               1               FSP HDR
1               1               1               INSERT BUFFER BITMAP
2               2               1               INDEX NODE PAGE
3               3               1               SYSTEM PAGE
4               4               1               INDEX PAGE
5               5               1               TRX SYSTEM PAGE
6               7               2               SYSTEM PAGE
8               8               1               SDI INDEX PAGE
9               12799           12790           FRESHLY ALLOCATED PAGE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;打开一个普通的用户表空间, 可以看到如下的结构.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;└─[$] ./inno -f ~/git/primary/dbs2250/sbtest/sbtest1.ibd -c list-page-type
File path /home/zongzhi.czz/git/primary/dbs2250/sbtest/sbtest1.ibd path, page num 0
page num 0
==========================space page type==========================
File size 2604662784
start           end             count           type
0               0               1               FSP HDR
1               1               1               INSERT BUFFER BITMAP
2               2               1               INDEX NODE PAGE
3               3               1               SDI INDEX PAGE
4               16383           16380           INDEX PAGE
16384           16384           1               XDES
16385           16385           1               INSERT BUFFER BITMAP
16386           31990           15605           INDEX PAGE
31991           31999           9               FRESHLY ALLOCATED PAGE
32000           32767           768             INDEX PAGE
32768           32768           1               XDES
32769           32769           1               INSERT BUFFER BITMAP
32770           49151           16382           INDEX PAGE
49152           49152           1               XDES
49153           49153           1               INSERT BUFFER BITMAP
49154           65535           16382           INDEX PAGE
65536           65536           1               XDES
65537           65537           1               INSERT BUFFER BITMAP
65538           81919           16382           INDEX PAGE
81920           81920           1               XDES
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;下一篇物理页管理我们会更详细的介绍.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;File Per Table&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;InnoDB 常见的file per table 模式下. 一个table 对应一个.ibd 文件.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/I2vFSGn.png&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;page 0, 1, 2 这3个page 所有的space file 都一样&lt;/p&gt;

&lt;p&gt;page 3 一般是 primary index root page.&lt;/p&gt;

&lt;p&gt;page 4 一般是 secondary index root page. 当然这里是create table 就指定的时候, 比如如下 page 4 一般是k_1 这个index 的root page&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mysql&quot;&gt;Create Table: CREATE TABLE `sbtest1` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `k` int(11) NOT NULL DEFAULT &apos;0&apos;,
  `c` char(120) NOT NULL DEFAULT &apos;&apos;,
  `pad` char(60) NOT NULL DEFAULT &apos;&apos;,
  PRIMARY KEY (`id`),
  KEY `k_1` (`k`)
) ENGINE=InnoDB AUTO_INCREMENT=237723 DEFAULT CHARSET=latin1
1 row in set (0.00 sec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果后面运行过程中再加的新的 secondary index, 新的Index的root page 那就不会是连续着的, 而是分散在其他page 上了&lt;/p&gt;

&lt;p&gt;alter table sbtest1 add index idx_c(c);&lt;/p&gt;

&lt;p&gt;比如执行alter table 以后, 额外增加的一个index, 通过inno_space 工具可以看到每一个index 的root page 所在等等&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Example 2:
./inno -f ~/git/primary/dbs2250/sbtest/sbtest1.ibd -c index-summary
File path /home/zongzhi.czz/git/primary/dbs2250/sbtest/sbtest1.ibd path, page num 0
==========================Space Header==========================
Space ID: 15
Highest Page number: 158976
Free limit Page Number: 152256
FREE_FRAG page number: 24
Next Seg ID: 7
File size 2604662784
========Primary index========
Primary index root page space_id 15 page_no 4
Btree hight: 2
&amp;lt;&amp;lt;&amp;lt;Leaf page segment&amp;gt;&amp;gt;&amp;gt;
SEGMENT id 4, space id 15
Extents information:
FULL extent list size 2140
FREE extent list size 0
PARTIALLY FREE extent list size 1
Pages information:
Reserved page num: 137056
Used page num: 137003
Free page num: 53

&amp;lt;&amp;lt;&amp;lt;Non-Leaf page segment&amp;gt;&amp;gt;&amp;gt;
SEGMENT id 3, space id 15
Extents information:
FULL extent list size 1
FREE extent list size 0
PARTIALLY FREE extent list size 1
Pages information:
Reserved page num: 160
Used page num: 116
Free page num: 44

========Secondary index========
Secondary index root page space_id 15 page_no 31940
Btree hight: 2
&amp;lt;&amp;lt;&amp;lt;Leaf page segment&amp;gt;&amp;gt;&amp;gt;
SEGMENT id 6, space id 15
Extents information:
FULL extent list size 7
FREE extent list size 0
PARTIALLY FREE extent list size 219
Pages information:
Reserved page num: 14465
Used page num: 12160
Free page num: 2305

&amp;lt;&amp;lt;&amp;lt;Non-Leaf page segment&amp;gt;&amp;gt;&amp;gt;
SEGMENT id 5, space id 15
Extents information:
FULL extent list size 0
FREE extent list size 0
PARTIALLY FREE extent list size 0
Pages information:
Reserved page num: 19
Used page num: 19
Free page num: 0

**Suggestion**
File size 2604662784, reserved but not used space 39354368, percentage 1.51%
Optimize table will get new fie size 2565308416

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;这里tablespace id 是15&lt;/li&gt;
  &lt;li&gt;Btree 的高度是3层&lt;/li&gt;
  &lt;li&gt;secondary Index 由于只存索引, 所以primary index 占用的空间是secondary index 的10倍&lt;/li&gt;
  &lt;li&gt;primary Index 上面大量的page 都是用满的状态, 而secondary 会20% 左右的空闲page&lt;/li&gt;
  &lt;li&gt;整体而言, 空闲page 只占了文件的1.51% 左右, 所以不需要做optimize table 操作的&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Mon, 29 Nov 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/11/29/inno-space/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/11/29/inno-space/</guid>
      </item>
    
      <item>
        <title>MySQL Repeatable-Read 的一些误解</title>
        <description>&lt;h5 id=&quot;背景&quot;&gt;背景&lt;/h5&gt;

&lt;p&gt;首先1992 年发表的SQL Standard 对隔离级别进行的定义是根据几个异象(Dirty Read, Non-Repeatable Read, Phantom Read) , 当然这个定义非常模糊, 后面Jim Grey 也有文章说这个不合理, 然而此时MVCC, snapshot isolation 还没被发明. 等有snapshot isolation 以后发现snapshot isolation 能够规避Dirty Read, Non-Repeatable Read, 因此认为snapshot isolation 和 Repeatable-read 很像, 所以MySQL, Pg 把他们实现的snapshot isolation 就称为了Repeatable-read isolation.&lt;/p&gt;

&lt;p&gt;另外snapshot isolation 其实也没有准确的定义, 因此MySQL 和 PG, Oracle 等等的实现也是有很大的区别的.&lt;/p&gt;

&lt;p&gt;关于&lt;strong&gt;snapshot isolation&lt;/strong&gt; 的定义:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A transaction running in Snapshot Isolation is never blocked attempting a read as long as the snapshot data from its Start-Timestamp can be maintained.The transaction’s writes (updates, inserts, and deletes) will also be reflected in this snapshot, to be read again if the transaction accesses (i.e., reads or updates) the data a second time.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里对于snapshot isolation 的定义不论对于读操作和写操作都是读取snapshot 的版本, 这也是pg, oracle 等等版本实现的, 但是InnoDB 不是这样的. InnoDB 只有读操作读取到的是snapshot 的版本, 但是DML 操作是读取当前已提交的最新版本.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When the transaction T1 is ready to commit, it gets a &lt;em&gt;Commit-Timestamp,&lt;/em&gt; which is larger than any existing Start-Timestamp or Commit-Timestamp. The transaction successfully commits only if no other transaction T2 with a Commit-Timestamp in T1’s &lt;em&gt;execution interval&lt;/em&gt; [&lt;em&gt;Start- Timestamp&lt;/em&gt;, &lt;em&gt;Commit-Timestamp&lt;/em&gt;] wrote data that T1 also wrote. Otherwise, T1 will abort. This feature, called &lt;em&gt;First- committer-wins&lt;/em&gt; prevents lost updates (phenomenon P4).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对于 first-committer-wins 的定义, 在si 模式下, 如果在Start-Timestamp -&amp;gt; Commit-Timestamp 这之间如果有其他的trx2 修改了当前trx1 修改过的内容, 并且在trx1 提交的时候, trx2 已经提交了. 那么trx1 就会abort, 这个叫first-committer-wins.&lt;/p&gt;

&lt;p&gt;但是InnoDB 也不是这样的. InnoDB 并不遵守这个规则, 在repeatable read 模式下, 如果trx1, trx2 都修改了同一行, trx2 是先提交的, 那么trx1 的提交会直接把trx2 覆盖. 而在类似PG, Oracle 实现的snapshot isolation 里面, 则是遵守first-committer-wins 的规则.&lt;/p&gt;

&lt;p&gt;所以InnoDB 的snapshot isolation&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;仅仅Read 操作读的是历史版本&lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;不遵守first-committer-wins 规则&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;官方把这种实现叫做&lt;strong&gt;Write committed Repeatable Read&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;MySQL 开发者对于InnoDB repeatable-read 实现的介绍:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;But when InnoDB Repeatable Read transactions modify the database, it is possible to get phantom reads added into the static view of the database, just as the ANSI description allows.  Moreover, InnoDB relaxes the ANSI description for Repeatable Read isolation in that it will also allow non-repeatable reads during an UPDATE or DELETE.  Specifically, it will write to newly committed records within its read view.  And because of gap locking, it will actually wait on other transactions that have pending records that may become committed within its read view.  So not only is an UPDATE or DELETE affected by pending or newly committed records that satisfy the predicate, but also ‘SELECT … LOCK IN SHARE MODE’ and ‘SELECT … FOR UPDATE’.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;This WRITE COMMITTED implementation of REPEATABLE READ is not typical of any other database that I am aware of.  But it has some real advantages over a standard ‘Snapshot’ isolation.  When an update conflict would occur in other database engines that implement a snapshot isolation for Repeatable Read, an error message would typically say that you need to restart your transaction in order to see the current data. So the normal activity would be to restart the entire transaction and do the same changes over again.  But InnoDB allows you to just keep going with the current transaction by waiting on other records which might join your view of the data and including them on the fly when the UPDATE or DELETE is done.  This WRITE COMMITTED implementation combined with implicit record and gap locking actually adds a serializable component to Repeatable Read isolation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PG 社区对于repeatable-read 实现的介绍:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UPDATE&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DELETE&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SELECT FOR UPDATE&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SELECT FOR SHARE&lt;/code&gt; commands behave the same as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SELECT&lt;/code&gt; in terms of searching for target rows: they will only find target rows that were committed as of the transaction start time. However, such a target row might have already been updated (or deleted or locked) by another concurrent transaction by the time it is found. In this case, the repeatable read transaction will wait for the first updating transaction to commit or roll back (if it is still in progress). If the first updater rolls back, then its effects are negated and the repeatable read transaction can proceed with updating the originally found row. But if the first updater commits (and actually updated or deleted the row, not just locked it) then the repeatable read transaction will be rolled back with the message&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;https://www.postgresql.org/docs/13/transaction-iso.html#XACT-READ-COMMITTED&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所以这里我们看一下MySQL repeatable-read 的具体行为, 也了解MySQL社区为什么要做这样的实现.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mysql&quot;&gt;mysql&amp;gt; create table checking (name char(20) key, balance int) engine InnoDB;
Query OK, 0 rows affected (0.03 sec)

mysql&amp;gt; insert into checking values (&quot;Tom&quot;, 1000), (&quot;Dick&quot;, 2000), (&quot;John&quot;, 1500);
Query OK, 3 rows affected (0.00 sec)
Records: 3  Duplicates: 0  Warnings: 0

Client #1                               Client #2
=====================================   =====================================
mysql&amp;gt; begin;
Query OK, 0 rows affected (0.00 sec)

mysql&amp;gt; select * from checking;
+------+---------+
| name | balance |
+------+---------+
| Dick |    2000 |
| John |    1500 |
| Tom  |    1000 |
+------+---------+
3 rows in set (0.00 sec)

mysql&amp;gt; begin;
Query OK, 0 rows affected (0.00 sec)

mysql&amp;gt; update checking
   set balance = balance - 250
   where name = &quot;Dick&quot;;
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql&amp;gt; update checking
   set balance = balance + 250
   where name = &quot;Tom&quot;;
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql&amp;gt; select * from checking;
+------+---------+
| name | balance |
+------+---------+
| Dick |    1750 |
| John |    1500 |
| Tom  |    1250 |
+------+---------+
3 rows in set (0.02 sec)
                                        mysql&amp;gt; begin;
                                        Query OK, 0 rows affected (0.00 sec)

                                        mysql&amp;gt; select * from checking;
                                        +------+---------+
                                        | name | balance |
                                        +------+---------+
                                        | Dick |    2000 |
                                        | John |    1500 |
                                        | Tom  |    1000 |
                                        +------+---------+
                                        3 rows in set (0.00 sec)
																				
                                        mysql&amp;gt; update checking
                                           set balance = balance - 200
                                           where name = &quot;John&quot;;
                                        Query OK, 1 row affected (0.00 sec)
                                        Rows matched: 1  Changed: 1  Warnings: 0
																				
                                        mysql&amp;gt; update checking
                                           set balance = balance + 200
                                           where name = &quot;Tom&quot;;

                                        ### Client 2 waits on the locked record
mysql&amp;gt; commit;
Query OK, 0 rows affected (0.00 sec)
                                        Query OK, 1 row affected (19.34 sec)
                                        Rows matched: 1  Changed: 1  Warnings: 0
mysql&amp;gt; select * from checking;
+------+---------+
| name | balance |
+------+---------+
| Dick |    1750 |
| John |    1500 |
| Tom  |    1250 |
+------+---------+
3 rows in set (0.00 sec)
                                        mysql&amp;gt; select * from checking;
                                        +------+---------+
                                        | name | balance |
                                        +------+---------+
                                        | Dick |    2000 |
                                        | John |    1300 | 
                                        | Tom  |    1450 |
                                        +------+---------+
                                        3 rows in set (0.00 sec)

                                      # 这里可以看到Tom = 1450, 而不是从上面 1000 + 200 = 1200, 
                                      # 因为update 的时候, InnoDB 实现的是write-committed repeatable, 
                                      # 不是基于场景的snapshot isolation的实现, 
                                      # write 操作是直接读取的已提交的最新版本的数据1250, 
                                      # 而不是snapshot 中的数据1000.
																				
                                        mysql&amp;gt; commit;
                                        Query OK, 0 rows affected (0.00 sec)

mysql&amp;gt; select * from checking;
+------+---------+
| name | balance |
+------+---------+
| Dick |    1750 |
| John |    1300 |
| Tom  |    1450 |
+------+---------+
3 rows in set (0.02 sec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里可以看到Tom = 1450, 而不是从上面 1000 + 200 = 1200, 因为update 的时候, InnoDB 实现的是write-committed repeatable, 不是基于场景的snapshot isolation的实现, write 操作是直接读取的已提交的最新版本的数据1250, 而不是snapshot 中的数据1000.&lt;/p&gt;

&lt;p&gt;对比在PG里面, 由于PG是使用常见的 snapshot isolation 实现repeatable-read, 那么trx2 在修改Tom 的时候, 同样必须等待trx1 commit or rollback, 因为PG 读取和修改基于trx 开始时候的snapshot 的record. 因此如果trx1 rollback, 那么trx2 则会基于开始snapshot 时候的值进行修改, 也就是Tom = 1200, 如果trx1 commit, 那么trx2 只能rollback, 并且会返回&lt;/p&gt;

&lt;p&gt;ERROR:  could not serialize access due to concurrent update&lt;/p&gt;

&lt;p&gt;也就是在上面的场景下 trx2 是会rollback.&lt;/p&gt;

&lt;p&gt;那么MySQL 为什么要这么做呢?&lt;/p&gt;

&lt;p&gt;MySQL 社区的观点是在常见的通过snapshot isolation 来实现repeatable Read 的方案里面, 经常会出现如果两个事务修改了同一个record, 那么就需要后提交的事务重试这个流程. 这种在小事务场景是可以接受的, 但是如果后提交的事务是大事务, 比如trx1 修改了1个record rec1并先提交了, 但是trx2 修改了100 行, 正好包含了rec1, 那么常见的snapshot isolation 的实现就需要trx2 返回错误, 然后重新执行这个事务. 这样对冲突多的场景是特别不友好的.&lt;/p&gt;

&lt;p&gt;但是Innodb 的实现则在修改rec1 的时候, 如果trx1 已经提交了, 那么直接读取trx1 committed 的结果, 这样就可以避免了让trx2 重试的过程了. 也可以达到几乎一样的效果.&lt;/p&gt;

&lt;p&gt;当然这个仅仅MySQL InnoDB 是这样的实现, 其他的数据库都不会这样.&lt;/p&gt;

&lt;p&gt;两种方案都有优缺点吧, 基于常见SI(snapshot isolation) 实现会存在更多的事务回滚, 一旦两个事务修改了同一个row, 那么必然有一个事务需要回滚, 但是InnoDB 的行为可以允许和其他trx 修改同一个record, 并且可以在其他trx 修改后的结果上进行更新, 不需要进行事务回滚, 效率会更高一些, 但是基于常见的snapshot isolation 的实现更符合直观感受.&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Aug 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/08/20/innodb-snapshot-isolation/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/08/20/innodb-snapshot-isolation/</guid>
      </item>
    
      <item>
        <title>路在脚下, 从BTree 到Polar Index</title>
        <description>&lt;p&gt;上一篇文章&lt;a href=&quot;https://zhuanlan.zhihu.com/p/151397269&quot;&gt;InnoDB BTree latch 优化历程&lt;/a&gt; 介绍了 InnoDB 的BTree latch 的优化历程, 我们知道在InnoDB 里面, 依然有一个全局的index latch, 由于全局的index latch 存在会导致同一时刻在Btree 中只有一个SMO 能够发生, index latch 依然会成为全局的瓶颈点, 导致在大批量插入场景, 比如TPCC 的场景中, 性能无法提高. 在MySQL 的官方性能测试人员Dimitrick 的&lt;a href=&quot;http://dimitrik.free.fr/blog/posts/mysql-80-tpcc-mystery.html&quot;&gt;MySQL Performance : TPCC “Mystery” [SOLVED]&lt;/a&gt; 中也可以看到, index lock contention 是最大的瓶颈点.&lt;/p&gt;

&lt;p&gt;在这之前, 我们进行了大量的探索和验证, 在这个&lt;a href=&quot;./https://zhuanlan.zhihu.com/p/50112182&quot;&gt;POLARDB · B+树并发控制机制的前世今生&lt;/a&gt; 和 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/50630867&quot;&gt;POLARDB · 敢问路在何方 — 论B+树索引的演进方向&lt;/a&gt; 中,  我们对比了blink-tree, bw-tree, masstree 等等, 其实学术界更多的探索在简单的场景中进行lock free, 多线程, 针对硬件相关的优化, 但是在实际工程中, MySQL 的索引结构已经不是一个简单的Btree, 它是和MySQL 的事务锁模块强绑定, 同时他还需要支持不仅仅是前序遍历, 还需要支持 modify_prev/search_prev, 需要对non-leaf node 进行加锁操作. 因此在MySQL 中的Btree 的修改就不仅仅是涉及到btree 子模块, 还需要涉及undo log, 事务子模块等等.&lt;/p&gt;

&lt;p&gt;因此PolarDB 提出来High Performance Polar Index 解决这个问题, 从而在我们某一个线上业务的实际场景中, 性能能够有3倍的提升, 在TPCC 场景下更是能够有有11倍的性能提升..&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/img/20210521014602.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;那Polar Index 的本质是什么, 如何实现的呢?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先再来回顾一下InnoDB SMO的加锁流程（简化起见，假设本次SMO只需分裂leaf page）：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;对全局index-&amp;gt;lock加SX锁&lt;/li&gt;
  &lt;li&gt;从root page开始，以不加锁的方式向下遍历non-leaf pages至level 2&lt;/li&gt;
  &lt;li&gt;对level 1的non-leaf page加X锁&lt;/li&gt;
  &lt;li&gt;对level 0的leaf page及其left、right page加X锁，完成leaf page的SMO&lt;/li&gt;
  &lt;li&gt;从root page开始，以不加锁的方式向下遍历至leaf page的parent&lt;/li&gt;
  &lt;li&gt;向parent page插入SMO中对应指向new page的nodeptr&lt;/li&gt;
  &lt;li&gt;释放所有锁，SMO结束&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这里可以看到有下面2个瓶颈点：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;对于单个SMO来说，参与SMO的leaf pages及其parent page的X锁会从一开始加着直到SMO结束，这样的加锁粒度有些大，其实SMO也是分层、从下到上依次操作的，如上面流程中：步骤4先在level 0对leaf page做分裂，然后再在步骤5向parent page插入指向new page的nodeptr，但其实在做步骤4的时候没必要先加着parent page X锁，同样再步骤5中也没必要还占着leaf pages X锁，这个问题在级联SMO场景（leaf page分裂引发其路径上多个non-leaf pages分裂）更为明显，这样在读写混合场景下，SMO路径上的读性能会受影响&lt;/li&gt;
  &lt;li&gt;虽然SMO对index-&amp;gt;lock加了SX锁，可以允许其他非SMO操作并发进来，但SX之间还是互斥的，也就是说多个SMO并不能并发，即使它们之间完全没有page交集，这样在高并发大写入压力下（剧烈触发SMO）性能不理想&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;这些瓶颈点在Polar Index是如何解决的?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;原先BTree 之所以需要持有Index latch 的原因是正常的搜索顺序是保证严格的自上而下, 自左向右, 但是SMO 操作由于需要保持对BTree 修改的原子性, 不能让其他线程访问到BTree 的中间状态, 因此需要持有叶子加点去加父节点的latch, 因此SMO 操作出现了自下而上的加锁操作, 在编程实现中, 一旦出现了多个线程无法遵守同一严格的加锁顺序, 那么死锁就无法避免, 为了避免这样的冲突InnoDB 通过将整个BTree index latch, 从而SMO 的时候, 不会有搜索操作进行.&lt;/p&gt;

&lt;p&gt;Polar Index 的核心想法是把 SMO 操作分成了两个阶段.&lt;/p&gt;

&lt;p&gt;在Polar Index 中每一个node 包含有一个link page 指针, 指向他的node.  以及fence key 记录的是link page 的最小值.&lt;/p&gt;

&lt;p&gt;比如split 阶段&lt;/p&gt;

&lt;p&gt;阶段1: 将一个page 进行split 操作,  然后建立一个link 连接在两个page 之间. 下图Polar Index 就是这样的状态&lt;/p&gt;

&lt;p&gt;阶段2: 给父节点添加一个指针, 从父节点指向新创建的page.&lt;/p&gt;

&lt;p&gt;当然还可以有一个阶段3 将两个page 之间的link 指针去掉.&lt;/p&gt;

&lt;p&gt;在Polar Index 中, 阶段1 和阶段2 的中间状态我们也认为是合理状态, 如果这个阶段实例crash, 那么在crash recovery 阶段可以识别当前page 有Link page, 那么会将SMO 的下一个阶段继续完成, 从而保证BTree 的完整性.&lt;/p&gt;

&lt;p&gt;这样带来的优点是在SMO 的过程中, 由于允许中间状态是合法状态, 那么就不需要为了防止出现中间状态的出现而需要持有叶子节点加父节点latch 的过程. 因此就避免的自下而上的加锁操作, 从而就不需要Index latch.&lt;/p&gt;

&lt;p&gt;如下图对比BTree 和 Polar Index.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/img/20210521014742.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在去掉Index latch 之后, 通过latch coupling 从而保证每一次的修改都只需要在btree 的某一层加latch, 从而最大的减少了latch 的粒度.&lt;/p&gt;

&lt;p&gt;如下是具体执行right split 的过程:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/baotiao/bb/main/img/20210521014719.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;带来的收益是:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;降低SMO的page加锁粒度，当前修改哪一层，就只对这一层相关的page加X锁，并且修改完之后立刻放锁再去修改其他层，这样读写并发就上来了。这样的做法要解决的问题就是：&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;对leaf page做完分裂之后，放锁放锁去修改parent，那么已经迁移到new page上的数据怎么被其他线程访问到呢？&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;这里Polar Index采用了类似Blink tree的做法，给分裂的leaf page设置一个high key，这个值为new page上最小的rec，这样如果leaf page放X锁之后，从parent下来的其他读操作检测到这个high key之后，就知道如果要查找的目标rec在当前leaf page没找到并且大于等于high key的话，就去next page（也就是new page）上查找。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;去掉全局index-&amp;gt;lock，正常的读写及SMO不对index-&amp;gt;lock加任何锁，这样写并发就能上来了。不过在具体实现中，不是简单的删掉代码那么容易，要解决去掉它之后各种各样的问题：&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;遍历BTree的加锁方式&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;InnoDB在普通读、写操作时遍历BTree的方式：是从root page开始，将路径上所有non-leaf pages加S锁，然后占着S锁去加目标leaf page的X锁，加到之后释放non-leaf pages的S锁；在SMO是遍历BTree的方式是前面流程中的步骤2。当我们去掉index-&amp;gt;lock，允许多个SMO并发起来，显然SMO的遍历方式是有问题的，因为在第一遍以无锁方式遍历BTree找到所有需要加X锁的page到第二遍遍历真正对这些page加锁之间，可能其他SMO已经修改了BTree结构。所以我们将遍历方式统一改成lock coupling，同时最多占2层page锁，这样做的好处是不管是普通读、写还是SMO操作，在遍历BTree时对non-leaf pages的加锁区间都很小，进一步提高并发&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;除此之外，在具体实现中，还要解决大量问题，比如：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;多个SMO之间有重叠的pages，如何解决冲突，避免死锁&lt;/li&gt;
    &lt;li&gt;对于左分裂、左合并这种右-&amp;gt;左的加锁，如何避免死锁&lt;/li&gt;
    &lt;li&gt;对于non-leaf page删除leftmost rec而触发其parent的级联删除如何处理&lt;/li&gt;
    &lt;li&gt;… …&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在InnoDB 里面, 依然有一个全局的Index latch, 由于全局的Index latch 存在会导致同一时刻在Btree 中只有一个SMO 能够发生, 从而导致性能无法提升.&lt;/p&gt;

&lt;p&gt;Polar Index 通过将SMO 操作分成两个阶段, 并保证中间状态的合理性, 从而避免了Index latch. 从而保证任意时刻在BTree 中只会持有一层latch, 从而实现性能极大提升.&lt;/p&gt;
</description>
        <pubDate>Fri, 21 May 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/05/21/polar-index/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/05/21/polar-index/</guid>
      </item>
    
      <item>
        <title>WorkLog InnoDB Faster truncate/drop table space</title>
        <description>&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在InnoDB 现有的版本里面, 如果一个table space 被truncated 或者 drop 的时候, 比如有一个连接创建了临时表, 连接断开以后, 对应的临时表都需要进行drop 操作.&lt;/p&gt;

&lt;p&gt;InnoDB 是需要将该tablespace 对应的所有的page 从LRU/FLUSH list 中删除, 如果没有这个操作, 新的table 的table spaceid 如果重复的话, 那么就可能访问到脏数据.&lt;/p&gt;

&lt;p&gt;为了将这些page 删除, 那么就需要全部遍历LRU/FLUSH list, 当bp 特别大的时候, 这样遍历的开销是很大的, 并且无论这个要删除的table 有多大, 都需要将这些LRU/FLUSH list 全部遍历..&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;解决方法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;解决方法和之前解决undo ACID DDL 的方法类似, 核心思想就是&lt;strong&gt;通过引用计数的方法, 对table_space 加reference, 然后后续lazy delete&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;bp 上的每一个page 都有自己对应的version, 当table space 被drop/rename 的时候, 只需要对fil_space 的version + 1, 那么bp 中该fil_space 对应的page 就因为version &amp;lt; fil_space.current_version 而变得无效.&lt;/p&gt;

&lt;p&gt;原先由drop/rename tablespace 触发的space_delete 操作就变的非常的轻量. 后续定期的将这些stable page 删除或者复用即可&lt;/p&gt;

&lt;p&gt;不过带来的额外开销就是, 每一次访问bp 中的一个page 就需要确认当前page 是否过期.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;具体实现&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;buf_page_t 增加 m_space, m_version.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Additions&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf_page_t&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
 &lt;span class=&quot;c1&quot;&gt;// 指向对应的fil_space_t&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;fil_space_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{};&lt;/span&gt;

 &lt;span class=&quot;c1&quot;&gt;// Version number of the page to check for stale pages. This value is&lt;/span&gt;
 &lt;span class=&quot;c1&quot;&gt;// &quot;inherited&quot; from the m_space-&amp;gt;m_version when we init a page.&lt;/span&gt;
 &lt;span class=&quot;c1&quot;&gt;// page 的version number, 在page_init 的时候设置成m_space-&amp;gt;m_version&lt;/span&gt;
 &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{};&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;fil_space_t 增加m_version,  m_n_ref_count.&lt;/p&gt;

&lt;p&gt;m_version 就是当前fil_space_t 的版本号, 每次delete/truncate 就会 + 1&lt;/p&gt;

&lt;p&gt;m_n_ref_count: bp 每增加一个page , m_n_ref_count + 1, 只能等到m_n_ref_count == 0 的时候, 改fil_space 才能被删除, 否则bp 里面的m_space 指针就会指向空&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Additions&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;fil_space_t&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
 &lt;span class=&quot;c1&quot;&gt;// Version number of the instance, not persistent. Every time we truncate&lt;/span&gt;
 &lt;span class=&quot;c1&quot;&gt;// or delete we bump up the version number.&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;lsn_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{};&lt;/span&gt;

 &lt;span class=&quot;c1&quot;&gt;// Reference count of how many pages point to this instance. An instance cannot&lt;/span&gt;
 &lt;span class=&quot;c1&quot;&gt;// be deleted if the reference count is greater than zero. The only exception&lt;/span&gt;
 &lt;span class=&quot;c1&quot;&gt;// is shutdown.&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;atomic_int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_n_ref_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{};&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;增加了lazy delete fil_space 以后, 那么什么时候将内存中的fil_space_t 删除呢?&lt;/p&gt;

&lt;p&gt;最后的删除操作在 master_thread 会定期执行, 将之前已经标记删除, 放入到m_deleted_spaces 中的space 一起删除&lt;/p&gt;

&lt;p&gt;/* Purge any deleted tablespace pages. */
fil_purge();  =&amp;gt; fil_shard.purge()&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;purge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mutex_acquire&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_deleted_spaces&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;begin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_deleted_spaces&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;space&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;// has_no_references() 说明该fil_space 对应的bp 已经都删除了, 那么该space 就可以删除&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;space&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;has_no_references&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ut_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;space&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;front&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_pending&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;space_free_low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_deleted_spaces&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;erase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mutex_release&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;drop/rename tablespace&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;执行drop/rename tablespace 的时候需要执行 row_drop_tablespace =&amp;gt; fil_delete_tablespace =&amp;gt; space_delete(space_id, buf_remove)&lt;/p&gt;

&lt;p&gt;新增加 buf_remove_t 类型: BUF_REMOVE_NONE. 不需要移除该tablespace 的所有bp.&lt;/p&gt;

&lt;p&gt;8.0.23 drop table 的时候, 执行 row_drop_tablespace =&amp;gt; fil_delete_tablespace,  之前delete tablespace 的时候, 传入的是 BUF_REMOVE_ALL_NO_WRITE, 需要将该space 对应的bp 都清理才可以完成操作.&lt;/p&gt;

&lt;p&gt;传入 BUF_REMOVE_NONE 就只需要将tablespace 标记删除, 放入到 m_deleted_spaces 中, 不需要清理bp, 然后将对应的物理文件删除即可. 该tablespace 对应bp 中的数据就变成 stale page, 后续会有操作将这些stale page 删除或者复用.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;enum&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf_remove_t&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;cm&quot;&gt;/** Don&apos;t remove any pages. */&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;BUF_REMOVE_NONE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;cm&quot;&gt;/** Remove all pages from the buffer pool, don&apos;t write or sync to disk */&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;BUF_REMOVE_ALL_NO_WRITE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;cm&quot;&gt;/** Remove only from the flush list, don&apos;t write or sync to disk */&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;BUF_REMOVE_FLUSH_NO_WRITE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;cm&quot;&gt;/** Flush dirty pages to disk only don&apos;t remove from the buffer pool */&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;BUF_REMOVE_FLUSH_WRITE&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;BUF_REMOVE_ALL_NO_WRITE:&lt;/p&gt;

&lt;p&gt;从flush list 和 LRU list 上面都删除, 数据不需要, 并且也不需要刷盘. 从LRU list 上面也都删除开销是比较大的, 因此更多的时候是使用BUF_REMOVE_FLUSH_NO_WRITE, 只删flush list, 不删LRU&lt;/p&gt;

&lt;p&gt;一般来说truncate table 的时候是执行这个.  在5.6/5.7 里面, 由于truncate table 了以后, space id 是不会变的, 那么就必须把这些space 对应的page 都删除, 否则如果新的table 的space id 和老的space id 一致, 那就访问到脏数据了.&lt;/p&gt;

&lt;p&gt;BUF_REMOVE_FLUSH_NO_WRITE:&lt;/p&gt;

&lt;p&gt;从flush list 删除删除, 并且不需要刷盘, 直接丢弃掉. 和BUF_REMOVE_ALL_NO_WRITE 相比, 把从LRU list 上面删除的操作放到了后台来做, 因为lru list 的大小是远远大于flush list, 删除lru list 的成本是很大的, 因此放在后来执行&lt;/p&gt;

&lt;p&gt;一般drop table 是执行这个操作, 让后台慢慢从lru list 里面把要drop 的tablespace 删除&lt;/p&gt;

&lt;p&gt;BUF_REMOVE_FLUSH_WRITE:&lt;/p&gt;

&lt;p&gt;从flush list 上删除, 并且刷脏, 那么就不需要从LRU list 上删除, 因为LRU list 上也是最新的&lt;/p&gt;

&lt;p&gt;常用场景, 执行DDL 以后,  DDL 只需要确保这个DDL 产生的page 必须进行刷脏. 执行刷脏逻辑&lt;/p&gt;

&lt;p&gt;BUF_REMOVE_NONE:&lt;/p&gt;

&lt;p&gt;只需要将tablespace 标记删除, 不需要清理bp, 该tablespace 对应bp 中的数据就变成 stale page, 后续会有操作将这些stale page 删除或者复用.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;那么什么时候会将这些 stale page 删除呢?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;总共有多个场景:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;在正常从bp 中读取page 的时候, 如果读取到的page 是 stale, 那么通过执行 buf_page_free_stale() 将该page 进行删除操作&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;在从double write buffer Double_write::write_pages() 到磁盘的时候, 如果这个时候改page 的space file 已经被删除, 那么这个时候通过 buf_page_free_stale_during_write() 进行删除&lt;/li&gt;
  &lt;li&gt;在刷脏操作buf_flush_batch()的时候, 从LRU_list 或者 flush_list 拿取page, 如果发现该page 是stale, 并且没有io 操作在这个page 上面, 那么通过 buf_page_free_stale() 进行删除操作&lt;/li&gt;
  &lt;li&gt;在single page flush 的时候, 同样判断该page 是stale, 那么通过buf_page_free_stale() 进行删除&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 08 Feb 2021 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2021/02/08/faster-truncate-drop/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/02/08/faster-truncate-drop/</guid>
      </item>
    
  </channel>
</rss>
