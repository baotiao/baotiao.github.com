<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title></title>
 <link href="http://baotiao.github.io//atom.xml" rel="self"/>
 <link href="http://baotiao.github.io/"/>
 <updated>2016-05-27T11:20:17+08:00</updated>
 <id>http://baotiao.github.io/</id>
 <author>
   <name></name>
   <email></email>
 </author>

 
 <entry>
   <title>xfs kmalloc failure problem</title>
   <link href="http://baotiao.github.io//2016/05/xfs-kmalloc-failure/"/>
   <updated>2016-05-26T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2016/05/xfs-kmalloc-failure</id>
   <content type="html">&lt;p&gt;记录一次线上实体机的xfs kmem_alloc 操作一直失败排查&lt;/p&gt;

&lt;p&gt;Hi all:&lt;/p&gt;

&lt;h3&gt;问题现象&lt;/h3&gt;

&lt;p&gt;线上有些实体机dmesg出现xfs 报错&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Apr 29 21:54:31 w-openstack86 kernel: XFS: possible memory allocation deadlock in kmem_alloc (mode:0x250)
Apr 29 21:54:33 w-openstack86 kernel: XFS: possible memory allocation deadlock in kmem_alloc (mode:0x250)
Apr 29 21:54:34 w-openstack86 kernel: INFO: task qemu-system-x86:6902 blocked for more than 120 seconds.
Apr 29 21:54:34 w-openstack86 kernel: &quot;echo 0 &amp;gt; /proc/sys/kernel/hung_task_timeout_secs&quot; disables this message.
Apr 29 21:54:34 w-openstack86 kernel: qemu-system-x86 D ffff88105065e800     0  6902      1 0x00000080
Apr 29 21:54:34 w-openstack86 kernel: ffff880155c63778 0000000000000086 ffff88099719d080 ffff880155c63fd8
Apr 29 21:54:34 w-openstack86 kernel: ffff880155c63fd8 ffff880155c63fd8 ffff88099719d080 ffff88099719d080
Apr 29 21:54:34 w-openstack86 kernel: ffff88081018de10 ffffffffffffffff ffff88081018de18 ffff88105065e800
Apr 29 21:54:34 w-openstack86 kernel: Call Trace:
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff8163a879&amp;gt;] schedule+0x29/0x70
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff8163c235&amp;gt;] rwsem_down_read_failed+0xf5/0x170
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffffa0256a30&amp;gt;] ? xfs_ilock_data_map_shared+0x30/0x40 [xfs]
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff81301764&amp;gt;] call_rwsem_down_read_failed+0x14/0x30
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff81639a90&amp;gt;] ? down_read+0x20/0x30
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffffa02569bc&amp;gt;] xfs_ilock+0xdc/0x120 [xfs]
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffffa0256a30&amp;gt;] xfs_ilock_data_map_shared+0x30/0x40 [xfs]
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffffa023f4f4&amp;gt;] __xfs_get_blocks+0x94/0x4b0 [xfs]
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff810654f2&amp;gt;] ? get_user_pages_fast+0x122/0x1a0
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffffa023f944&amp;gt;] xfs_get_blocks_direct+0x14/0x20 [xfs]
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff8121d704&amp;gt;] do_blockdev_direct_IO+0x13f4/0x2620
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffffa023f930&amp;gt;] ? xfs_get_blocks+0x20/0x20 [xfs]
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff8121e985&amp;gt;] __blockdev_direct_IO+0x55/0x60
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffffa023f930&amp;gt;] ? xfs_get_blocks+0x20/0x20 [xfs]
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffffa023f210&amp;gt;] ? xfs_finish_ioend_sync+0x30/0x30 [xfs]
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffffa023e5ca&amp;gt;] xfs_vm_direct_IO+0xda/0x180 [xfs]
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffffa023f930&amp;gt;] ? xfs_get_blocks+0x20/0x20 [xfs]
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffffa023f210&amp;gt;] ? xfs_finish_ioend_sync+0x30/0x30 [xfs]
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff8116afed&amp;gt;] generic_file_direct_write+0xcd/0x190
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffffa027f1fc&amp;gt;] xfs_file_dio_aio_write+0x1f3/0x232 [xfs]
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffffa024bd2d&amp;gt;] xfs_file_aio_write+0x13d/0x150 [xfs]
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff811ddde9&amp;gt;] do_sync_readv_writev+0x79/0xd0
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff811df3be&amp;gt;] do_readv_writev+0xce/0x260
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffffa024bbf0&amp;gt;] ? xfs_file_buffered_aio_write+0x260/0x260 [xfs]
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff811ddca0&amp;gt;] ? do_sync_read+0xd0/0xd0
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff810e506e&amp;gt;] ? do_futex+0xfe/0x5b0
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff811df5e5&amp;gt;] vfs_writev+0x35/0x60
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff811df9e2&amp;gt;] SyS_pwritev+0xc2/0xf0
Apr 29 21:54:34 w-openstack86 kernel: [&amp;lt;ffffffff816458c9&amp;gt;] system_call_fastpath+0x16/0x1b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进而导致虚拟机因为文件系统的内存申请操作出现了问题, 导致这个虚拟机挂掉&lt;/p&gt;

&lt;p&gt;解决办法:&lt;/p&gt;

&lt;p&gt;sync &amp;amp;&amp;amp; echo 3 &gt; /proc/sys/vm/drop_caches&lt;/p&gt;

&lt;p&gt;将page cache 里面的内容清空, 那么就不再报错. 但是为什么简单的清空page cache 就可以解决这个问题, 如果系统被page cache 占用着难道不应该申请内存操作的时候将一部分page cache 里面的内存刷回, 然后让出部分空闲空间么?&lt;/p&gt;

&lt;h3&gt;问题分析&lt;/h3&gt;

&lt;p&gt;看了一下代码, 出现这个报错在xfs module里面, 这个错误是在kmalloc 失败以后就会报出来, 并且会重试100次, 如果100 次以后还是失败, 就直接return error. 那么为什么kmalloc 会失败呢?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void *
kmem_alloc(size_t size, xfs_km_flags_t flags)
{
  int retries = 0;
  gfp_t lflags = kmem_flags_convert(flags);
  void  *ptr;

  do {
    ptr = kmalloc(size, lflags);
    if (ptr || (flags &amp;amp; (KM_MAYFAIL|KM_NOSLEEP)))
      return ptr;
    if (!(++retries % 100))
      xfs_err(NULL,
    &quot;possible memory allocation deadlock in %s (mode:0x%x)&quot;,
          __func__, lflags);
    congestion_wait(BLK_RW_ASYNC, HZ/50);
  } while (1);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里首先我们必须知道如果一个操作kmalloc() 是向slab allocator 申请具体的小块的内存, 而slab allocator 是想buddy system 通过&lt;code&gt;__alloc_pages()&lt;/code&gt; 去申请连续的内存. 那么肯定就是在&lt;code&gt;__alloc_pages()&lt;/code&gt; 申请内存的时候失败了, 那为什么进行&lt;code&gt;__alloc_pages()&lt;/code&gt; 操作的时候会失败呢? 即使实际物理内存里面还有page cache页以及swap 空间还没占满&lt;/p&gt;

&lt;p&gt;从出现问题的机器上面我们可以看到, 机器的状态大概是这个样子&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[chenzongzhi@w-openstack86 ~]$ free -m
              total        used        free      shared  buff/cache   available
Mem:          64272       26298        4379         129       33595       37051
Swap:         32255           0       32255
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里我们同时看一下机器的内存碎片状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;这里并不是现场的机器, 只是另一台线上内存用的差不多的机器 
[chenzongzhi@w-openstack81 ~]$ cat /proc/buddyinfo
Node 0, zone      DMA      0      0      0      1      2      1      1      0      1      1      3
Node 0, zone    DMA32   2983   2230   1037    290    121     63     47     61     16      0      0
Node 0, zone   Normal  13707   1126    285    268    291    160     64     21     11      0      0
Node 1, zone   Normal  10678   5041   1167    705    316    158     61     22      0      0      0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到这里比较大块的连续的page 是基本没有的. 因为在xfs 的申请内存操作里面我们看到有这种连续的大块的内存申请的操作的请求,  比如:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;6000:   map = kmem_alloc(subnex * sizeof(*map), KM_MAYFAIL | KM_NOFS);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因此比较大的可能是线上虽然有少量的空闲内存, 但是这些内存非常的碎片, 因此只要有一个稍微大的的连续内存的请求都无法满足. 但是为什么不使用page cache呢?&lt;/p&gt;

&lt;p&gt;到这里. 我们可以看到整个系统的内存基本全部被使用, 有少量的空闲. 但是这里面包含了大量的page cache. 理论上page cache 里面只会包含几M的需要刷回磁盘的内容, 大量的page cache 只是为了加快读, 里面的内容应该可以随时清空掉. 所以在kmalloc申请内存的时候应该能够把page cache 里面的内容清空, 然后给kernel 留出空闲的连续的大内存空间才是. 那么为什么这次申请内存操作&lt;code&gt;__alloc_pages()&lt;/code&gt;的时候不把page cache 里面的内容清空一部分, 然后给这次&lt;code&gt;__alloc_pages()&lt;/code&gt; 预留出来空间呢?&lt;/p&gt;

&lt;p&gt;这里xfs 的申请内存操作因为是文件系统的申请内存操作, 所以一般带上GFP_NOFS 这个参数, 这个参数的意思是&lt;/p&gt;

&lt;p&gt;GFP_NOFS&lt;/p&gt;

&lt;p&gt;These flags function like GFP_KERNEL, but they add restrictions on what the kernel can do to satisfy the request. A GFP_NOFS allocation is not allowed to perform any filesystem calls, while GFP_NOIO disallows the initiation of any I/O at all. They are used primarily in the filesystem and virtual memory code where an allocation may be allowed to sleep, but recursive filesystem calls would be a bad idea.&lt;/p&gt;

&lt;p&gt;也就是说如果带上这个GFP_NOFS的flag, 那么本次 &lt;code&gt;__alloc_pages()&lt;/code&gt; 是不允许有任何的filesystem calls的操作的, 那么如果物理内存不够了, 也就是不能触发这个page_reclaim 的操作. 具体的实现是在page reclaim 的 do_try_to_free_pages 里面shrink_page_list 的时候会判断这次的scan_control 里面有没有这个 __GFP_FS flag, 如果是没有GFP_FS flag, 就不会就行这个page_reclaim&lt;/p&gt;

&lt;p&gt;为什么要这样, 因为如果在文件系统申请内存的时候, 你又触发了一次文件系统相关的操作, 比如把page cache 里面的内容刷会到文件, 那么刷会到文件这个操作必然又会有内存申请相关的操作, 这样就进入是循环了. kernel 为了避免这样的死循环尝试, 所以在文件系统相关的内存申请就不允许有任何filesystem calls. 也就是这个原因导致kernel 本身kmalloc 一直失败&lt;/p&gt;

&lt;p&gt;那么接下来 sync &amp;amp;&amp;amp; echo 3 &gt; /proc/sys/vm/drop_caches 操作为什么能够成功, 并且后续就不会有报错了呢?&lt;/p&gt;

&lt;p&gt;因为drop_caches 这个操作属于外部操作, 不属于文件系统本身的操作, 因此没有GFP_NOFS这个flag, 因此可以很轻松的就把page cache 里面的内容清空, 让Kernel 有足够多的连续的大内存. 线上自然就不报错了&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>我对存储的一些看法</title>
   <link href="http://baotiao.github.io//2016/05/my-opnion-of-storage/"/>
   <updated>2016-05-26T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2016/05/my-opnion-of-storage</id>
   <content type="html">&lt;p&gt;其实计算机主要分成两个部分 计算 + 存储, 存储应该是计算的基石&lt;/p&gt;

&lt;p&gt;那么存储其实又主要分成两个部分 在线存储 + 离线存储&lt;/p&gt;

&lt;p&gt;离线存储的需求很统一, 就是离线数据分析, 产生报表等等. 也因为这统一的需求, 所以目前hdfs 为首的离线存储基本统一了离线存储这个平台. 离线存储最重要的就是吞吐, 以及资源的利用率. 对性能, 可靠性的要求其实并不多. (所以这也是为什么java系在离线存储这块基本一统的原因, java提供的大量的基础库, 包等等. 而离线存储又对性能, 可靠性没有比较高的要求, 因此java GC等问题也不明显)&lt;/p&gt;

&lt;p&gt;所以我们可以看到虽然现在离线的分析工具一直在变, 有hadoop, spark, storm 等等, 但是离线的存储基本都没有变化. 还是hdfs 一统这一套. 所以我认为未来离线存储这块不会有太大的变化&lt;/p&gt;

&lt;p&gt;在线存储&lt;/p&gt;

&lt;p&gt;指的是直接面向用户请求的存储类型. 由于用户请求的多样性, 因此在线存储通常需要满足各种不同场景的需求.&lt;/p&gt;

&lt;p&gt;比如用户系统存储是提供对象的服务, 能够直接通过HTTP接口来访问, 那么自然就诞生了对象存储这样的服务&lt;/p&gt;

&lt;p&gt;比如用户希望所存储的数据是关系性数据库的模型, 能够以SQL 的形式来访问, 那么其实就是mysql, 或者现在比较火热的NewSql&lt;/p&gt;

&lt;p&gt;比如用户只希望访问key, value的形式, 那么我们就可以用最简单的kv接口, 那么就有Nosql, bada, cassandra 等等就提供这样的服务&lt;/p&gt;

&lt;p&gt;当然也有多数据结构的求情, hash, list 等等就有了redis, 有POSIX文件系统接口了请求, 那么就有了CephFs. 有了希望提供跟磁盘一样的iSCSI 这样接口的快设备的需求, 就有了块存储, 就是ceph.&lt;/p&gt;

&lt;p&gt;从上面可以看到和离线存储对比, 在线存储的需求更加的复杂, 从接口类型, 从对访问延期的需求, 比如对于kv的接口, 我们一般希望是2ms左右, 那么对于对象存储的接口我们一般在10ms~20ms. 对于SQL, 我们的容忍度可能更高一些, 可以允许有100 ms. 处理延迟的需求, 我们还会有数据可靠性的不同, 比如一般在SQL 里面我们一般需要做到强一致. 但是在kv接口里面我们一般只需要做到最终一致性即可. 同样对于资源的利用也是不一样, 如果存储的是稍微偏冷的数据, 一定是EC编码, 然后存在大的机械盘. 对于线上比较热的数据, 延迟要求比较高. 一定是3副本, 存在SSD盘上&lt;/p&gt;

&lt;p&gt;从上面可以看到在线存储的需求多样性, 并且对服务的可靠性要求各种不一样, 因此我们很难看到有一个在线存储能够统一满足所有的需求. 这也是为什么现在没有一个开源的在线存储服务能够像hdfs 那样的使用率. 因此一定是在不同的场景下面有不同的存储的解决方案&lt;/p&gt;

&lt;p&gt;可以看到Facebook infrastructure stack 里面就包含的各种的在线存储需求&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/LpZw633.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;比如里面包含了热的大对象存储Haystack, 一般热的大对象存储f4, 图数据库Tao. key-value 存储memcached 集群等等&lt;/p&gt;

&lt;p&gt;同样google 也会有不同的在线存储产品&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/aUTxFTN.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对应于Google 有MegaStore, Spanner 用于线上的SQL 类型的在线存储, BigTable 用于类似稀疏map 的key-value存储等等&lt;/p&gt;

&lt;p&gt;个人认为对于在线存储还是比较适合C++来做这一套东西, 因为比较在线存储一般对性能, 可靠性, 延迟的要求比较高.&lt;/p&gt;

&lt;p&gt;那么这些不同的存储一般都怎么实现呢?, 很多在线存储比如对象存储的实现一般都是基于底下的key-value进行封装来实现对象存储的接口. ceph 就是这方面这个做法的极致.&lt;/p&gt;

&lt;p&gt;ceph 底下的rados 本质是一个对象存储, 这里的对象存储跟s3 的对象存储还不一样, 只是提供了存储以为key 对应的value 是对象的形式.
然后基于上层基于librados 封装了librbd 就实现了块设备的协议, 那么就是一个块存储. 基于librados 实现了Rados Gateway 提供了s3 的对象存储的协议就封装成s3对象存储. 基于librados 实现了POSIX 文件系统的接口, 就封装成了分布式文件系统Ceph FS. (不过我认为ceph 底下的rados实现的还不够纯粹, 因为rados对应的value 是类似于一个对象文件. 比如在基于librados 实现librbd的时候很多对象属性的一些方法是用不上的)
&lt;img src=&quot;http://i.imgur.com/grsvIND.png&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;同样google 的F1 是基于spanner 的key-value 接口实现了SQL了接口. 就封装成了NewSql&lt;/p&gt;

&lt;p&gt;因此其实我们也可以这么说对于这么多接口的实现, 其实后续都会转换成基于key-value 接口实现另一种接口的形式, 因为key-value 接口足够简单, 有了稳定的key-value 存储, 只需要在上层提供不同接口转换成key-value 接口的实现即可. 当然不同的接口实现难度还是不太一样, 比如实现SQL接口, POSIX文件系统接口, 图数据库肯定要比实现一个对象存储的接口要容易很多&lt;/p&gt;

&lt;p&gt;未来我们应该也在朝这个方向做吧&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>pika introduction</title>
   <link href="http://baotiao.github.io//2016/05/pika-introduction/"/>
   <updated>2016-05-18T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2016/05/pika-introduction</id>
   <content type="html">&lt;p&gt;今天主要向大家介绍一下pika&lt;/p&gt;

&lt;p&gt;pika 是360 DBA和基础架构组联合开发的类redis 存储系统, 完全支持Redis协议，用户不需要修改任何代码, 就可以将服务迁移至pika. 有维护redis 经验的DBA 维护pika 不需要学习成本&lt;/p&gt;

&lt;p&gt;pika 主要解决的是用户使用redis的内存大小超过50G, 80G 等等这样的情况, 会遇到比如启动恢复时间长,  一主多从代价大, 硬件成本贵, 缓冲区容易写满等等问题. pika 就下针对这些场景的一个解决方案&lt;/p&gt;

&lt;p&gt;pika 目前已经开源, github地址:
https://github.com/Qihoo360/pika&lt;/p&gt;

&lt;p&gt;重点:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;pika 的单线程的性能肯定不如redis, pika是多线程的结构, 因此在线程数比较多的情况下, 某些数据结构的性能可以优于redis&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;pika 肯定不是完全优于redis 的方案, 只是在某些场景下面更适合. 所以目前公司内部redis, pika 是共同存在的方案, DBA会根据业务的场景挑选合适的方案&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;本次分享分成4个部分&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;大容量redis 容易遇到的问题&lt;/li&gt;
&lt;li&gt;pika 整体架构&lt;/li&gt;
&lt;li&gt;pika 具体实现&lt;/li&gt;
&lt;li&gt;pika vs redis&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;背景&lt;/h3&gt;

&lt;p&gt;redis 提供了丰富的多数据结构的接口, 在redis 之前, 比如memcache 都认为后端只需要存储kv的结构就可以, 不需要感知这个value 里面的内容. 用户需要使用的话通过json_encode, json_decode 等形式进行数据的读取就行. 但是其实redis 类似做了一个微创新, redis 提供了多数据结构的支持, 让前端写代码起来更加的方便了&lt;/p&gt;

&lt;p&gt;因此redis 在公司的使用率也是越来越广泛, 用户不知不觉把越来越多的数据存储在redis中, 随着用户的使用, DBA 发现有些redis 实例的大小也是越来越大. 在redis 实例内存使用比较大的情况下, 遇到的问题也会越来越多, 因此DBA和我们一起实现了大容量redis 的解决方案&lt;/p&gt;

&lt;p&gt;最近半年公司每天redis 的访问情况
&lt;img src=&quot;http://i.imgur.com/dpHD828.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;redis 架构方案&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/plSfqUF.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;大容量redis 遇到的问题&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;恢复时间长&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;我们线上的redis 一般同时开启rdb 和 aof.
我们知道aof的作用是实时的记录用户的写入操作, rdb 是redis 某一时刻数据的完整快照. 那么恢复的时候一般是通过 rdb + aof 的方式进行恢复, 根据我们线上的情况 50G redis 恢复时间需要差不多70分钟&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一主多从, 主从切换代价大&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;redis 在主库挂掉以后, 从库升级为新的主库. 那么切换主库以后, 所有的从库都需要跟新主做一次全同步, 全量同步一次大容量的redis, 代价非常大.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;缓冲区写满问题&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;为了防止同步缓冲区被复写，dba给redis设置了2G的巨大同步缓冲区，这对于内存资源来讲代价很大. 当由于机房之间网络有故障, 主从同步出现延迟了大于2G以后, 就会触发全同步的过程. 如果多个从库同时触发全同步的过程, 那么很容易就将主库给拖死&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;内存太贵&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;我们一般线上使用的redis 机器是 64G, 96G. 我们只会使用80% 的空间.&lt;/p&gt;

&lt;p&gt;如果一个redis 的实例是50G, 那么基本一台机器只能运行一个redis 实例. 因此特别的浪费资源&lt;/p&gt;

&lt;p&gt;总结: 可以看到在redis 比较小的情况下, 这些问题都不是问题, 但是当redis 容量上去以后. 很多操作需要的时间也就越来越长了&lt;/p&gt;

&lt;h3&gt;pika 整体架构&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/tm5ubVp.png&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;主要组成:
1. 网络模块 pink
2. 线程模块
3. 存储引擎 nemo
4. 日志模块 binlog&lt;/p&gt;

&lt;h3&gt;pink 网络模块&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;基础架构团队开发网络编程库, 支持pb, redis等等协议. 对网络编程的封装, 用户实现一个高性能的server 只需要实现对应的DealMessage() 函数即可&lt;/li&gt;
&lt;li&gt;支持单线程模型, 多线程worker模型&lt;/li&gt;
&lt;li&gt;github 地址: https://github.com/baotiao/pink&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;线程模块&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/FdaK0H5.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;pika 基于pink 对线程进行封装. 使用多个工作线程来进行读写操作，由底层nemo引擎来保证线程安全，线程分为11种：&lt;/p&gt;

&lt;p&gt;PikaServer：主线程&lt;/p&gt;

&lt;p&gt;DispatchThread：监听端口1个端口，接收用户连接请求&lt;/p&gt;

&lt;p&gt;ClientWorker：存在多个（用户配置），每个线程里有若干个用户客户端的连接，负责接收处理用户命令并返回结果，每个线程执行写命令后，追加到binlog中&lt;/p&gt;

&lt;p&gt;Trysync：尝试与master建立首次连接，并在以后出现故障后发起重连&lt;/p&gt;

&lt;p&gt;ReplicaSender：存在多个（动态创建销毁，本master节点挂多少个slave节点就有多少个），每个线程根据slave节点发来的同步偏移量，从binlog指定的偏移开始实时同步命令给slave节点&lt;/p&gt;

&lt;p&gt;ReplicaReceiver：存在1个（动态创建销毁，一个slave节点同时只能有一个master），将用户指定或当前的偏移量发送给master节点并开始接收执行master实时发来的同步命令，在本地使用和master完全一致的偏移量来追加binlog&lt;/p&gt;

&lt;p&gt;SlavePing：slave用来向master发送心跳进行存活检测&lt;/p&gt;

&lt;p&gt;HeartBeat：master用来接收所有slave发送来的心跳并恢复进行存活检测&lt;/p&gt;

&lt;p&gt;bgsave：后台dump线程&lt;/p&gt;

&lt;p&gt;scan：后台扫描keyspace线程&lt;/p&gt;

&lt;p&gt;purge：后台删除binlog线程&lt;/p&gt;

&lt;h3&gt;存储引擎 nemo&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Pika 的存储引擎, 基于Rocksdb 修改. 封装Hash, List, Set, Zset等数据结构&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;我们知道redis 是需要支持多数据结构的, 而rocksdb 只是一个kv的接口, 那么我们如何实现的呢?&lt;/p&gt;

&lt;p&gt;比如对于Hash 数据结构:&lt;/p&gt;

&lt;p&gt;对于每一个Hash存储，它包括hash键（key），hash键下的域名（field）和存储的值 （value）.&lt;/p&gt;

&lt;p&gt;nemo的存储方式是将key和field组合成为一个新的key，将这个新生成的key与所要存储的value组成最终落盘的kv键值对。同时，对于每一个hash键，nemo还为它添加了一个存储元信息的落盘kv，它保存的是对应hash键下的所有域值对的个数。&lt;/p&gt;

&lt;p&gt;每个hash键、field、value到落盘kv的映射转换
&lt;img src=&quot;http://i.imgur.com/Tlu69Dk.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;每个hash键的元信息的落盘kv的存储格式
&lt;img src=&quot;http://i.imgur.com/ntsReMS.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;比如对于List 数据结构:&lt;/p&gt;

&lt;p&gt;顾名思义，每个List结构的底层存储也是采用链表结构来完成的。对于每个List键，它的每个元素都落盘为一个kv键值对，作为一个链表的一个节点，称为元素节点。和hash一样，每个List键也拥有自己的元信息。&lt;/p&gt;

&lt;p&gt;每个元素节点对应的落盘kv存储格式
&lt;img src=&quot;http://i.imgur.com/20RrJdm.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;每个元信息的落盘kv的存储格式
&lt;img src=&quot;http://i.imgur.com/n9UC9Ky.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其他的数据结构实现的方式也类似, 通过将hash_filed 拼成一个key, 存储到支持kv的rocksdb 里面去. 从而实现多数据结构的结构&lt;/p&gt;

&lt;h3&gt;日志模块 binlog&lt;/h3&gt;

&lt;p&gt;Pika的主从同步是使用Binlog来完成的.
binlog 本质是顺序写文件, 通过Index + offset 进行同步点检查.&lt;/p&gt;

&lt;p&gt;解决了同步缓冲区太小的问题&lt;/p&gt;

&lt;p&gt;支持全同步 + 增量同步&lt;/p&gt;

&lt;p&gt;master 执行完一条写命令就将命令追加到Binlog中，ReplicaSender将这条命令从Binlog中读出来发送给slave，slave的ReplicaReceiver收到该命令，执行，并追加到自己的Binlog中.&lt;/p&gt;

&lt;p&gt;当发生主从切换以后, slave仅需要将自己当前的Binlog Index + offset 发送给master，master找到后从该偏移量开始同步后续命令&lt;/p&gt;

&lt;p&gt;为了防止读文件中写错一个字节则导致整个文件不可用，所以pika采用了类似leveldb log的格式来进行存储，具体如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/x1H8loY.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;主要功能&lt;/h3&gt;

&lt;p&gt;pika 线上架构&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/V4Ufgh1.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;主从架构&lt;/h3&gt;

&lt;p&gt;旁白: 为了减少用户的学习成本, 目前pika 的主从同步功能是和redis完全一样, 只需要slaveof 就可以实现主从关系的建立, 使用起来非常方便&lt;/p&gt;

&lt;p&gt;背景
1. Pika Replicate&lt;/p&gt;

&lt;p&gt;pika支持master/slave的复制方式，通过slave端的slaveof命令激发
salve端处理slaveof命令，将当前状态变为slave，改变连接状态
slave的trysync线程向master发起trysync，同时将要同步点传给master
master处理trysync命令，发起对slave的同步过程，从同步点开始顺序发送binlog或进行全同步&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Binlog&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;pika同步依赖binlog
binlog文件会自动或手动删除
当同步点对应的binlog文件不存在时，需要通过全同步进行数据同步&lt;/p&gt;

&lt;p&gt;全同步&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;简介&lt;/p&gt;

&lt;p&gt; 需要进行全同步时，master会将db文件dump后发送给slave
 通过rsync的deamon模式实现db文件的传输&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;实现逻辑&lt;/p&gt;

&lt;p&gt; slave在trysnc前启动rsync进程启动rsync服务
 master发现需要全同步时，判断是否有备份文件可用，如果没有先dump一份
 master通过rsync向slave发送dump出的文件
 slave用收到的文件替换自己的db
 slave用最新的偏移量再次发起trysnc
 完成同步&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/AvOKHHg.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Slave连接状态&lt;/p&gt;

&lt;p&gt; No Connect：不尝试成为任何其他节点的slave
 Connect：Slaveof后尝试成为某个节点的slave，发送trysnc命令和同步点
 Connecting：收到master回复可以slaveof，尝试跟master建立心跳
 Connected: 心跳建立成功
 WaitSync：不断检测是否DBSync完成，完成后更新DB并发起新的slaveof&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/ffummqK.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;主从命令同步&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/YjYMsCd.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图1是一个主从同步的一个过程（即根据主节点数据库的操作日志，将主节点数据库的改变过程顺序的映射到从节点的数据库上），从图1中可以看出，每一个从节点在主节点下都有一个唯一对应的BinlogSenderThread。&lt;/p&gt;

&lt;p&gt;（为了说明方便，我们定一个“同步命令”的概念，即会改变数据库的命令，如set，hset，lpush等，而get，hget，lindex则不是）&lt;/p&gt;

&lt;p&gt;主要模块的功能：&lt;/p&gt;

&lt;p&gt;WorkerThread：接受和处理用户的命令；&lt;/p&gt;

&lt;p&gt;BinlogSenderThread：负责顺序地向对应的从节点发送在需要同步的命令；&lt;/p&gt;

&lt;p&gt;BinlogReceiverModule: 负责接受主节点发送过来的同步命令&lt;/p&gt;

&lt;p&gt;Binglog：用于顺序的记录需要同步的命令&lt;/p&gt;

&lt;p&gt;主要的工作过程：
1.当WorkerThread接收到客户端的命令，按照执行顺序，添加到Binlog里；&lt;/p&gt;

&lt;p&gt;2.BinglogSenderThread判断它所负责的从节点在主节点的Binlog里是否有需要同步的命令，若有则发送给从节点；&lt;/p&gt;

&lt;p&gt;3.BinglogReceiverModule模块则做以下三件事情：
    a. 接收主节点的BinlogSenderThread发送过来的同步命令；
    b. 把接收到的命令应用到本地的数据上；
    c. 把接收到的命令添加到本地Binlog里&lt;/p&gt;

&lt;p&gt;至此，一条命令从主节点到从节点的同步过程完成&lt;/p&gt;

&lt;p&gt;BinLogReceiverModule的工作过程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/mgIB0P8.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图2是BinLogReceiverModule（在源代码中没有这个对象，这里是为了说明方便，抽象出来的）的组成，从图2中可以看出BinlogReceiverModule由一个BinlogReceiverThread和多个BinlogBGWorker组成。&lt;/p&gt;

&lt;p&gt;BinlogReceiverThread: 负责接受由主节点传送过来的命令，并分发给各个BinlogBGWorker，若当前的节点是只读状态（不能接受客户端的同步命令），则在这个阶段写Binlog&lt;/p&gt;

&lt;p&gt;BinlogBGWorker：负责执行同步命令；若该节点不是只读状态（还能接受客户端的同步命令），则在这个阶段写Binlog（在命令执行之前写）&lt;/p&gt;

&lt;p&gt;BinlogReceiverThread接收到一个同步命令后，它会给这个命令赋予一个唯一的序列号（这个序列号是递增的），并把它分发给一个BinlogBGWorker；而各个BinlogBGWorker则会根据各个命令的所对应的序列号的顺序来执行各个命令，这样也就保证了命令执行的顺序和主节点执行的顺序一致了&lt;/p&gt;

&lt;p&gt;之所以这么设计主要原因是：
        a. 配备多个BinlogBGWorker是可以提高主从同步的效率，减少主从同步的滞后延迟；
        b. 让BinlogBGWorker在执行执行之前写Binlog可以提高命令执行的并行度；
        c. 在当前节点是非只读状态，让BinglogReceiverThread来写Binlog，是为了让Binglog里保存的命令顺序和命令的执行顺序保持一致；&lt;/p&gt;

&lt;h4&gt;数据备份&lt;/h4&gt;

&lt;p&gt;不同于Redis，Pika的数据主要存储在磁盘中，这就使得其在做数据备份时有天然的优势，可以直接通过文件拷贝实现&lt;/p&gt;

&lt;p&gt;流程:
    打快照：阻写，并在这个过程中或的快照内容
    异步线程拷贝文件：通过修改Rocksdb提供的BackupEngine拷贝快照中文件，这个过程中会阻止文件的删除&lt;/p&gt;

&lt;p&gt;快照内容&lt;/p&gt;

&lt;p&gt;当前db的所有文件名
manifest文件大小
sequence_number
同步点: binlog index + offset&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/mSkkqVY.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;秒删大量的key&lt;/h3&gt;

&lt;p&gt;在我们大量的使用场景中. 对于Hash, zset, set, list这几种多数据机构，当member或者field很多的时候，用户有批量删除某一个key的需求, 那么这个时候实际删除的就是rocksdb 底下大量的kv结构, 如果只是单纯暴力的进行删key操作, 那时间肯定非常的慢, 难以接受. 那我们如何快速删除key？&lt;/p&gt;

&lt;p&gt;刚才的nemo 的实现里面我们可以看到, 我们在value 里面增加了version, ttl 字段, 这两个字段就是做这个事情.&lt;/p&gt;

&lt;p&gt;Solution 0：暴力删除每一个member，时间复杂度O(m) , m是member的个数；&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;优点：易实现；
缺点：同步处理，会阻碍请求；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Solution 1: 启动后台线程，维护删除队列，执行删除，时间复杂度O（m)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;优点：不会明显阻住server；
缺点：仍然要O(m)去删除members，可以优化删除的速度；

Redis 是怎么做的？

    旧版本的Del接口，在实际free大量内存的时候仍然会阻塞server；
    新版增加了lazy free,根据当前server的负载，多次动态free；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Solution 2: 不删除, 只做标记, 时间复杂度O(1)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;优点：效率就够了；
缺点：需要改动下层rocksdb，一定程度破坏了rocksdb的封装，各个模块之间耦合起来；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;方案：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Key的元信息增加版本，表示当前key的有效版本；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Put：查询key的最新版本，后缀到val；
Get：查询key的最新版本，过滤最新的数据；
Iterator： 迭代时，查询key的版本，过滤旧版本数据；

Compact：数据的实际删除是在Compact过程中，根据版本信息过滤；
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前nemo 采用的就是第二种, 通过对rocksdb 的修改, 可以实现秒删的功能, 后续通过修改rocksdb compact的实现, 在compact 的过程中, 将历史的数据淘汰掉&lt;/p&gt;

&lt;h4&gt;数据compact 策略&lt;/h4&gt;

&lt;p&gt;rocksdb 的compact 策略是在写放大, 读放大, 空间放大的权衡.&lt;/p&gt;

&lt;p&gt;那么我们DBA经常会存在需求尽可能减少空间的使用, 因此DBA希望能够随时触发手动compact, 而又尽可能的不影响线上的使用, 而rocksdb 默认的手动compact 策略是最高优先级的, 会阻塞线上的正常流程的合并.&lt;/p&gt;

&lt;p&gt;rocksdb 默认的 manual compact 的限制&lt;/p&gt;

&lt;p&gt;a) 当manual compact执行时，会等待所有的自动compact任务结束, 然后才会执行本次manual compact；
b) manual执行期间，自动compact无法执行&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当manual执行很长时间，无法执行自动compact，导致线上新的写请求只能在memtable中；&lt;/li&gt;
&lt;li&gt;当memtable个数超过设置的level0_slowdown_writes_trigger(默认20)，写请求会出被sleep；&lt;/li&gt;
&lt;li&gt;再严重一些，当超过level0_stop_writes_trigger（默认24)，完全停写；&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;为了避免这种情况，我们对compact的策略进行调整，使得自动compact一直优先执行，避免停写；&lt;/p&gt;

&lt;p&gt;总结:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;恢复时间长
  pika 的存储引擎是nemo, nemo 使用的是rocksdb,  我们知道 rocksdb 启动不需要加载全部数据, 只需要加载几M的log 文件就可以启动, 因此恢复时间非常快&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;一主多从, 主从切换代价大
  在主从切换的时候, 新主确定以后, 从库会用当前的偏移量尝试与新主做一次部分同步, 如果部分同步不成功才做全同步. 这样尽可能的减少全同步次数&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;缓冲区写满问题
  pika 不适用内存buffer 进行数据同步, pika 的主从同步的操作记录在本地的binlog 上, binlog 会随着操作的增长进行rotate操作. 因此不会出现把缓冲区写满的问题&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;内存昂贵问题
  pika 的存储引擎nemo 使用的是rocksdb, rocksdb 和同时使用内存和磁盘减少对内存的依赖. 同时我们尽可能使用SSD盘来存放数据, 尽可能跟上redis 的性能.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;pika vs redis&lt;/h3&gt;

&lt;p&gt;pika相对于redis，最大的不同就是pika是持久化存储，数据存在磁盘上，而redis是内存存储，由此不同也给pika带来了相对于redis的优势和劣势&lt;/p&gt;

&lt;p&gt;优势:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;容量大：Pika没有Redis的内存限制, 最大使用空间等于磁盘空间的大小&lt;/li&gt;
&lt;li&gt;加载db速度快：Pika 在写入的时候, 数据是落盘的, 所以即使节点挂了, 不需要rbd或者aof，pika 重启不用重新加载数据到内存而是直接使用已经持久化在磁盘上的数据, 不需要任何数据回放操作，这大大降低了重启成本。&lt;/li&gt;
&lt;li&gt;备份速度快：Pika备份的速度大致等同于cp的速度（拷贝数据文件后还有一个快照的恢复过程，会花费一些时间），这样在对于百G大库的备份是快捷的，更快的备份速度更好的解决了主从的全同步问题&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;劣势:&lt;/p&gt;

&lt;p&gt;由于Pika是基于内存和文件来存放数据, 所以性能肯定比Redis低一些, 但是我们一般使用SSD盘来存放数据, 尽可能跟上Redis的性能。&lt;/p&gt;

&lt;p&gt;总结:&lt;/p&gt;

&lt;p&gt;从以上的对比可以看出, 如果你的业务场景的数据比较大，Redis 很难支撑， 比如大于50G，或者你的数据很重要，不允许断电丢失，那么使用Pika 就可以解决你的问题。&lt;/p&gt;

&lt;p&gt;而在实际使用中，大多数场景下pika的性能大约是Redis的50%~80%，在某些特定场景下，例如range 500，pika的性能只有redis的20%，针对这些场景我们仍然在改进&lt;/p&gt;

&lt;p&gt;在360内部使用情况:&lt;/p&gt;

&lt;p&gt;粗略的统计如下：&lt;/p&gt;

&lt;p&gt;当前每天承载的总请求量超过100亿, 实例数超过100个&lt;/p&gt;

&lt;p&gt;当前承载的数据总量约3 TB&lt;/p&gt;

&lt;h3&gt;性能对比&lt;/h3&gt;

&lt;p&gt;Server Info:
    CPU: 24 Cores, Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    MEM: 165157944 kB
    OS: CentOS release 6.2 (Final)
    NETWORK CARD: Intel Corporation I350 Gigabit Network Connection&lt;/p&gt;

&lt;p&gt;测试过程, 在pika 中先写入150G 大小的数据. 写入Hash key 50个, field 1千万级别.
redis 写入5G 大小的数据&lt;/p&gt;

&lt;p&gt;Pika:
18个线程&lt;/p&gt;

&lt;p&gt;redis:
单线程&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/4tFI6kq.png&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;结论: pika 的单线程的性能肯定不如redis, pika是多线程的结构, 因此在线程数比较多的情况下, 某些数据结构的性能可以优于redis&lt;/p&gt;

&lt;h3&gt;wiki&lt;/h3&gt;

&lt;p&gt;github 地址:&lt;/p&gt;

&lt;p&gt;https://github.com/Qihoo360/pika&lt;/p&gt;

&lt;p&gt;github wiki:&lt;/p&gt;

&lt;p&gt;https://github.com/Qihoo360/pika/wiki/pika介绍&lt;/p&gt;

&lt;h2&gt;FAQ&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;如果我们想使用新DB, 那核心问题是如何进行数据迁移. 从redis迁移到pika需要经过几个步骤？&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;开发需要做的：&lt;/p&gt;

&lt;p&gt;开发不需要做任何事，不用改代码、不用替换driver（pika使用原生redis的driver），什么都不用动，看dba干活就好&lt;/p&gt;

&lt;p&gt;dba需要做的：
    1.dba迁移redis数据到pika
    2.dba将redis的数据实时同步到pika，确保redis与pika的数据始终一致
    3.dba切换lvs后端ip，由pika替换redis&lt;/p&gt;

&lt;p&gt;迁移过程中需要停业务/业务会受到影响吗：
    然而并不会&lt;/p&gt;

&lt;p&gt;迁移是无缝且温和的吗：
    那当然&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;这个和你们公司内部的bada 有什么区别?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;我们之前在bada 上面支持过redis 的多数据结构, 并且兼容redis协议, 但是遇到了问题.&lt;/p&gt;

&lt;p&gt;在分布式系统里面, 对key 的hash 场景的通常是两种方案:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;以BigTable 为代表的, 支持range key 的hash 方案. 这个方案的好处是可以实现动态的扩展&lt;/li&gt;
&lt;li&gt;以Dynamo 为代表的, 取模的hash 方案. 这个方案的好处是时间简单&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;我们bada 目前支持的是取模的hash 方案, 在实现redis 的多数据结构的时候, 比如hash 我们采用key取模找到对应的分片. 那么这样带来的问题是由于多数据结构里面key 不多, field 比较多的场景还是大部分的情况, 因此极容易照成分片的不均匀, 性能退化很明显.&lt;/p&gt;

&lt;p&gt;360基础架构组公众号:&lt;/p&gt;

&lt;p&gt; &lt;img src=&quot;http://i.imgur.com/Yk2A0NS.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>谈谈paxos, multi-paxos, raft</title>
   <link href="http://baotiao.github.io//2016/05/paxos-raft/"/>
   <updated>2016-05-05T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2016/05/paxos-raft</id>
   <content type="html">&lt;p&gt;本文假设你已经看过了paxos make simpe, paxos make live, 关于raft 你看过对应的paper, multi-paxos 其实我觉得介绍的最好的还是Diego Ongaro 为了对比raft 和multi-paxos 的学习的难易程度写的&lt;a href=&quot;https://www.youtube.com/watch?v=JEpsBg0AO6o&quot;&gt;视频&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;关于paxos, multi-paxos 的关系&lt;/p&gt;

&lt;p&gt; 其实paxos 是关于对某一个问题达成一致的一个协议. paxos make simple 花大部分的时间解释的就是这个一个提案的问题, 然后在结尾的Implementing a State Machine 的章节介绍了我们大部分的应用场景是对一堆连续的问题达成一致, 所以最简单的方法就是实现每一个问题独立运行一个Paxos 的过程, 但是这样每一个问题都需要Prepare, Accept 两个阶段才能够完成. 所以我们能不能把这个过程给减少. 那么可以想到的解决方案就是把Prepare 减少, 那么就引入了leader, 引入了leader 就必然有选leader 的过程. 才有了后续的事情, 这里可以看出其实lamport 对multi-paxos 的具体实现其实是并没有细节的指定的, 只是简单提了一下. 所以才有各种不同的multi-paxos 的实现&lt;/p&gt;

&lt;p&gt; 那么paxos make live 这个文章里面主要讲的是如何使用multi paxos 实现chubby 的过程, 以及实现过程中需要解决的问题, 比如需要解决磁盘冲突, 如何优化读请求, 引入了Epoch number等, 可以看成是对实现multi-paxos 的一些&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;关于 multi-paxos 和 raft 的关系&lt;/p&gt;

&lt;p&gt; 从上面可以看出其实我们对比的时候不应该拿paxos 和 raft 对比, 因为paxos 是对于一个问题达成一致的协议, 而raft 本身是对一堆连续的问题达成一致的协议. 所以应该比较的是multi-paxos 和raft&lt;/p&gt;

&lt;p&gt; 那么multi-paxos 和 raft 的关系是什么呢?&lt;/p&gt;

&lt;p&gt; raft 是基于对multi paxos 的两个限制形成的&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;发送的请求的是连续的, 也就是说raft 的append 操作必须是连续的. 而paxos 可以并发的. (其实这里并发只是append log 的并发提高, 应用的state machine 还是必须是有序的)&lt;/li&gt;
&lt;li&gt;选主是有限制的, 必须有最新, 最全的日志节点才可以当选. 而multi-paxos 是随意的
所以raft 可以看成是简化版本的multi paxos(这里multi-paxos 因为允许并发的写log, 因此不存在一个最新, 最全的日志节点, 因此只能这么做. 这样带来的麻烦就是选主以后, 需要将主里面没有的log 给补全, 并执行commit 过程)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 基于这两个限制, 因此raft 的实现可以更简单, 因为raft 的但是multi-paxos 的并发度理论上是更高的.&lt;/p&gt;

&lt;p&gt; 可以对比一下multi-paxos 和 raft 可能出现的日志&lt;/p&gt;

&lt;p&gt; &lt;strong&gt;multi-paxos&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt; &lt;img src=&quot;http://i.imgur.com/SsIeodM.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;strong&gt;raft&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt; &lt;img src=&quot;http://i.imgur.com/2KO9khV.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt; 可以看出, raft 里面follower 的log 一定是leader log 的子集, 而raft 不做这个保证&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;关于paxos, multi-paxos, raft 的关系&lt;/p&gt;

&lt;p&gt; 所以我觉得multi-paxos, raft 都是对一堆连续的问题达成一致的协议, 而paxos 是对一个问题达成一致的协议, 因此multi-paxos, raft 其实都是为了简化paxos 在多个问题上面达成一致的需要的两个阶段, 因此都简化了prepare 阶段, 提出了通过有leader 来简化这个过程. multi-paxos, raft 只是简化不一样, raft 让用户的log 必须是有序, 选主必须是有日志最全的节点, 而multi-paxos 没有这些限制. 因此raft 的实现会更简单.&lt;/p&gt;

&lt;p&gt; 因此从这个角度来看, Diego Ongaro 实现raft 这个论文实现的初衷应该是达到了, 让大家更容易理解这个paxos 这个东西&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;关于lock service 和 consensus library 的对比&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;chubby 是根据multi-paxos 实现的一个global lock service, 为什么是lock service 而不是一个consensus service 或者 consensus library呢?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;首先是library 和 service 的对比&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;当用户需要一个consensus 的需求的时候, 一般是随着业务的增长才需要, 那么如果是一个library 的话, 需要对用户的代码改动比较大才能够需求, 而如果是一个service 的话, 那么需要的改动量就非常的小, 仅仅是从consensus service 获得这个服务的地址等等&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;lock service 和 consensus library 区别&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;比如在选主的场景下面,
我们使用lock service 的做法就是其中的某一个process 去获得这个lock, 然后由这个获得lock 的process 进行选主操作, 这里选主主要涉及每一个process 的log.&lt;/p&gt;

&lt;p&gt;另一种是consensus library 的做法, 那么这个时候实现的过程就应该是任意一个process 都可以进行选举操作, 然后选举的过程通过consensus library 来进行.&lt;/p&gt;

&lt;p&gt;从这里可以看出 consensus library 的做法是一个业务的本质需求, 但是实现起来对consensus library 需要有深入的了解, consensus library 和上层的逻辑耦合比较高, 而使用lock service 则是一个更简单, 更清晰的做法&lt;/p&gt;

&lt;p&gt;最后推广一下我们实现的一个元信息管理模块 &lt;a href=&quot;https://github.com/baotiao/floyd&quot;&gt;floyd&lt;/a&gt;, 是一个Library, 而不是一个service. 提供consensus library, 也提供lock library&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>epoll implementation</title>
   <link href="http://baotiao.github.io//2016/03/epoll_implementation/"/>
   <updated>2016-03-06T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2016/03/epoll_implementation</id>
   <content type="html">&lt;h3&gt;epoll Implementation&lt;/h3&gt;

&lt;p&gt;epoll 在kernel 的具体实现主要在 fs/eventpoll.c include/linux/eventpoll.h 里面&lt;/p&gt;

&lt;h4&gt;主要的数据结构&lt;/h4&gt;

&lt;p&gt;eventpoll 是这里面最重要的结构, 在epoll_create 的时候就会生成这个eventpoll 结构, 后续对这个fd 的操作都是在这个eventpoll 这个结构下面的, 这个eventpoll 结构会被保存在file struct 的 private_data 这个结构体里面&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct eventpoll {
    /* Protect the this structure access */
    spinlock_t lock;

    /*
     * This mutex is used to ensure that files are not removed
     * while epoll is using them. This is held during the event
     * collection loop, the file cleanup path, the epoll file exit
     * code and the ctl operations.
     */
    struct mutex mtx;

    /* Wait queue used by sys_epoll_wait() */
  /*
   * 这个wq 这个等待队列是在sys_epoll_wait 里面使用的,
   * 里面放入的进程应该只有执行epoll_wait 这个进程
   */
    wait_queue_head_t wq;

    /* Wait queue used by file-&amp;gt;poll() */
  /*
   * 因为eventpoll 本身也是一个file, 所以也会有poll 操作, 有poll
   * 操作肯定就会有一个对应的wait_queue_head_t 队列用来唤醒上面的进程或者函数
   * 就跟pipe_inode_info 里面会有  wait_queue_head_t wait; 一样
   *
   * 不过我们很少看到把一个epoll 的fd 再挂载到另外一个fd 下面
   *
   */
    wait_queue_head_t poll_wait;

    /* List of ready file descriptors */
  /*
   * 在epoll_wait 阶段, 如果有哪些fd 就绪, 会把就绪的fd 放在这个rdllist 里面
   * 这个rdllist 里面放的是epitem 这个结构
   */
    struct list_head rdllist;

    /* RB tree root used to store monitored fd structs */
    struct rb_root rbr;

    /*
     * This is a single linked list that chains all the &quot;struct epitem&quot; that
     * happened while transfering ready events to userspace w/out
     * holding -&amp;gt;lock.
     */
    struct epitem *ovflist;

    /* The user that created the eventpoll descriptor */
    struct user_struct *user;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;epitem 这个表示的是每一个加入到eventpoll 的结构里面的fd 的时候, 都会有一个epitem 这个结构体, 这个结构体是是连成一个红黑树挂载eventpoll 下面的, 后续查找某一个fd 是否有事件等等都是在这个epitem 上面进行操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/*
 * Each file descriptor added to the eventpoll interface will
 * have an entry of this type linked to the &quot;rbr&quot; RB tree.
 */
struct epitem {
    /* RB tree node used to link this structure to the eventpoll RB tree */
  /*
   * 这是这个fd 挂载的红黑树的节点
   */
    struct rb_node rbn;

    /* List header used to link this structure to the eventpoll ready list */
  /*
   * 这个是将这个epitem 连接到eventpoll 里面的rdllist 的时候list指针
   */
    struct list_head rdllink;

    /*
     * Works together &quot;struct eventpoll&quot;-&amp;gt;ovflist in keeping the
     * single linked chain of items.
     */
    struct epitem *next;

    /* The file descriptor information this item refers to */
  /*
   * epoll 监听的fd
   */
    struct epoll_filefd ffd;

    /* Number of active wait queue attached to poll operations */
  /*
   * 因为一个epitem 可能会被多个eventpoll 监听, 那么就会对应生成多个eppoll_entry
   * 这里nwait 就是记录这个数目
   */
    int nwait;

    /* List containing poll wait queues */
    struct list_head pwqlist;

    /* The &quot;container&quot; of this item */
  /*
   * 当前这个epitem 所属于的eventpoll
   */
    struct eventpoll *ep;

    /* List header used to link this item to the &quot;struct file&quot; items list */
    struct list_head fllink;

    /* The structure that describe the interested events and the source fd */
    struct epoll_event event;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;eppoll_entry
一个epitem 关联到一个eventpoll, 就会有一个对应的eppoll_entry&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/* Wait structure used by the poll hooks */
struct eppoll_entry {
    /* List header used to link this structure to the &quot;struct epitem&quot; */
  /*
   * 因为一个epitem 可以会被多个eventpoll 监听, 因为每一个进程打开的这个文件的fd
   * 和epitem 是一一对应的, 因此这里epitem 对应的是多个eppoll_entry,
   * 所以这个llink 会被连接到pwqlist 里面
   */
    struct list_head llink;

    /* The &quot;base&quot; pointer is set to the container &quot;struct epitem&quot; */
  /*
   * 对应的epitem
   */
    struct epitem *base;

    /*
     * Wait queue item that will be linked to the target file wait
     * queue head.
   * wait_queue_t 里面存的就是wait_queue_head_t 下面具体的内容
     */
  /*
 struct __wait_queue {
     unsigned int flags;
  #define WQ_FLAG_EXCLUSIVE 0x01
  // 这里的这个private 一般指向某一个进程task_struct
  void *private;
  wait_queue_func_t func;
  struct list_head task_list;
  };
  */
    wait_queue_t wait;

    /* The wait queue head that linked the &quot;wait&quot; wait queue item */
    wait_queue_head_t *whead;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;epoll_create 的过程&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;/*
 * Open an eventpoll file descriptor.
 */
SYSCALL_DEFINE1(epoll_create1, int, flags)
{
    int error;
    struct eventpoll *ep = NULL;

    /* Check the EPOLL_* constant for consistency.  */
    BUILD_BUG_ON(EPOLL_CLOEXEC != O_CLOEXEC);

    if (flags &amp;amp; ~EPOLL_CLOEXEC)
        return -EINVAL;
    /*
     * Create the internal data structure (&quot;struct eventpoll&quot;).
   * 初始化建立这个主题的 eventpoll 这个结构
     */
    error = ep_alloc(&amp;amp;ep);
    if (error &amp;lt; 0)
        return error;
    /*
     * Creates all the items needed to setup an eventpoll file. That is,
     * a file structure and a free file descriptor.
   *
   * 这个也是一个重要的过程, 将eventpoll 生成的fd绑定到匿名file上, 
   * 因为我们可以看到epoll_create 出来的也是一个文件fd. 那么这个fd
   * 绑定到一个file 以后, 那么这个epoll具体的内容就在这个struct file 上面的
   * private_data 上面
   *
   * 这里这个error 写的比较坑, 如果正常其实这里返回的就是epoll_create
   * 生成的fd
     */

  // 这里这个ep 就是一直要传下去的epollevent, 最后这个ep 会被赋值到struct
  // file-&amp;gt;private_data 里面去
  // 可以看到这里代表着另外一种文件类型, 匿名文件类型需要建立一个fd的过程
  // 这里其实如果是socket 的话, 那么不一样的地方就是这里传入的不是ep,
  // 而是一个socket
  // 这也是linux 所有数据都是文件的一个体现
    error = anon_inode_getfd(&quot;[eventpoll]&quot;, &amp;amp;eventpoll_fops, ep,
                 flags &amp;amp; O_CLOEXEC);
    if (error &amp;lt; 0)
        ep_free(ep);

    return error;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;epoll_ctl&lt;/h4&gt;

&lt;p&gt;epoll_ctl 做的主要事情就是把某一个fd 要监听的事件过载到eventpoll 里面&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
SYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd,
        struct epoll_event __user *, event)
{
    int error;
    struct file *file, *tfile;
    struct eventpoll *ep;
    struct epitem *epi;
    struct epoll_event epds;

    error = -EFAULT;
  /*
   * 这里检查这次操作是否有这个操作, 并将用户空间传进来的epoll_event 拷贝到内核空间
   */
    if (ep_op_has_event(op) &amp;amp;&amp;amp;
        copy_from_user(&amp;amp;epds, event, sizeof(struct epoll_event)))
        goto error_return;

    /* Get the &quot;struct file *&quot; for the eventpoll file */
    error = -EBADF;
  /*
   * 这里就是使用文件的好处了, 根据这个fd, fget 就可以获得当前这个进程fd
   * 号码是pdfd 的file, 然后就操作file 里面的内容, 虽然这里的file 不是正在的file
   * 而是一个匿名文件, 这样操作起来非常的方便
   */
    file = fget(epfd);
    if (!file)
        goto error_return;

    /* Get the &quot;struct file *&quot; for the target file */
  /*
   * 这里的epoll_ctl 要关注的那个文件的fd, 也一样根据这个fd 找到这个文件
   */
    tfile = fget(fd);
    if (!tfile)
        goto error_fput;

  // TODO 这里descriptoer support poll 是什么意思, 是不是有实现poll 函数就行
    /* The target file descriptor must support poll */
    error = -EPERM;
    if (!tfile-&amp;gt;f_op || !tfile-&amp;gt;f_op-&amp;gt;poll)
        goto error_tgt_fput;

    /*
     * We have to check that the file structure underneath the file descriptor
     * the user passed to us _is_ an eventpoll file. And also we do not permit
     * adding an epoll file descriptor inside itself.
   * 这里可以看到如果把自己的epoll 的fd 添加到 epoll_ctl 里面的fd 是有问题的
     */
    error = -EINVAL;
    if (file == tfile || !is_file_epoll(file))
        goto error_tgt_fput;

    /*
     * At this point it is safe to assume that the &quot;private_data&quot; contains
     * our own data structure.
     */
    ep = file-&amp;gt;private_data;

    mutex_lock(&amp;amp;ep-&amp;gt;mtx);

    /*
     * Try to lookup the file inside our RB tree, Since we grabbed &quot;mtx&quot;
     * above, we can be sure to be able to use the item looked up by
     * ep_find() till we release the mutex.
   * 根据这个fd 找出对应的epitem, 这里把所有的epitem 根据fd号挂载到红黑树上
     */
    epi = ep_find(ep, tfile, fd);

    error = -EINVAL;
    switch (op) {
    case EPOLL_CTL_ADD:
        if (!epi) {
            epds.events |= POLLERR | POLLHUP;
            error = ep_insert(ep, &amp;amp;epds, tfile, fd);
        } else
            error = -EEXIST;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么接下来主要看ep_insert 的过程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
/*
 * Must be called with &quot;mtx&quot; held.
 * 这里可以看到往这个队列里面加入一个epoll_event 的过程
 */
static int ep_insert(struct eventpoll *ep, struct epoll_event *event,
             struct file *tfile, int fd)
{
    int error, revents, pwake = 0;
    unsigned long flags;
    struct epitem *epi;
    struct ep_pqueue epq;

    if (unlikely(atomic_read(&amp;amp;ep-&amp;gt;user-&amp;gt;epoll_watches) &amp;gt;=
             max_user_watches))
        return -ENOSPC;
    if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL)))
        return -ENOMEM;

    /* Item initialization follow here ... */
    INIT_LIST_HEAD(&amp;amp;epi-&amp;gt;rdllink);
    INIT_LIST_HEAD(&amp;amp;epi-&amp;gt;fllink);
    INIT_LIST_HEAD(&amp;amp;epi-&amp;gt;pwqlist);
  /*
   * 这里会反向标记这个epitem 属于的eventpoll
   */
    epi-&amp;gt;ep = ep;
  /*
   * 这里就是去设定 epitem 里面的ffd 字段, 标记里面的ffd 字段
   * 就是初始化好file, fd 字段
   */
    ep_set_ffd(&amp;amp;epi-&amp;gt;ffd, tfile, fd);
    epi-&amp;gt;event = *event;
    epi-&amp;gt;nwait = 0;
    epi-&amp;gt;next = EP_UNACTIVE_PTR;

    /* Initialize the poll table using the queue callback */
    epq.epi = epi;
  /*
   * 这里是注册poll 里面有事件到达的时候的处理函数
   * 在ep_ptable_queue_proc 里面去添加事件到达的时候处理函数
   * init_waitqueue_func_entry(&amp;amp;pwq-&amp;gt;wait, ep_poll_callback);
   */
    init_poll_funcptr(&amp;amp;epq.pt, ep_ptable_queue_proc);

    /*
     * Attach the item to the poll hooks and get current event bits.
     * We can safely use the file* here because its usage count has
     * been increased by the caller of this function. Note that after
     * this operation completes, the poll callback can start hitting
     * the new item.
     */

  /* 
   *
   * 这里调用对应的fd 的poll 操作, 看是否有事件到达.
   * 并且注册事件到达的时候poll_table
   * 这里是调用用户关注的fd 的.poll()函数, 不是eventpoll 里面的
   * ep_eventpoll_poll
   */  
    revents = tfile-&amp;gt;f_op-&amp;gt;poll(tfile, &amp;amp;epq.pt);

    /*
     * We have to check if something went wrong during the poll wait queue
     * install process. Namely an allocation for a wait queue failed due
     * high memory pressure.
     */
    error = -ENOMEM;
    if (epi-&amp;gt;nwait &amp;lt; 0)
        goto error_unregister;

    /* Add the current item to the list of active epoll hook for this file */
    spin_lock(&amp;amp;tfile-&amp;gt;f_lock);
    list_add_tail(&amp;amp;epi-&amp;gt;fllink, &amp;amp;tfile-&amp;gt;f_ep_links);
    spin_unlock(&amp;amp;tfile-&amp;gt;f_lock);

    /*
     * Add the current item to the RB tree. All RB tree operations are
     * protected by &quot;mtx&quot;, and ep_insert() is called with &quot;mtx&quot; held.
   * 这里是具体把这个epitem 加入到这个eventpoll 的 rb tree 的过程
     */
    ep_rbtree_insert(ep, epi);

    /* We have to drop the new item inside our item list to keep track of it */
    spin_lock_irqsave(&amp;amp;ep-&amp;gt;lock, flags);

    /* If the file is already &quot;ready&quot; we drop it inside the ready list */
    if ((revents &amp;amp; event-&amp;gt;events) &amp;amp;&amp;amp; !ep_is_linked(&amp;amp;epi-&amp;gt;rdllink)) {
        list_add_tail(&amp;amp;epi-&amp;gt;rdllink, &amp;amp;ep-&amp;gt;rdllist);

        /* Notify waiting tasks that events are available */
        if (waitqueue_active(&amp;amp;ep-&amp;gt;wq))
            wake_up_locked(&amp;amp;ep-&amp;gt;wq);
        if (waitqueue_active(&amp;amp;ep-&amp;gt;poll_wait))
            pwake++;
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么到这里已经将这个fd 注册到对应的eventpoll上, 那么我们看一下这里注册的函数, 函数是&lt;/p&gt;

&lt;p&gt;init_poll_funcptr(&amp;amp;epq.pt, ep_ptable_queue_proc);
这个函数ep_ptable_queue_proc 里面又会包含对应的当有事件发生的时候的回调函数 ep_poll_callback&lt;/p&gt;

&lt;p&gt;那么接下来就是 ep_table_queue_proc 的实现&lt;/p&gt;

&lt;p&gt;在ep_table_queue_proc 里面将当前这个进程注册到了想要监听的fd 的唤醒队列里面, 这个唤醒的行数是 ep_poll_callback,&lt;/p&gt;

&lt;p&gt;其实唤醒的时候主要分两种类似
1. 唤醒注册时候的进程, 让注册的进程重新执行. 比如在epoll_wait 的时候对应的唤醒函数就是唤醒这个执行 epoll_wait 的这个进程
2. 唤醒的时候执行注册的某一个函数&lt;/p&gt;

&lt;p&gt;这里因为epoll 注册的是唤醒的时候执行注册的某一个函数, 这个函数就是ep_poll_callback&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
/*
 * This is the callback that is used to add our wait queue to the
 * target file wakeup lists.
 */
static void ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead,
                 poll_table *pt)
{
  /*
   * 从知道poll_table 的地址去获得这个对应的epitem的地址, 跟kernel 里面list的做法类似
   */
    struct epitem *epi = ep_item_from_epqueue(pt);


  /*
   * 这里eppoll_entry 是对应于每一个epitem 注册挂载到某一个eventpoll 都会有一个eppoll_entry
   */
    struct eppoll_entry *pwq;

    if (epi-&amp;gt;nwait &amp;gt;= 0 &amp;amp;&amp;amp; (pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL))) {
    /*
     * 这里就是把pwq-&amp;gt;wait 这个元素的callback 设置成 ep_poll_callback 这个函数
     * 然后初始化的时候一般把这个private 设置NULL 的原因是
     * 这里当要等待的这个队列被唤醒的时候, 并不在于哪一个进程,
     * 只要唤醒的时候执行指定的函数就可以了
     * 需要看一下唤醒的时候具体是怎么执行的,
     * 是不是唤醒的时候如果注册的是函数就执行指定的函数,
     * 如果是进程就执行指定的进程
     */
        init_waitqueue_func_entry(&amp;amp;pwq-&amp;gt;wait, ep_poll_callback);
        pwq-&amp;gt;whead = whead;
        pwq-&amp;gt;base = epi;
    // 这里就是将这个要等待的fd 添加到等待队列里面去了,
    // 这里如果这个要等待的fd是pipe, 那么这里这个whead就是pipe-&amp;gt;wait了
    // 这里要等待的fd 只有  wait_queue_head_t wait; 这个wait_queue_head,
    // 然后就是将这里epoll_entry 里面的wait_queue_t wait 连在一起,
    // 实际他们组合成了一个等待的队列, 然后想取得这个队列里面的元素, 跟kernel
    // list 一样, 需要根据struct 里面某一个元素的地址, 然后去获得这个元素
    // 比如这里想反向获得这个 epoll_entry 里面的epitem 就是
    //   struct epitem *epi = ep_item_from_wait(wait);
        add_wait_queue(whead, &amp;amp;pwq-&amp;gt;wait);
        list_add_tail(&amp;amp;pwq-&amp;gt;llink, &amp;amp;epi-&amp;gt;pwqlist);
        epi-&amp;gt;nwait++;
    } else {
        /* We have to signal that an error occurred */
        epi-&amp;gt;nwait = -1;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ep_poll_callback&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/*
 * This is the callback that is passed to the wait queue wakeup
 * machanism. It is called by the stored file descriptors when they
 * have events to report.
 * 这里就是被static void __wake_up_common 这个函数下面进行循环调用的
 *
 * 也就是某一个fd 的等待队列里面注册的这个函数的callback,
 * 然后这个fd有事件发生了, 就会执行这个ep_poll_callback 函数了
 *
 * 这个key 参数是从 wake_up_interruptible_poll() 函数里面一直传进来的, 
 * 这里这个key 在这里表示的就是要监听的这个fd 有什么事件 
 *
 * 其实这里epitem 里面已经包含了发生的事件, 和这里的key 参数其实是一样了,
 * 因为并不是所有的device 都会把发生的事件都放在key 这个参数里面的
 *
 * 这个函数主要就是由监听的fd发生事件了, 然后触发这个回调函数, 
 * 然后这里会把触发的fd 添加到eventpoll 中的 rdllist
 *
 */
static int ep_poll_callback(wait_queue_t *wait, unsigned mode, int sync, void *key)
{
    int pwake = 0;
    unsigned long flags;
  /*
   * 因为之前王wait_queue_t 里面放的是epoll_entry 这个struct 里面的wait 元素,
   * 这里加入的地方是在eventpoll.c:ep_ptable_queue_proc
   * 这个函数里面的
   * add_wait_queue(whead, &amp;amp;pwq-&amp;gt;wait);
   * 在这个函数里面有详细的注释
   * 这样要做的事情就是根据struct 里面的某一个元素wait, 去获得这个struct
   * epoll_entry 里面的epitem
   * 到这里的时候, 我们已经知道是这个epitem 里面指定的fd 有时间发生,
   * 那么这个时候就去检查这个fd 里面发生了什么事件
   */
    struct epitem *epi = ep_item_from_wait(wait);


  /*
   * 这里由于需要记录这个epitem 是属于哪一个eventpoll, 因此epitem
   * 会保留这个反向的指向eventpoll 的指针
   */
    struct eventpoll *ep = epi-&amp;gt;ep;

    spin_lock_irqsave(&amp;amp;ep-&amp;gt;lock, flags);

    /*
     * If the event mask does not contain any poll(2) event, we consider the
     * descriptor to be disabled. This condition is likely the effect of the
     * EPOLLONESHOT bit that disables the descriptor when an event is received,
     * until the next EPOLL_CTL_MOD will be issued.
     */
    if (!(epi-&amp;gt;event.events &amp;amp; ~EP_PRIVATE_BITS))
        goto out_unlock;

    /*
     * Check the events coming with the callback. At this stage, not
     * every device reports the events in the &quot;key&quot; parameter of the
     * callback. We need to be able to handle both cases here, hence the
     * test for &quot;key&quot; != NULL before the event match test.
   * 这里其实key 有可能记录了当前这个fd的事件, 如果记录了,
   * 就判断key里面返回的事件和 这个fd关注的事件是否有重合
     */
    if (key &amp;amp;&amp;amp; !((unsigned long) key &amp;amp; epi-&amp;gt;event.events))
        goto out_unlock;

    /*
     * If we are trasfering events to userspace, we can hold no locks
     * (because we&#39;re accessing user memory, and because of linux f_op-&amp;gt;poll()
     * semantics). All the events that happens during that period of time are
     * chained in ep-&amp;gt;ovflist and requeued later on.
     */
  /*
   * 这里把这个epi 添加到ovflist
   */
    if (unlikely(ep-&amp;gt;ovflist != EP_UNACTIVE_PTR)) {
        if (epi-&amp;gt;next == EP_UNACTIVE_PTR) {
            epi-&amp;gt;next = ep-&amp;gt;ovflist;
            ep-&amp;gt;ovflist = epi;
        }
        goto out_unlock;
    }

    /* If this file is already in the ready list we exit soon */
  // 这个epitem 已经在rdllist 里面就不再添加
  // 这里只要判断这个epitem-&amp;gt;rdllink 是否有连接上, 也就是next
  // 指针是否为空就可以知道这个epitem 有没有被连接成一个List
    if (!ep_is_linked(&amp;amp;epi-&amp;gt;rdllink))
        list_add_tail(&amp;amp;epi-&amp;gt;rdllink, &amp;amp;ep-&amp;gt;rdllist);

    /*
     * Wake up ( if active ) both the eventpoll wait list and the -&amp;gt;poll()
     * wait list.
     */
    if (waitqueue_active(&amp;amp;ep-&amp;gt;wq))
        wake_up_locked(&amp;amp;ep-&amp;gt;wq); // 这里的wake_up ep-&amp;gt;wq 就把等待在epoll_wait 里面的直接schedule_timeout()的那个函数给唤醒
  // 那么这个时候如果进程等待在epoll_wait 上面, 进程就会重新执行
    if (waitqueue_active(&amp;amp;ep-&amp;gt;poll_wait)) // 这里这个默认的pwake 初始化是0, 表示当前这个ep-&amp;gt;poll_wait 所等待的fd 里面有多少个已经wakeup了
    // 为什么这里需要把加这个pwake, 因为如果不加这个pwake, 那么每一个ep
    // 所等待的fd 醒来都去wakeup 一下这个ep这个线程,
    // 那么ep这个线程其实就被wakeup 了很多次
        pwake++;

out_unlock:
    spin_unlock_irqrestore(&amp;amp;ep-&amp;gt;lock, flags);

    /* We have to call this outside the lock */
    if (pwake)
        ep_poll_safewake(&amp;amp;ep-&amp;gt;poll_wait);

    return 1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;epoll_wait&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;
/*
 * Implement the event wait interface for the eventpoll file. It is the kernel
 * part of the user space epoll_wait(2).
 */
SYSCALL_DEFINE4(epoll_wait, int, epfd, struct epoll_event __user *, events,
        int, maxevents, int, timeout)
{
    int error;
    ...
    error = ep_poll(ep, events, maxevents, timeout);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要的执行逻辑在 ep_poll() 上&lt;/p&gt;

&lt;p&gt;ep_poll()&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events,
           int maxevents, long timeout)
{
    int res, eavail;
    unsigned long flags;
    long jtimeout;
    wait_queue_t wait;

    /*
     * Calculate the timeout by checking for the &quot;infinite&quot; value (-1)
     * and the overflow condition. The passed timeout is in milliseconds,
     * that why (t * HZ) / 1000.
     */
    jtimeout = (timeout &amp;lt; 0 || timeout &amp;gt;= EP_MAX_MSTIMEO) ?
        MAX_SCHEDULE_TIMEOUT : (timeout * HZ + 999) / 1000;

retry:
    spin_lock_irqsave(&amp;amp;ep-&amp;gt;lock, flags);

    res = 0;
    if (list_empty(&amp;amp;ep-&amp;gt;rdllist)) {
        /*
         * We don&#39;t have any available event to return to the caller.
         * We need to sleep here, and we will be wake up by
         * ep_poll_callback() when events will become available.
     *
     * 由于ep_poll 是被epoll_wait 的时候被调用, 因此这里这个current
     * 进程就是调用epoll_wait 的那个进程
     *
     * 将这个触发当前进程执行的wait_queue_t wait 加入到ep-&amp;gt;wq wait_queue_head_t
     * 里面去, 那么下次这个wait_queue 有事件到达的时候, 就会触发当前这个wait
     * 里面的内容
         */
        init_waitqueue_entry(&amp;amp;wait, current);
        wait.flags |= WQ_FLAG_EXCLUSIVE;
        __add_wait_queue(&amp;amp;ep-&amp;gt;wq, &amp;amp;wait);

    /*
     * 这里这个死循环的做法是将current 进程的状态改成TASK_INTERRUPTIBLE,
     * 然后最后重新陷入到执行schedule(),
     * 这里因为当前的进程的状态已经改成TASK_INTERRUPTIBLE了,
     * 因此肯定会切换到其他进程去执行
     */
        for (;;) {
            /*
             * We don&#39;t want to sleep if the ep_poll_callback() sends us
             * a wakeup in between. That&#39;s why we set the task state
             * to TASK_INTERRUPTIBLE before doing the checks.
             */
            set_current_state(TASK_INTERRUPTIBLE);
      /*
       * 这里可以看到如果有ep rdllist 里面有元素, 那么说明有时间已经出发了,
       * 那么就退出这个for 循环
       */
            if (!list_empty(&amp;amp;ep-&amp;gt;rdllist) || !jtimeout)
                break;
            if (signal_pending(current)) {
                res = -EINTR;
                break;
            }

            spin_unlock_irqrestore(&amp;amp;ep-&amp;gt;lock, flags);
      /*
       * 当前这个进程把CPU 交给其他的进程执行, 当切换回来的时候肯定eventpoll
       * 里面的内容应该会被修改了, 比如某一个事件触发的时候, 会往rdllist
       * 里面写入数据内容
       *
       * 这里就开始等待之前在epoll_insert
       * 注册的ep_poll_callback这个函数来把当前这个进程给唤醒,
       * 否则是sleep一段时间以后就返回了
       */
            jtimeout = schedule_timeout(jtimeout);
            spin_lock_irqsave(&amp;amp;ep-&amp;gt;lock, flags);
        }
    /*
     * 当前这个进程已经被唤醒, 那么就从eventpoll 里面把这个进程删除掉
     */
        __remove_wait_queue(&amp;amp;ep-&amp;gt;wq, &amp;amp;wait);

    /*
     * 设置当前的进程的状态, 到这里我觉得进程已经正常运行了
     */
        set_current_state(TASK_RUNNING);
    }
    /* Is it worth to try to dig for events ? */
    eavail = !list_empty(&amp;amp;ep-&amp;gt;rdllist) || ep-&amp;gt;ovflist != EP_UNACTIVE_PTR;

    spin_unlock_irqrestore(&amp;amp;ep-&amp;gt;lock, flags);

    /*
     * Try to transfer events to user space. In case we get 0 events and
     * there&#39;s still timeout left over, we go trying again in search of
     * more luck.
   * 到这里肯定从schedule_timeout 里面退出, 已经有事件发生并且传送过来了,
   * 这个就是就把这些事件传给用户进程
     */
    if (!res &amp;amp;&amp;amp; eavail &amp;amp;&amp;amp;
        !(res = ep_send_events(ep, events, maxevents)) &amp;amp;&amp;amp; jtimeout)
        goto retry;

    return res;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后就是将发生的events 传递给用户空间&lt;/p&gt;

&lt;p&gt;ep_send_events-&gt;ep_scan_ready_list-&gt;ep_send_events_proc&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * ep_scan_ready_list - Scans the ready list in a way that makes possible for
 *                      the scan code, to call f_op-&amp;gt;poll(). Also allows for
 *                      O(NumReady) performance.
 *
 * @ep: Pointer to the epoll private data structure.
 * @sproc: Pointer to the scan callback.
 * @priv: Private opaque data passed to the @sproc callback.
 *
 * Returns: The same integer error code returned by the @sproc callback.
 */
static int ep_scan_ready_list(struct eventpoll *ep,
                  int (*sproc)(struct eventpoll *,
                       struct list_head *, void *),
                  void *priv)
{
    int error, pwake = 0;
    unsigned long flags;
    struct epitem *epi, *nepi;
    LIST_HEAD(txlist);

    /*
     * We need to lock this because we could be hit by
     * eventpoll_release_file() and epoll_ctl().
     */
    mutex_lock(&amp;amp;ep-&amp;gt;mtx);

    /*
     * Steal the ready list, and re-init the original one to the
     * empty list. Also, set ep-&amp;gt;ovflist to NULL so that events
     * happening while looping w/out locks, are not lost. We cannot
     * have the poll callback to queue directly on ep-&amp;gt;rdllist,
     * because we want the &quot;sproc&quot; callback to be able to do it
     * in a lockless way.
     */
    spin_lock_irqsave(&amp;amp;ep-&amp;gt;lock, flags);

  // 这里将txlist 指向eventpoll 里面的rdllist, 那么接下来就会将txlist
  // 里面的内容也就是rdllist里面的内容拷贝给用户空间
  // 这个rdllist 是在ep_poll_callback 的时候添加进去的内容的
  // 这里执行完这个list_splice_init 以后, ep-&amp;gt;rdllist 就变成空的了
  // 那么接下来可以同时处理txllist 里面的内容, 也就是原先rdllist 里面的内容.
  // 也可以同时让新的元素往这个队里里面添加新的内容
  //
  // 这是一个很好的减少锁范围的例子
  // 相当于先生成一个队列, 然后后续要处理这个队列里面内容,
  // 同时又添加和删除操作的时候, 可以把这个添加操作放到一个新的队列上,
  // 那么这个时候就可以同时的添加和删除操作, 不用去竞争一个锁
  // NICE
    list_splice_init(&amp;amp;ep-&amp;gt;rdllist, &amp;amp;txlist);
    ep-&amp;gt;ovflist = NULL;
    spin_unlock_irqrestore(&amp;amp;ep-&amp;gt;lock, flags);

    /*
     * Now call the callback function.
   * 这里的sproc 就是 ep_send_events_proc
     */
    error = (*sproc)(ep, &amp;amp;txlist, priv);

    spin_lock_irqsave(&amp;amp;ep-&amp;gt;lock, flags);
    /*
     * During the time we spent inside the &quot;sproc&quot; callback, some
     * other events might have been queued by the poll callback.
     * We re-insert them inside the main ready-list here.
     */
    for (nepi = ep-&amp;gt;ovflist; (epi = nepi) != NULL;
         nepi = epi-&amp;gt;next, epi-&amp;gt;next = EP_UNACTIVE_PTR) {
        /*
         * We need to check if the item is already in the list.
         * During the &quot;sproc&quot; callback execution time, items are
         * queued into -&amp;gt;ovflist but the &quot;txlist&quot; might already
         * contain them, and the list_splice() below takes care of them.
         */
        if (!ep_is_linked(&amp;amp;epi-&amp;gt;rdllink))
            list_add_tail(&amp;amp;epi-&amp;gt;rdllink, &amp;amp;ep-&amp;gt;rdllist);
    }
    /*
     * We need to set back ep-&amp;gt;ovflist to EP_UNACTIVE_PTR, so that after
     * releasing the lock, events will be queued in the normal way inside
     * ep-&amp;gt;rdllist.
     */
    ep-&amp;gt;ovflist = EP_UNACTIVE_PTR;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ep_send_events_proc 函数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这里priv 就是用户传进来的events 的包装 ep_send_events_data
// 这里就是具体的把已经就绪的list copy 给用户空间
static int ep_send_events_proc(struct eventpoll *ep, struct list_head *head,
                   void *priv)
{
  /*
   * 这里esed 里面的内容就是 ep-&amp;gt;rdllist 里面的内容
   */
    struct ep_send_events_data *esed = priv;
    int eventcnt;
    unsigned int revents;
    struct epitem *epi;
    struct epoll_event __user *uevent;

    /*
     * We can loop without lock because we are passed a task private list.
     * Items cannot vanish during the loop because ep_scan_ready_list() is
     * holding &quot;mtx&quot; during this call.
     */
    for (eventcnt = 0, uevent = esed-&amp;gt;events;
         !list_empty(head) &amp;amp;&amp;amp; eventcnt &amp;lt; esed-&amp;gt;maxevents;) {
        epi = list_first_entry(head, struct epitem, rdllink);

    /*
     * 这里就是默认将这个有时间到达的epi从这个rdllist 里面删除掉了,
     * 下面可以看到如果是epoll LT 模式的话, 这个epi-&amp;gt;rdllink
     * 会重新被加入到这个rdllist 里面
     */
        list_del_init(&amp;amp;epi-&amp;gt;rdllink);

    /*
     * 返回当前这个监听的fd的事件 &amp;amp; 这个fd 我们关注的事件
     */
        revents = epi-&amp;gt;ffd.file-&amp;gt;f_op-&amp;gt;poll(epi-&amp;gt;ffd.file, NULL) &amp;amp;
            epi-&amp;gt;event.events;

        /*
         * If the event mask intersect the caller-requested one,
         * deliver the event to userspace. Again, ep_scan_ready_list()
         * is holding &quot;mtx&quot;, so no operations coming from userspace
         * can change the item.
         */
    /*
     * 如果这个fd发生的事件有我们关注的事件,
     * 那么我们就把这个事件返回到用户空间去
     */
        if (revents) {
            if (__put_user(revents, &amp;amp;uevent-&amp;gt;events) ||
                __put_user(epi-&amp;gt;event.data, &amp;amp;uevent-&amp;gt;data)) {
        /*
         * 这里是向用户空间拷贝函数失败的分支
         * 这里是如果失败, 那么直接返回当前有多少个事件
         * 如果一个事件也没用, 那么直接返回EFAULT
         */
                list_add(&amp;amp;epi-&amp;gt;rdllink, head);
                return eventcnt ? eventcnt : -EFAULT;
            }
            eventcnt++;
            uevent++;
      // 这里上面这个分支就是EPOLLET 的情况
            if (epi-&amp;gt;event.events &amp;amp; EPOLLONESHOT)
                epi-&amp;gt;event.events &amp;amp;= EP_PRIVATE_BITS;
            else if (!(epi-&amp;gt;event.events &amp;amp; EPOLLET)) {
        /*
         * 这里是EPOLLLT 的情况, EPOLLLT 是默认情况, 从这里看出, LT
         * 模式会将这个fd重新加入到这个rdllist 里面, 那么下次会再通知这个fd
         * 里面的事件. 那么问题来了, 什么时候才会从这个rdllist 里面删除呢?
         * 
         * 这个删除操作是默认都做的, 就在上面
         * list_del_init(&amp;amp;epi-&amp;gt;rdllink);
         *
         * 比如说pipe 写进来了5个byte 的内容, 第一次读取了2个字节, 然后就返回了,
         * 在下一次执行epoll_wai()的时候, 因为LT 模式默认又加入了rdllist里面,
         * 所有会再一次检查这个fd 里面的状态, 这个时候发现里面还是有数据,
         * 就再一次返回给用户这个状态是
         *
         * 如果是ET模式, 这里可以看到不会添加回去这个rdllist,
         * 那么只有等到这个pipe里面再写入数据的时候才会触发ep_poll_callback把这个fd添加到rdllist里面,
         *
         * 从这里可以看出EPOLL_LT模式是多做了一次的检查,
         * 因为每次完成以后又会加入到这个rdllist里面, 因此性能肯定不如ET模式,
         * 但是ET模式存在说如果一次读取fd没有都读完,
         * 那么必须等到这个fd再有事件过来, 才会通知这个fd
         *
         * 另外一个问题: 这里直接往rdllist 里面添加元素, 这里遍历这个txlist
         * 不会直接把这个元素给遍历到么? 因为这里正好添加到了这个list的尾部.
         * 注释里面为什么写着是下一次的 epoll_wait 在进行一次判断呢?
         *
         * 答: 这里是epoll 实现的时候对锁粒度的一个优化, 每一次有时间的epitem
         * 是存入到idllist 这个队列里面, 然后在进行要传给用户空间操作的时候
         * ep_send_events_proc, 在上面讲这个idllist 的指针传递给了txlist指针,
         * 那么接下来就可以同时往idllist 里面添加元素,
         * 又可以处理txlist(也就是原先的rdllist)上面的内容, 这里就不需要加锁了
         */
                /*
                 * If this file has been added with Level
                 * Trigger mode, we need to insert back inside
                 * the ready list, so that the next call to
                 * epoll_wait() will check again the events
                 * availability. At this point, noone can insert
                 * into ep-&amp;gt;rdllist besides us. The epoll_ctl()
                 * callers are locked out by
                 * ep_scan_ready_list() holding &quot;mtx&quot; and the
                 * poll callback will queue them in ep-&amp;gt;ovflist.
                 */
                list_add_tail(&amp;amp;epi-&amp;gt;rdllink, &amp;amp;ep-&amp;gt;rdllist);
            }
        }
    }

    return eventcnt;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Tips:&lt;/h3&gt;

&lt;h5&gt;如何唤醒监听的fd&lt;/h5&gt;

&lt;p&gt;这里加入我们有队列注册在某一个fd 上面, 那么当这个fd 有时间到达的时候, 是如何唤醒这个队列里面所有的进程呢&lt;/p&gt;

&lt;p&gt;比如具体的 socket 类型的文件, 那么当这个文件有事件到达的时候, 如何进行唤醒的呢?
这里比如tcp 调用的就是  tcp_prequeue 方法, 然后里面有 wake_up_interruptible_poll
这里wake_up_interruptible_poll 就是去调用一个唤醒队列的方法了
最后都会调用到kernel/sched.c:__wake_up_common 方法
这里就跟进程的调度模块比较相关了&lt;/p&gt;

&lt;h5&gt;对了file_operations 里面的poll 操作的解释&lt;/h5&gt;

&lt;p&gt;poll(file, poll_table)
Checks whether there is activity on a file and goes to sleep until something happens on it.
可以看上一个blog&lt;/p&gt;

&lt;h5&gt;当一个fd上面有事件发生的时候, 是会唤醒监听这个文件上等待的进程的. 这个是怎么做到的?&lt;/h5&gt;

&lt;p&gt;每一个设备在linux 看来都是一个文件, 那么对于文件操作来说, 每一个都需要实现file_operations 里面的poll 这个操作, 这个操作的意思是检查这个fd是否活动, 可以看上一篇blog 对应的pollc操作&lt;/p&gt;

&lt;p&gt;比如以 pipe 的实现举例子的话&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct pipe_inode_info {
  wait_queue_head_t wait;
  unsigned int nrbufs, curbuf;
  struct page *tmp_page;
  unsigned int readers;

这里有一个pipe_inode_info, 如果当这个pipe 有数据写入的时候
static ssize_t
pipe_read(struct kiocb *iocb, const struct iovec *_iov,
     unsigned long nr_segs, loff_t pos)
{
    if (do_wakeup) {
      wake_up_interruptible_sync(&amp;amp;pipe-&amp;gt;wait);
      kill_fasync(&amp;amp;pipe-&amp;gt;fasync_writers, SIGIO, POLL_OUT);
    }
}

最后的wake_up 函数其实最后会调用到
static void __wake_up_common(wait_queue_head_t *q, unsigned int mode,
      int nr_exclusive, int wake_flags, void *key)

这个函数里面去了, 然后最后的调用是
    /*
     * 这里的func 默认会调用到default_wake_function, 因为在wait_queue_t
     * 初始化的时候回设置func = default_wake_function
     * 如果有注册函数就执行对应的注册函数
     */
    if (curr-&amp;gt;func(curr, mode, wake_flags, key) &amp;amp;&amp;amp;
        (flags &amp;amp; WQ_FLAG_EXCLUSIVE) &amp;amp;&amp;amp; !--nr_exclusive)
      break;
  }
就是执行注册的函数
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后就会调用wake_up_interruptible_sync, 这个就把pipe_inode_info 上面的wait_queue_head_t 上面的等待进程队列唤醒, 那么这里pipe-&gt;wait 上面的这个等待队列是什么时候加上去的呢?&lt;/p&gt;

&lt;p&gt;说明每一个文件都有这个文件的唤醒队列, 当这个文件有操作的时候, 会通过wake_up 操作来将这个唤醒队列的进程给唤醒,&lt;/p&gt;

&lt;h4&gt;那么一般是怎么加入到这个唤醒队列里面的呢?&lt;/h4&gt;

&lt;p&gt;首先是注册到这个fd 的wait_queue_t 里面&lt;/p&gt;

&lt;p&gt;过程是在epoll_insert 的时候会调用一下这个pipe fd的poll() 函数
这里在epoll 里面会调用&lt;/p&gt;

&lt;p&gt;revents = tfile-&gt;f_op-&gt;poll(tfile, &amp;amp;epq.pt);&lt;/p&gt;

&lt;p&gt;这个操作就是将当前的进程加入到tfile 这个fd 的唤醒队列里面去.&lt;/p&gt;

&lt;p&gt;对应于在pipe 里面的调用就是&lt;/p&gt;

&lt;p&gt;pipe_poll(struct file &lt;em&gt;filp, poll_table &lt;/em&gt;wait)&lt;/p&gt;

&lt;p&gt;然后在pipe_poll 里面调到的最重要的是&lt;/p&gt;

&lt;p&gt;poll_wait(filp, &amp;amp;pipe-&gt;wait, wait);&lt;/p&gt;

&lt;p&gt;poll_wait 本质到最后调用的是ep_ptable_queue_proc函数, 因为在epoll 调用tfile-&gt;f_op-&gt;poll 的时候已经注册了这个回调函数&lt;/p&gt;

&lt;p&gt;static void ep_ptable_queue_proc(struct file &lt;em&gt;file, wait_queue_head_t &lt;/em&gt;whead,
         poll_table *pt)
{&lt;/p&gt;

&lt;h4&gt;为什么说 poll/select 的性能比较低&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;在poll/select 的实现里面, 如果监听多个fd, 只要其中有一个fd 有事件达到, 那么久遍历一个list 去检查到底是哪一个事件到达, 并没有像epoll 一样将这些fd 放在一个红黑树上&lt;/li&gt;
&lt;/ol&gt;


&lt;pre&gt;&lt;code&gt;
  /*
   * 程序在这里面schedule_timeout 中sleep, 如果有事件到达,
   * 从schedule_timeout中唤醒, 然后重新执行逻辑
   */
    for (;;) {
        unsigned long *rinp, *routp, *rexp, *inp, *outp, *exp;

        inp = fds-&amp;gt;in; outp = fds-&amp;gt;out; exp = fds-&amp;gt;ex;
        rinp = fds-&amp;gt;res_in; routp = fds-&amp;gt;res_out; rexp = fds-&amp;gt;res_ex;

    /*
     * 这里检查监听的所有的fd 的状态, 如果这个fd 有事件, 就把对应的rinp, routp,
     * rexp 进行修改
     * 这里可以看到, 即使只有一个fd有事件到达, 这里也要把所有的fd 都遍历一遍,
     * 这就是性能很低的原因了
     */
        for (i = 0; i &amp;lt; n; ++rinp, ++routp, ++rexp) {
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;在进行select 的过程中, 是先将要监听的fd 从用户空间拷贝到内核空间, 然后在内核空间里面进行修改以后, 在拷贝回去给用户空间. 这里就设计到内核空间申请内存, 释放内存等等过程&lt;/li&gt;
&lt;/ol&gt;


&lt;pre&gt;&lt;code&gt;int core_sys_select(int n, fd_set __user *inp, fd_set __user *outp,
               fd_set __user *exp, struct timespec *end_time)
{
    fd_set_bits fds;
    void *bits;
    int ret, max_fds;
    unsigned int size;
    struct fdtable *fdt;
  // 可以看到select 是先尝试分配空间在栈上, 下面代码可以看出加入这个需要监听的fd
  // 比较多, 那么就会去申请堆空间里面的内容
  // 这里SELECT_STACK_ALLOC 默认是 256, 那么也就是这个当监听的fd
  // 的个数小于256个的时候, 这里的空间实在栈上,
  // 如果大于256其实空间实在堆上面申请的
    /* Allocate small arguments on the stack to save memory and be faster */

    long stack_fds[SELECT_STACK_ALLOC/sizeof(long)];

    ret = -EINVAL;
    if (n &amp;lt; 0)
        goto out_nofds;

    /* max_fds can increase, so grab it once to avoid race */
    rcu_read_lock();
    fdt = files_fdtable(current-&amp;gt;files);
    max_fds = fdt-&amp;gt;max_fds;
    rcu_read_unlock();
    if (n &amp;gt; max_fds)
        n = max_fds;

    /*
     * We need 6 bitmaps (in/out/ex for both incoming and outgoing),
     * since we used fdset we need to allocate memory in units of
     * long-words. 
     */
    size = FDS_BYTES(n);
    bits = stack_fds;
    if (size &amp;gt; sizeof(stack_fds) / 6) {
        /* Not enough space in on-stack array; must use kmalloc */
        ret = -ENOMEM;
        bits = kmalloc(6 * size, GFP_KERNEL);
        if (!bits)
            goto out_nofds;
    }
  /*
   * 这里分别为in, out, ex, res_in, res_out, res_ex 初始化指针到对应的空间里面
   *
   * 我们经常说的select 需要拷贝这个fd 值得就是这里,
   * 就是将用户空间注册的要监听的fd inp, outp, exp 拷贝到fds.in, fds.out,
   * fds.ex. 然后这些fd 的事件的结果我们保存在 fds.res_in, fds.res_out,
   * fds.res_ex. 然后再将这里面的内容拷贝回去到用户空间 inp, outp, exp
   */
    fds.in      = bits;
    fds.out     = bits +   size;
    fds.ex      = bits + 2*size;
    fds.res_in  = bits + 3*size;
    fds.res_out = bits + 4*size;
    fds.res_ex  = bits + 5*size;

  /*
   * 这里就是我们经常说的select需要去用户空间拷贝内容的代码
   * 这里就是把存在inp 里面注册的内容拷贝到 fds.in 里面
   */
    if ((ret = get_fd_set(n, inp, fds.in)) ||
        (ret = get_fd_set(n, outp, fds.out)) ||
        (ret = get_fd_set(n, exp, fds.ex)))
        goto out;
    zero_fd_set(n, fds.res_in);
    zero_fd_set(n, fds.res_out);
    zero_fd_set(n, fds.res_ex);

  /*
   * 这里是最主要的select 的过程了
   * ret 返回的是当前有事件的fd 的个数
   */
    ret = do_select(n, &amp;amp;fds, end_time);

    if (ret &amp;lt; 0)
        goto out;
    if (!ret) {
        ret = -ERESTARTNOHAND;
        if (signal_pending(current))
            goto out;
        ret = 0;
    }

  /*
   * 这里就是讲内核空间fds.res_in, out, ex里面的内容拷贝回去到inp, outp, exp
   * 的过程
   */
    if (set_fd_set(n, inp, fds.res_in) ||
        set_fd_set(n, outp, fds.res_out) ||
        set_fd_set(n, exp, fds.res_ex))
        ret = -EFAULT;

out:
    if (bits != stack_fds)
        kfree(bits);
out_nofds:
    return ret;
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>kernel list</title>
   <link href="http://baotiao.github.io//2016/02/kernel-list/"/>
   <updated>2016-02-28T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2016/02/kernel-list</id>
   <content type="html">&lt;h3&gt;困惑点&lt;/h3&gt;

&lt;p&gt;之前看kernel list 的时候困惑的地方在于这个list里面居然没有指针指向这个list
对应的struct, 而是直接指向struct 里面的list 元素,
比如这样&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct my_cool_list{
  struct list_head list; /* kernel&#39;s list structure */
  int my_cool_data;
  void* my_cool_void;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么怎么返回这个实际包含这个list 里面的元素的struct 的结构体呢?&lt;/p&gt;

&lt;p&gt;答案: 其实最重要的一点就是有list_entry(ptr, type, member) 这个宏定义,
这个宏实现可以从一个struct 里面的一个元素, 然后返回这个struct 的地址,
这个是怎么做的呢?&lt;/p&gt;

&lt;p&gt;其实也很好实现, 就是把struct 里面的偏移量拿来加减就可以了, 比如
struct node {
  int a;
  int b;
}&lt;/p&gt;

&lt;p&gt;知道这个node.b 的地址, 那么很容易根据偏移量减去这个地址就可以了. 所以&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#define list_entry(ptr, type, member) \
        ((type *)((char *)(ptr)-(unsigned long)(&amp;amp;((type *)0)-&amp;gt;member)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里ptr 就是这个b 的地址, 然后type 就是这个node 这个结构体, member 就是这个b.&lt;/p&gt;

&lt;p&gt;那么这里我们是怎么知道b 这个元素在这个结构体里面的偏移量呢?&lt;/p&gt;

&lt;p&gt;Now the question is how can we compute the offset of an element in a structure? Suppose you have a data structure struct foo_bar and you want to find the offset of element boo in it, this is how you do it:
(unsigned long)(&amp;amp;((struct foo_bar *)0)-&gt;boo)&lt;/p&gt;

&lt;p&gt;这样就可以了
这里的做法就是用foo_bar 结构体指针指向这个0这个地址, -&gt; boo的操作其实就是增加这个偏移量, 然后获得这个元素的地址了&lt;/p&gt;

&lt;p&gt;其他的地方就是普通的list 结构, 然后封装好了比较方便的操作了
&lt;img src=&quot;http://i.imgur.com/513DxAK.jpg&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;kernel list 好在哪里呢?&lt;/h3&gt;

&lt;p&gt;我们平常自己实现的list 一般是这么实现的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
struct my_list{
  void *myitem;
  struct my_list *next;
  struct my_list *prev;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里我们想要获得下一个list元素, 一般有一个对应struct 类型的指针 *next;&lt;br/&gt;
然后这个next 指针一般指向下一个my_list;&lt;/p&gt;

&lt;p&gt;然而kernel 里面的list 是这么实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct my_cool_list{
  struct list_head list; /* kernel&#39;s list structure */
  int my_cool_data;
  void* my_cool_void;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里可以看到相比较于我们自己实现的list, kernel list 的优点有&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;直接把这个list_head 结构体放在一个struct 内部, 就可以让这个struct 实现一个list 结构, 不需要知道这个struct 的类型, 实现的非常的通用, 这里也可以在这个 把这个list 连接的不是这个my_cool_list 类型, 连接其他类型也是完全可以&lt;/li&gt;
&lt;li&gt;可以放多个list_head 结构, 这样这个结构体就可以连成多个list, 虽然原生的方法也可以, 不过这样看上去非常的简洁&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;这个是list 的具体使用方法,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;

#include &quot;list.h&quot;

struct kool_list{
  int to;
  struct list_head list;
  int from;
};

int main(int argc, char **argv){

  struct kool_list *tmp;
  struct list_head *pos, *q;
  unsigned int i;

  struct kool_list mylist;
  INIT_LIST_HEAD(&amp;amp;mylist.list);
  /* or you could have declared this with the following macro
  * LIST_HEAD(mylist); which declares and initializes the list
   */

  /* adding elements to mylist */
  for(i=5; i!=0; --i){
    tmp= (struct kool_list *)malloc(sizeof(struct kool_list));

    /* INIT_LIST_HEAD(&amp;amp;tmp-&amp;gt;list);
    *
    * this initializes a dynamically allocated list_head. we
    * you can omit this if subsequent call is add_list() or
    * anything along that line because the next, prev
    * fields get initialized in those functions.
     */
    printf(&quot;enter to and from:&quot;);
    scanf(&quot;%d %d&quot;, &amp;amp;tmp-&amp;gt;to, &amp;amp;tmp-&amp;gt;from);

    /* add the new item &#39;tmp&#39; to the list of items in mylist */
    list_add(&amp;amp;(tmp-&amp;gt;list), &amp;amp;(mylist.list));
    /* you can also use list_add_tail() which adds new items to
    * the tail end of the list
     */
  }
  printf(&quot;\n&quot;);

  /* now you have a circularly linked list of items of type struct kool_list.
  * now let us go through the items and print them out
   */

  /* list_for_each() is a macro for a for loop.
  * first parameter is used as the counter in for loop. in other words, inside the
  * loop it points to the current item&#39;s list_head.
  * second parameter is the pointer to the list. it is not manipulated by the macro.
   */
  printf(&quot;traversing the list using list_for_each()\n&quot;);
  list_for_each(pos, &amp;amp;mylist.list){

    /* at this point: pos-&amp;gt;next points to the next item&#39;s &#39;list&#39; variable and
    * pos-&amp;gt;prev points to the previous item&#39;s &#39;list&#39; variable. Here item is
    * of type struct kool_list. But we need to access the item itself not the
    * variable &#39;list&#39; in the item! macro list_entry() does just that. See &quot;How
    * does this work?&quot; below for an explanation of how this is done.
     */
    tmp= list_entry(pos, struct kool_list, list);

    /* given a pointer to struct list_head, type of data structure it is part of,
    * and it&#39;s name (struct list_head&#39;s name in the data structure) it returns a
    * pointer to the data structure in which the pointer is part of.
    * For example, in the above line list_entry() will return a pointer to the
    * struct kool_list item it is embedded in!
     */

    printf(&quot;to= %d from= %d\n&quot;, tmp-&amp;gt;to, tmp-&amp;gt;from);

  }
  printf(&quot;\n&quot;);
  /* since this is a circularly linked list. you can traverse the list in reverse order
  * as well. all you need to do is replace &#39;list_for_each&#39; with &#39;list_for_each_prev&#39;
  * everything else remain the same!
  *
  * Also you can traverse the list using list_for_each_entry() to iterate over a given
  * type of entries. For example:
   */
  printf(&quot;traversing the list using list_for_each_entry()\n&quot;);
  list_for_each_entry(tmp, &amp;amp;mylist.list, list)
    printf(&quot;to= %d from= %d\n&quot;, tmp-&amp;gt;to, tmp-&amp;gt;from);
  printf(&quot;\n&quot;);

  /* now let&#39;s be good and free the kool_list items. since we will be removing items
  * off the list using list_del() we need to use a safer version of the list_for_each()
  * macro aptly named list_for_each_safe(). Note that you MUST use this macro if the loop
  * involves deletions of items (or moving items from one list to another).
   */
  printf(&quot;deleting the list using list_for_each_safe()\n&quot;);
  list_for_each_safe(pos, q, &amp;amp;mylist.list){
    tmp= list_entry(pos, struct kool_list, list);
    printf(&quot;freeing item to= %d from= %d\n&quot;, tmp-&amp;gt;to, tmp-&amp;gt;from);
    list_del(pos);
    free(tmp);
  }

  return 0;
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>kernel file poll operations</title>
   <link href="http://baotiao.github.io//2016/02/kernel-file-poll-operations/"/>
   <updated>2016-02-28T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2016/02/kernel-file-poll-operations</id>
   <content type="html">&lt;h4&gt;在file_operations 里面, poll 这个操作到底是什么意思&lt;/h4&gt;

&lt;p&gt;在ulk 里面poll 操作的解释是&lt;/p&gt;

&lt;p&gt;Checks whether there is activity on a file and goes to sleep until something happens on it.&lt;/p&gt;

&lt;p&gt;然后我看了kernel(2.6.32) 里面的实现, 其实并没有sleep的过程&lt;/p&gt;

&lt;p&gt;看不同kernel 里面fd 的poll 实现可以发现其实poll 操作做的事情主要是两个&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;注册这个唤醒队列的回调函数, 也就是设置当这个fd 有事件到达的时候的执行函数&lt;/li&gt;
&lt;li&gt;返回当前这个fd 的事件状态, 比如这里pipe 的状态就是根据 nrbufs 里面的内容的多少来返回这个当前fd的状态, tcp的判断就更加复杂一些&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;比如这个是pipe 上面的 poll 操作 pipe_poll()&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
/* No kernel lock held - fine */
static unsigned int
pipe_poll(struct file *filp, poll_table *wait)
{
  unsigned int mask;
  struct inode *inode = filp-&amp;gt;f_path.dentry-&amp;gt;d_inode;
  struct pipe_inode_info *pipe = inode-&amp;gt;i_pipe;
  int nrbufs;

  poll_wait(filp, &amp;amp;pipe-&amp;gt;wait, wait);

  /* Reading only -- no need for acquiring the semaphore.  */
  nrbufs = pipe-&amp;gt;nrbufs;
  mask = 0;
  if (filp-&amp;gt;f_mode &amp;amp; FMODE_READ) {
    // 这里nrbufs &amp;gt; 0, 说明这个pipe里面是有内容的, 因此这个fd 有可读事件
    mask = (nrbufs &amp;gt; 0) ? POLLIN | POLLRDNORM : 0;
    if (!pipe-&amp;gt;writers &amp;amp;&amp;amp; filp-&amp;gt;f_version != pipe-&amp;gt;w_counter)
      mask |= POLLHUP;
  }

  if (filp-&amp;gt;f_mode &amp;amp; FMODE_WRITE) {
    // 只要nrbufs &amp;lt; PIPE_BUFFERS, 说明这个pipe 还没被写满, 那么这个fd 就是可写的
    mask |= (nrbufs &amp;lt; PIPE_BUFFERS) ? POLLOUT | POLLWRNORM : 0;
    /*
     * Most Unices do not set POLLERR for FIFOs but on Linux they
     * behave exactly like pipes for poll().
     */
    if (!pipe-&amp;gt;readers)
      mask |= POLLERR;
  }

  return mask;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对应的tcp 里面是否有时间到达的poll 函数是 tcp_poll()&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;unsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)
{
  unsigned int mask;
  struct sock *sk = sock-&amp;gt;sk;
  struct tcp_sock *tp = tcp_sk(sk);
  sock_poll_wait(file, sk-&amp;gt;sk_sleep, wait);
  if (sk-&amp;gt;sk_state == TCP_LISTEN)
    return inet_csk_listen_poll(sk);

  /* Socket is not locked. We are protected from async events
   * by poll logic and correct handling of state changes
   * made by other threads is impossible in any case.
   */

  mask = 0;
  if (sk-&amp;gt;sk_err)
    mask = POLLERR;

...
  if (sk-&amp;gt;sk_shutdown == SHUTDOWN_MASK || sk-&amp;gt;sk_state == TCP_CLOSE)
    mask |= POLLHUP;
  if (sk-&amp;gt;sk_shutdown &amp;amp; RCV_SHUTDOWN)
    mask |= POLLIN | POLLRDNORM | POLLRDHUP;

  /* Connected? */
  if ((1 &amp;lt;&amp;lt; sk-&amp;gt;sk_state) &amp;amp; ~(TCPF_SYN_SENT | TCPF_SYN_RECV)) {
    int target = sock_rcvlowat(sk, 0, INT_MAX);

    if (tp-&amp;gt;urg_seq == tp-&amp;gt;copied_seq &amp;amp;&amp;amp;
        !sock_flag(sk, SOCK_URGINLINE) &amp;amp;&amp;amp;
        tp-&amp;gt;urg_data)
      target--;

    /* Potential race condition. If read of tp below will
     * escape above sk-&amp;gt;sk_state, we can be illegally awaken
     * in SYN_* states. */
    if (tp-&amp;gt;rcv_nxt - tp-&amp;gt;copied_seq &amp;gt;= target)
      mask |= POLLIN | POLLRDNORM;

    if (!(sk-&amp;gt;sk_shutdown &amp;amp; SEND_SHUTDOWN)) {
      if (sk_stream_wspace(sk) &amp;gt;= sk_stream_min_wspace(sk)) {
        mask |= POLLOUT | POLLWRNORM;
      } else {  /* send SIGIO later */
        set_bit(SOCK_ASYNC_NOSPACE,
          &amp;amp;sk-&amp;gt;sk_socket-&amp;gt;flags);
        set_bit(SOCK_NOSPACE, &amp;amp;sk-&amp;gt;sk_socket-&amp;gt;flags);
    /* Potential race condition. If read of tp below will
     * escape above sk-&amp;gt;sk_state, we can be illegally awaken
     * in SYN_* states. */
    if (tp-&amp;gt;rcv_nxt - tp-&amp;gt;copied_seq &amp;gt;= target)
      mask |= POLLIN | POLLRDNORM;

    }

    if (tp-&amp;gt;urg_data &amp;amp; TCP_URG_VALID)
      mask |= POLLPRI;
  }
  return mask;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里可以看到, tcp 的tcp_poll() 里面也是同样调用socket_poll_wait, 然后socket_poll_wait 调用poll_wait来注册当有时间发生的时候的回调函数.&lt;/p&gt;

&lt;p&gt;然后这里tcp 这个是否有时间到达需要进行的判断就比pipe 要复杂的多, 比如这里需要判断socket 的是否shut_down, 需要判断tp-&gt;rcv_nxt 等等, 最后才能获得这个fd 上面的事件的内容&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Haystack object storage</title>
   <link href="http://baotiao.github.io//2015/12/facebook-haystack/"/>
   <updated>2015-12-25T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2015/12/facebook-haystack</id>
   <content type="html">&lt;h3&gt;HayStack&lt;/h3&gt;

&lt;p&gt;HayStack 是facebook 的一个针对图片存储的object storage. 以下大概是HayStack设计&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;图片大部分的应用场景是 只写一次, 然后经常读取, 以及从来不会对图片进行修改. 并且极其少的可能去修改这个图片&lt;/li&gt;
&lt;li&gt;Haystack 主要包含Haystack Directory, Haystack Cache, Haystack Store. 那么读写的流程分别是
&lt;img src=&quot;http://i.imgur.com/Xa1E5zg.png&quot; alt=&quot;Imgur&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/Hjn71u5.png&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看出这里Haystack Directory 里面应该保存的是数据的元信息&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Haystack Store 可以说自己实现了一套存取图片的引擎. 如果只是用操作系统的文件系统, 存在的问题是.

&lt;ul&gt;
&lt;li&gt;某一个目录下面存放大量的图片的文件, 那么由于文件系统里面目录也是跟保存在一个block里面, 那么就会造成这个目录的node下面的data block的内容过大, 那么为了取到这个目录的meta信息, 就需要读取多个block 才能读取得到需要的内容&lt;/li&gt;
&lt;li&gt;只是用文件系统来保存图片的话, 为了读取到一次图片, 我们需要首先读取对应的目录的inode数据, 然后是目录的data block, 然后从里面找到我们需要的文件, 然后读取这个文件的inode, 然后是读取这个文件的data block. 从这里可以看出, 我们为了找到一个文件, 需要经过多次的磁盘IO最终才能找到这个数据.&lt;/li&gt;
&lt;li&gt;如果我们存的图片不大, 那么就会找出大量的小文件的情况发生&lt;/li&gt;
&lt;li&gt;通过后续的计算可以看出,比如在xfs系统下面建立一个文件xfs inode 信息大小是536 byte, 而如果可以通过自己实现我们需要的元信息的的大小可以做到40 bytes&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 那么Haystack Store 的主要的一个事情就是建立一个大文件, 然后在这个大文件头建立SuperBlock的信息, 然后底下的数据模块具体放文件的内容. 那么在这个大文件里面找到某一个文件的信息就需要一个Index 的信息
&lt;img src=&quot;http://i.imgur.com/Hyowus5.png&quot; alt=&quot;Imgur&quot; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;这个是Index File 的信息, Index File 里面needle 的数据的顺序必须严格和数据文件的needle 一致
&lt;img src=&quot;http://i.imgur.com/OyAWZOt.png&quot; alt=&quot;Imgur&quot; /&gt;
Since index records do not reflect deleted photos, a Store machine may retrieve a photo that has in fact been deleted. To address this issue, after a Store machine reads the entire needle for a photo, that machine can then inspect the deleted flag. If a needle is marked as deleted the Store machine updates its in-memory map- ping accordingly and notifies the Cache that the object was not found.&lt;/p&gt;

&lt;p&gt;所以从这里可以看出来, 具体的文件是否被删除的信息是存放在volume file里面的, 我有一个疑惑? 为什么不直接将这个文件是否被删除的flag存放在index file 里面呢&lt;/p&gt;

&lt;p&gt;总结: HayStack Store的核心其实是自己来分配这个磁盘空间, 就是memcache 一样, 因为作为应用层, 我自己对我当前的应用的需求更加的了解, 而kernel的内存分配规则是针对通用的应用的需求. 因此memcache的做法就是自己提前申请一块大内存, 然后在这块大内存上面自己进行内存分配. 那么HayStack Store 的做法就是自己提前申请一块大的磁盘空间, 然后在这个大的磁盘空间上面进行空间分配, 因为HayStack Store 对这个磁盘的需求的理解是由于kernel的&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Haystack 的优化也主要在于会对文件进行compact, 这里的compact指的是将这个volume file里面标记成删除的文件删除掉, 还有一个是尽可能的减少一个文件需要的inode信息的大小&lt;/li&gt;
&lt;li&gt;Haystack 的思想和 log struct file system 很像, 就是顺序写入到磁盘, 然后大部分的读取是会命中cache的, 这样读的时候虽然需要查找多次才能读到数据, 由于cache的命中率高. 所以还是可以接受的. 并且Haystack 由于应用的场景是图片. 所以这种场景对于写入的数据修改的情况就更少了.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>File System Summarize</title>
   <link href="http://baotiao.github.io//2015/11/filesystem-summarize/"/>
   <updated>2015-11-23T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2015/11/filesystem-summarize</id>
   <content type="html">&lt;p&gt;最近在看关于file system 的一些东西, 总觉得这个领域发展的比较成熟, 应该可以从这个领域学到东西应用到其他地方. 看了一些论文发现没有综述性质的文章, 只能自己进行分类了, 后续继续更新...&lt;/p&gt;

&lt;h3&gt;journal file system&lt;/h3&gt;

&lt;p&gt;xfs, ffs(unix fast file system), ext3, ext4&lt;/p&gt;

&lt;p&gt;为了防止磁盘文件的丢失, 磁盘不可能每次都刷新. 所以有两种思路做recovery&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;synchronous meta-data update + fsck&lt;/li&gt;
&lt;li&gt;logging (xv6 and linux ext3)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;journal 不能保证所有的数据都不丢失, 但是journal 可以让让file system 保证一个一致的, 可用的状态, 不会出现某一个垃圾的block&lt;/p&gt;

&lt;p&gt;journal file system 一般会把journal 日志放在不同于data的盘上, 这样可以用journal 日志来恢复data目录上的数据, 而log-structure file system 因为Log 也是它的数据, 所以所有的数据放在同一个目录下面&lt;/p&gt;

&lt;p&gt;一般用 journal 来记录的只有metadata 的信息, 不会记录具体的数据信息, 因为如果记录具体的数据信息的话. 一次正常的写入就相当写入了两次, 就会有明显的写放大的问题&lt;/p&gt;

&lt;p&gt;ffs
使用cylinder group 的目的是因为, 使用cylinder group 来存数据的话, 那么磁头是不需要旋转的.
磁盘移动的比较慢的原因:
1. 横向移动, 就是从一个track 移动到 另一个 track
2. rotational movement, 就是在sector 之间移动, 这个也是比较慢, 不过比横向移动还是要来得快&lt;/p&gt;

&lt;p&gt;cylinder group: All of the data that can be read on the disk without moving the head. Comes from multiple patters&lt;/p&gt;

&lt;p&gt;ffs 是第一个考虑到了具体的数据文件的位置和磁盘的具体位置的关系
ffs superblock contained datailed disk geometry information allowing FFS to attempt to perform better block placement&lt;/p&gt;

&lt;p&gt;ffs 是把所有的inode map的信息放在disk 上面的一个固定的位置, 而lfs 则是把这个imap信息一直更新, 放在Log的最末尾
&lt;img src=&quot;http://i.imgur.com/KbfeHJD.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在磁盘结构里面的 inode, direct blocks, double-indirect blocks, single-indirect block 的结构存在的目的都是为了允许文件系统里面存在任意多的动态的block. 如果都只是存在direct blocks 的话, 那么需要提前分配好所有的inode 和 block 的空间, 然后inode 指向对应的block, 这样就是old filesystem 的做法, 这样的做法首先浪费大量的空间, 第二整个磁盘的大小就受到了限制&lt;/p&gt;

&lt;p&gt;我们可以具体算一下这个single-level indirect table, double-level indirect table 所能索引的磁盘的大小, 以及ffs具体的磁盘的大小
&lt;img src=&quot;http://i.imgur.com/i0ZllJp.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;log-structured file system&lt;/h3&gt;

&lt;p&gt;NILFS(New implementation of log-structure file system)
F2FS&lt;/p&gt;

&lt;p&gt;在lfs 的观点看来, ffs 最大的一个问题就是每一次创建一个文件, ffs 需要存在5次的写入操作, 又因为ffs 的文件的meta信息, 文件的data信息是在不同的block上, 因此那么大量的时间就花费在seek到相应的Block的时间上了&lt;/p&gt;

&lt;p&gt;而lfs的做法就是将这5次操作缓存在一个cache里面, 那么等写入的时候一次顺序的写入到磁盘上, 因为lfs 不会将meta信息和data信息分开, 所以只需要一次的磁盘io操作就可以&lt;/p&gt;

&lt;p&gt;log-structured 的最基本的思路就是一直将数据顺序写入到磁盘中, 那么对应的就是一个文件有一个inode信息, 然后一个inode负责多个block, 那么在进行过一次更新以后, 会生成一个新的inode. 那么如果知道哪些inode 是新的, 哪些inode是旧的呢, 就会有imap, 用来记录哪些inode 是新的, 哪些inode是旧的, 并且每次更新以后都会去更新这个imap的信息. 这里会将imap的信息切成多份, 然后有一个checkpoint 会记录这个所有的imap存放的地方, 并且操作系统经常会将这个imap的地址放在内存中, 所以读取的时候还是很快的. 因为这个imap只是保存了inode的信息, 所以是足够小的&lt;/p&gt;

&lt;p&gt;In Unix FFS each inode is at a fixed location on disk; given the identifying number for a file, a simple calculation yields the disk address of the file’s inode. In contrast, Sprite LFS doesn’t place inodes at fixed positions; they are written to the log. Sprite LFS uses a data structure called an inode map to maintain the current location of each inode.
这个解释了为什么FFS 不需要一个inode map, 而 lfs 需要一个inode map的原因&lt;/p&gt;

&lt;p&gt;lfs 做cleaning 的时候, 是把所有的disk 分成多个segment, 然后是以segment 为维度做cleaning. 为什么这么做呢?
1. 做cleaning 的时候, 一般是拿一个有脏数据的磁盘 和 一个空闲的磁盘直接做cleaning, 这样比较方便, 不用在只有一个内存里面做
2. 如果以一整个disk 为维度来做, 太大&lt;/p&gt;

&lt;p&gt;在每一个segment 里面会有一个 segment summary 记录这个segment 里面的信息&lt;/p&gt;

&lt;p&gt;所以 lfs 最大的devil 就是clean, 什么时候做clean
因为cleaning 的存在, 所以我们很难对lfs 系统进行benchmark&lt;/p&gt;

&lt;p&gt;lfs 的checkpoint 就是用来解决ffs在crash 以后如何恢复数据的问题, 因为ffs 在crash 以后只能全部重新扫描磁盘才能恢复&lt;/p&gt;

&lt;p&gt;Sprite LFS uses a two-phase process to create a checkpoint. First, it writes out all modified information to the log, including file data blocks, indirect blocks, inodes, and blocks of the inode map and segment usage table. Second, it writes a checkpoint region to a special fixed position on disk. The checkpoint region contains the addresses of all the blocks in the inode map and segment usage table, plus the current time and a pointer to the last segment written.&lt;/p&gt;

&lt;p&gt;但是只是从checkpoint 恢复解决不了在checkpoint 之后写入数据的问题, 而且lfs 默认的checkpoint 的时间是30s. 所以为了尽可能多的恢复数据, 引入了Roll-forward 机制&lt;/p&gt;

&lt;p&gt;Roll-forward 机制就是为了恢复尽可能多的在checkpoint 以后的数据&lt;/p&gt;

&lt;p&gt;ffs 的数据的回复基本是通过fsck来做的. fsck 做的方法就是将整个磁盘需要扫描一遍&lt;/p&gt;

&lt;h3&gt;copy-on-write file system&lt;/h3&gt;

&lt;p&gt;zfs, btrfs&lt;/p&gt;

&lt;h3&gt;other:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;fsck(file system consistency check) 是一个用来检测文件系统是否正常的一个工具&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;debugfs 一个用来看文件系统是否有问题的工具
sudo debugfs -R &quot;show_super_stats&quot; /dev/sdd
可以查看某一个磁盘的具体信息&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;目前现在还在争论之中的事情是, 有os 来控制这个disk的具体的写入, 还是由disk 控制数据的写入, 原因1, os更知道哪些时刻的负载等信息, 那么可以具体的告诉disk 你把这个数据放哪. 而disk的优势在于disk对disk自己的内部的结构更加的了解&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;为什么文件系统需要存在attribute 和 data区分开来的概念, 原因也是因为 attribute 比较小, 所以可以直接将这部分的信息存放在inode里面, 而data信息是实际存放的地址. 因此会将这两个内容区分开来&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>About TCP keepalive</title>
   <link href="http://baotiao.github.io//2015/09/tcp-keepalive/"/>
   <updated>2015-09-25T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2015/09/tcp-keepalive</id>
   <content type="html">&lt;p&gt;这两天正好看到了TCP keepalive的一些疑惑, 具体就查看了一下&lt;/p&gt;

&lt;p&gt;首先, 在建立tcp 连接以后, 就算你不使用这个连接, 这个连接是会一直保留着, 那么一般来说操作系统的句柄数是有限的, 所以我们有必要关闭那些虽然连着, 但是已经没有人使用的连接.&lt;/p&gt;

&lt;p&gt;一般会有两种检查&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;有应用层面做这个检查, 比如当这个连接有操作的时候, 更新最后一次操作的时间. 那么如果在规定时间内没有操作, 就将这个连接关闭掉. Bada的服务端关闭连接操作就是这么做的, Redis 的timeout 选项也是这样实现的.&lt;/li&gt;
&lt;li&gt;使用tcp keepalive 选项来检查&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;后续我们会看到, 这两个选项是有区别的&lt;/p&gt;

&lt;p&gt;tcp 的 keepalive 选项比较适合用来检测一个连接是否断开, 但是这个keepalive是会消耗流量的.&lt;/p&gt;

&lt;p&gt;其实理论上tcp 某一个端关闭连接, 另外的一端是会收到一个空包, 但是为什么还是需要这个tcp keepalive 这个选项呢? tcp keepalive 和 直接关闭tcp 连接的关系?&lt;/p&gt;

&lt;h4&gt;场景1&lt;/h4&gt;

&lt;p&gt;首先当这个进程正常退出, 或者被kill 掉的时候, 这个时候关闭的这个进程所在的socket 会由操作系统来发出一个FIN 包给对端的socket的
因此只有由于直接挂机 或者 网络断开的情况下 这个包才是无法发送出去的. 所以也就是在这种情况下 tcp keepalive 会发挥作用.&lt;/p&gt;

&lt;p&gt;这里如果A 已经挂掉了, 而A重新起来以后, B会认为这个连接还在, 因此会继续往这个连接发送信息, 那么A就会拒绝这个连接, 因为在A看来不存在过这个连接的.就回复B一个RST packet, 导致B最终去关闭这个连接&lt;/p&gt;

&lt;h4&gt;场景2&lt;/h4&gt;

&lt;p&gt;另一个场景就是经常在proxy 里面经常用. 因为Proxy 经常需要保留固定个数的连接, 因为硬件肯定有限制的, 常见的做法就是将这些连接放在一个队列里面, 那么当连接不够用的时候, 常见的做法就是将这个队列里面的最老的连接删除掉. 所以如果你的连接使用keepalive 就可以保持自己的连接在队列里面较开始的位置&lt;/p&gt;

&lt;p&gt;不过我觉得这个作用比较牵强, 如果所有连接都用keepalive,那基本等于没用了&lt;/p&gt;

&lt;h3&gt;总结&lt;/h3&gt;

&lt;p&gt;所以tcp keepalive 常见的用途
1. 检查peers 是否存活
2. Preventing disconnection due to network inactivity&lt;/p&gt;

&lt;p&gt;而第一种应用层来检查连接存活经常用来检查客户端连接的存活, 因为客户端经常会保持发送连接&lt;/p&gt;

&lt;h3&gt;后续&lt;/h3&gt;

&lt;p&gt;可以修改 tcp keepalive 的三个参数&lt;/p&gt;

&lt;p&gt;tcp_keepalive_time 开始发送keepalive包的时间, 这里的时间是距离最后一次这个连接发包的时间 这个默认参数是 7200&lt;/p&gt;

&lt;p&gt;the interval between the last data packet sent (simple ACKs are not considered data) and the first keepalive probe; after the connection is marked to need keepalive, this counter is not used any further&lt;/p&gt;

&lt;p&gt;tcp_keepalive_intvl 开始发送keepalive以后, 间隔多长时间可以发送下一个keepalive 包&lt;/p&gt;

&lt;p&gt;the interval between subsequential keepalive probes, regardless of what the connection has exchanged in the meantime&lt;/p&gt;

&lt;p&gt;tcp_keepalive_probes 总共发送keepalive 包的个数&lt;/p&gt;

&lt;p&gt;the number of unacknowledged probes to send before considering the connection dead and notifying the application layer&lt;/p&gt;

&lt;p&gt;可以通过 /proc 接口查看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  # cat /proc/sys/net/ipv4/tcp_keepalive_time
  7200

  # cat /proc/sys/net/ipv4/tcp_keepalive_intvl
  75

  # cat /proc/sys/net/ipv4/tcp_keepalive_probes
  9
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;setsockopt 对应的几个选项&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TCP_KEEPCNT: overrides tcp_keepalive_probes

TCP_KEEPIDLE: overrides tcp_keepalive_time

TCP_KEEPINTVL: overrides tcp_keepalive_intvl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写了一个程序验证了一下, 服务端添加keepalive 的支持以后, 并在建立连接以后, 用iptable把往客户端发送连接的端口关闭, 在经过TCP_KEEPINTVL的发送消息次数以后, 服务端会自动的关闭这个连接&lt;/p&gt;

&lt;p&gt;EPOLLERR 8 EPOLLHUP 16 EPOLLIN 1 EPOLLOUT 2 EPOLLPRI 4&lt;/p&gt;

&lt;p&gt;经过测试
客户端正常关闭的时候, 服务端收到的时间是
tfe-&gt;mask_ 1
也就是 EPOLLIN
也就是说正常的关闭逻辑, 只是返回一个EPOLLIN 1, 表示有数据到达, 然后这个时候read这个socket 的数据发现是空, 就知道是关闭的事件&lt;/p&gt;

&lt;p&gt;如果是因为keepalive 收到事件, 那么返回的结果是这样
tfe-&gt;mask_ 25
也就是 EPOLLIN | EPOLLERR | EPOLLHUP
也就是说epoll 返回的事件是 有读, 错误, 并且挂断
这个时候其实程序也能读取这个fd 发现是空, 就知道是关闭的事件, 不过这里我认为这个事件应该是本地操作系统通知给这个fd的.
就是说当这个keepalive 发现对端失败的时候会notifying the application layer&lt;/p&gt;

&lt;p&gt;参考资料: http://tldp.org/HOWTO/TCP-Keepalive-HOWTO/usingkeepalive.html&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>360 Nosql Bada introduce</title>
   <link href="http://baotiao.github.io//2015/07/bada-introduce/"/>
   <updated>2015-07-29T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2015/07/bada-introduce</id>
   <content type="html">&lt;pre&gt;&lt;code&gt;此文根据【QCON高可用架构群】分享内容，由群内【编辑组】志愿整理，转发请注明来自“高可用架构（ArchNotes）”微信公众号。  
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;&lt;p&gt;陈宗志：奇虎360基础架构组 高级存储研发工程师，目前负责360分布式存储系统Bada的设计和实现，同时负责360虚拟化相关技术的研究。&lt;/p&gt;&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h4&gt;本次分享主题&lt;/h4&gt;

&lt;p&gt;主要向大家介绍一下360自主研发的分布式存储系统Nosql-Bada，作为设计者我一直觉得设计过程就是在做一些折衷，所以大部分的内容是我们开发实现Bada过程中的一些经验和坑, 也有很多的权衡, 希望和大家一起分享, 有不对的地方欢迎指出。&lt;/p&gt;

&lt;p&gt;虽然项目目前还未开源, 但是我们的一些组件, 用于异步同步数据的Mario库等, 均已经开源，后续Bada也会开源。这是360官方的Github账号https://github.com/Qihoo360&lt;/p&gt;

&lt;h4&gt;主要应用场景&lt;/h4&gt;

&lt;p&gt;我们的定位是海量数据的持久化存储, 为线上的热门应用服务。不过我们目前没有接入跟钱相关的业务, 因为我们的系统毕竟是最终一致性的系统。&lt;/p&gt;

&lt;p&gt;我们倾向使用Bada的用户数据value的大小在10k以内, 那么我们的延迟能够做到1ms左右。我们为了读取性能有一定的优势, 一般要求机器都挂载SSD盘。如果用于存储冷数据, 我们会建议用户存数据到公司的其他存储产品, 比如hbase,cassandra等等。&lt;/p&gt;

&lt;p&gt;目前公司内部云盘, 移动搜索, LBS, Onebox, 导航影视, 白名单等多个业务均在使用。&lt;/p&gt;

&lt;p&gt;云盘的场景是：通过Bada查询文件所在的存储位置。这个业务数据量千亿级别, 每天的访问量近百亿&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/Ydg7yKJ.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;LBS这个业务是将所有的POI的信息存储在Bada中, 业务需要在5个机房进行数据同步。每天的请求量十亿级别。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/fm2DIKz.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;整体架构&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/0BdMZdG.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bada SDK 是我们提供给用户SDK, &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzAwMDU1MTE1OQ==&amp;amp;mid=208311811&amp;amp;idx=1&amp;amp;sn=1c0b16d2e284ad0d5d65e5311e81010d#rd&quot; title=&quot;360 QConf 配置管理服务&quot;&gt;360 QConf 配置管理服务&lt;/a&gt; 大家之前也了解过, 我们是QConf的重度用户。用户通过SDK从QConf中获得存活的Bada节点, 然后进行访问。&lt;/p&gt;

&lt;p&gt;Data Server是我们的服务节点，其设计是学习自Amazon Dynamo(不过好像Dynamo 本身也被很多人喷), 每一个节点都是对等结构, 每一个节点存储了所有的元信息。为什么这么做?&lt;/p&gt;

&lt;p&gt;目前主流的设计一般是两种：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;BigTable 为代表的, 有MetaServer, DataServer的设计, MetaServer存储元数据信息, DataServer存储实际的数据。包括 BigTable, HBase, 百度的Mola等等。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dynamo 为代表的, 对等结构设计. 每一个节点都是一样的结构, 每一个节点都保存了数据的元信息以及数据. 包括 Cassandra, Riak 等等。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Bada 的选择&lt;/h4&gt;

&lt;p&gt;其实我觉得两个结构都是合适的。为了部署, 扩展等方便，我们不希望部署的时候需要分开部署Meta节点, Data节点。计算机行业, 加一层可以解决大部分问题, 因此我们觉得对等网络的设计更有挑战性。个人观点, 在数据量更大的情况下, Meta 节点极有可能成为瓶颈。当然Dynamo的结构肯定也有自身的缺点, 比如如何保证元数据的一致性等问题。&lt;/p&gt;

&lt;h4&gt;Data Server主要模块&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Network Proxy： 用于接收客户端的请求, 我们的协议是定制的protobuf 协议, Network Proxy模块负责解析协议, 然后请求转发到对应的节点&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Meta Info： 用于存储公共的元信息, 元信息包括每一个分片存储在哪个节点&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;DB Engine： 我们底下的引擎是基于LevelDB的定制化开发, 包括支持cas, 过期时间, 多数据结构等等&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;数据分布策略&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/4Ajza4u.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到我们目前使用的是有主从的副本策略, 图中的Primary 是主节点, Secondary 是从节点。为什么这么做?&lt;/p&gt;

&lt;p&gt;首先为什么不使用ec编码（erasure code 纠删码）, 因为ec编码主要用于保存偏冷数据, ec编码遇到的问题是如果某一个副本挂掉以后, 想要恢复副本的过程必须与其他多个节点进行通信来恢复数据, 会照成大量的网络开销. 因此这里3副本更合适。&lt;/p&gt;

&lt;p&gt;常见的分布式系统的多副本策略主要分成两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;以Cassandra, Dynamo 为主的, 没有主从结构的设计, 读写的时候满足quorum W + R &gt; N, 因此写入的时候写入2个副本成功才能返回。读的时候需要读副本然后返回最新的。这里的最新可以是时间戳或者逻辑时间。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;以MongoDB, Bada为主的, 有主从结构的设计, 那么读写的时候, 客户端访问的都是主副本, 通过binlog/oplog 来将数据同步给从副本。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;两种设计都只能满足最终一致性。那么我们再从CAP理论上看, 那么都是在哪些维度做了权衡？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;从性能上来看,有主从的设计很明显性能会由于无主从的, 因为有主从的设计只需要访问一个副本就可以返回, 而无主从的至少两个副本返回才可以。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;从一致性来看，有主从的设计如果挂掉一个节点, 如果这个节点是主, 那么就会造成由于数据同步的不及时, 这段时间写入的数据丢。如果挂掉的是从节点, 那么则对数据没有任何的影响。只要这个节点在接下来的时间内能够起来即可。无主从的设计如果挂掉一个节点, 理论上对结果是无影响的, 因为返回的时候会比较最新的结果。有主从的结构由于写入都在一个节点, 因此不存在冲突。而无主从的结构由于写入的是任意的两个副本, 会存在对同一个key的修改在不同的副本, 导致客户端读取的时候是两个不一致的版本, 这个时候就需要去解决冲突, 常见的方案就涉及到vector clock, 时间戳等等。不过, 总体来看无主从的设计一致性应该优于有主从的设计。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;从分区容错来看, 两边都必须有一半以上的节点存活才能够对外提供服务, 因为有主从的设计中必须获得超过一半节点的投票才能成为主节点。而无主从的结构, 常见在W = 2, R = 2的情况下, 必须2个副本以上才能对外提供服务。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;从可靠性来看,有主从的设计因为只访问一个副本, 性能优于无主从的设计。而且无主从的设计中, 因为对单条数据必须有两次读取, 因此对系统的访问压力也会比无主从的来的多。当然有主从的设计容易造成主落在同一个机器上, 造成负载不均的情况, 但是这里只要将主平均到所有的机器, 就可以解决这个问题。但是有主从的设计在切换主从的时候, 必然有一段时间无法对外提供服务, 而无主从的设计则不存在这样的问题。总体来说, 笔者认为从可靠性的角度来说, 有主从的设计应该比无主从来的可靠。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;我们使用的是有主从结构的设计, 原因:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bada主要的应用场景对性能的要求比较高, 大部分的请求需要在1ms左右的时间返回, 因此有主从的设计, 性能更满足需求&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;线上服务的可靠性是我们另外一个考虑的因素&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;具体的分析过程可以看 http://baotiao.github.io/2015/03/Bada-design-replicaset/&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;数据分片策略,我们叫两次映射.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;key -&gt; PartitionId(hash)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;PartitionId -&gt; Node(MetaData)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;比如上面这张图中我们可以看出, 我们将所有数据分成10个Partition, 然后每一个机器存有主节点和从节点. 我们会尽可能的保证每一个机器上面的主节点是一样多的, 这样能够做到每一个节点的负载都是均衡的。&lt;/p&gt;

&lt;h4&gt;请求流程&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;当请求的数据Primary正好是当前这个节点&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/mDQypTy.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当请求的数据Primary 不是当前节点&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/5shtLxX.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;多机房架构&lt;/h4&gt;

&lt;p&gt;360的机房是比较多的, 而且某些机房之间的网络较差。业务部署一个服务的时候, 后端的DB也需要部署在多个机房上, 因此这个常常是业务的痛点。因此我们设计之初就考虑多机房的架构。&lt;/p&gt;

&lt;p&gt;我们的多机房架构能保证&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;用户不用管理多个机房, 任意一个机房数据写入, 其他机房能够读取&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在机房存在问题的时候, 我们可以立刻切换机房的流量&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;提供每一个机房之间数据的统计和Check&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;整体实现&lt;/h4&gt;

&lt;p&gt;这个是目前LBS业务的场景&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/PgS9Uv5.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看出我们这里有一个专门的队列用于同步机房之间的数据。这个QBus 是我们团队内部基于kafka开发的消息队列服务。&lt;/p&gt;

&lt;p&gt;目前主流的机房同步方法也是两种：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;节点负责机房数据的同步, 比如Cassandra, CouchBase, Riak&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;由外部的队列来同步机房之间的数据, 比如 Yahoo pnuts&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h5&gt;Cassandra 做法&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/hbI7ctK.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在写入的时候, 每一个机房的协调者。比如这个图里面10这个节点。会把写入发送给其它机房的某一个节点, 这个时候Client这边收到的只是根据配置的一致性级别就可以返回, 比如这里配置的只要1个返回即可, 那么Client写入成功10这个节点以后,即可返回。至于与其他机房同步是10这个节点的事情, 这样子客户端的写入就可以在本地写入, 不用管多机房的latency。&lt;/p&gt;

&lt;p&gt;这里我们可以看到是Eventual Consistency. 那么Cassandra是如何做到冲突修复的呢. 这里Cassandra 读的时候有一个Read Repair 机制, 就是读取的时候读取本地多个副本. 如果副本不一致, 那么就选时间戳最新的重新写入. 让数据重新同步, 这里Cassandra只是说修复本地多副本数据不一致的方法, 同样的方法我们也可以用在多个IDC里面, 可以同时跑多个任务check不同机房的数据, 然后修复他们.&lt;/p&gt;

&lt;h5&gt;CouchBase 做法&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/5bWaPuz.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Continuous Replication. 提供配置的不同Server之间同步的Stream的个数, 也就是不同的机房之间连接的数目是可配置的. 解决冲突办法. CouchBase提供的是最终一致性的方法. 不同的版本之间首先根据修改的次数, 然后是修改时间等信息.&lt;/p&gt;

&lt;p&gt;我们最后考虑的是使用团队内部的QBus作为我们通信的队列, 主要考虑&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;省去了自己实现队列的麻烦&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;稳定运行于线上, 有专门的同事维护. 减少的很多问题&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Bada 目前线上3种多机房的使用场景&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;单机房写入, 任意机房读取&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;跨机房写入, 任意机房读取&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;任意机房写入, 任意机房读取&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;我们的实现方案也是通过QConf来实现。客户端访问的时候, 从QConf中读取目前需要访问的机房, 默认是访问本机房, 如果需要跨机房访问, 将QConf中的配置制定成需要访问的机房就可以了。&lt;/p&gt;

&lt;h3&gt;多机房写入的冲突解决方案&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;时间戳最新&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt; 任意机房写入数据, 根据时间戳来进行冲突解决。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Yahoo Pnuts Primary Key&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这里我们对每一个Key 有一个Primary IDC, 也就是这个Key的修改删除等操作都只会在当前这个IDC完成, 然后读取可以有多个IDC去读取. 那么因为对于同一个Key的修改, 我们都在同一个IDC上. 我们通过给每一个Key加上一个Version信息, 类似Memcached的cas操作, 那么我们就可以保证做到支持单条数据的事务。如果这条数据的Primary IDC是在本机房, 那么插入操作很快。&lt;/p&gt;

&lt;p&gt;如果这条数据的Primary IDC不是本机房, 那么就有一个Cross IDC的修改操作, 延迟将会比较高。不过我们考虑一下我们大部分的应用场景,比如微博, 90%的数据的修改应该会在同一个机房。比如一个用户有一个profile信息, 那么和修改这个信息的基本都是这个用户本人, 90%的情况下应该就是在同一个地点改, 当然写入也会在同一个机房. 所以大部分的修改应该是同一个机房的修改。但是访问可能来自各个地方，当然为了做优化, 有些数据可能在一个地方修改过了以后, 多次在其他地方修改, 那么我们就可以修改这个Key的Primary IDC到另外这个机房。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vector Lock&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Vector Lock的核心思想就是Client对这个数据的了解是远远超过服务端的, 因为对于服务端而言, 这个Key 对应的Value 对于Server 端只是一个字符串。而Client端能够具体了解这个Value所代表的含义, 对这个Value进行解析。那么对于这个例子，当这两个不一样的Value写入到两个副本中的时候, Client进行一次读取操作读取了多个副本。&lt;/p&gt;

&lt;p&gt;Client发现读到的两个副本的结果是有冲突的, 这里我们假设原始的Key的Vector Lock信息是[X:1], 那么第一次修改就是[X:1,Y:1], 另一个客户端是基于[X:1]的Vector Lock修改的, 所以它的Vector Lock信息就应该是[X:1,Z:1]。这个时候我们只要检查这个Vector Lock信息就可以可以发现他们冲突, 这个就是就交给客户端去处理这个冲突.并把结果重新Update即可。&lt;/p&gt;

&lt;p&gt;我们线上目前支持的是时间戳最新, 以及Primary Key的方案. 大部分使用的是时间戳最新来进行冲突解决。&lt;/p&gt;

&lt;h3&gt;多数据结构支持&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;我们开发了一套基于leveldb的多数据结构的引擎。目前支持 Hash, List, Set, Zset等结构。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;主要是由于用户习惯了Redis提供的多数据结构, 能够满足用于快速开发业务的过程, 因此我们也提供了多数据结构的支持。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;为什么不使用ZooKeeper&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ZooKeeper 和 Mnesia 对比, ZooKeeper 是一个服务, 而 Mnesia是一个库, 因此如果使用ZooKeeper的话, 我们需要额外的维护一套服务。而 Mnesia可以直接集成在代码里面，使用更方便。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mnesia 和 Erlang 集成的更好，Mnesia本身就是用Erlang 来开发。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Bada 和 MongoDB对比&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;360的MongoDB 之前也是我们团队在维护, 在使用MongoDB的过程中, 我们也遇到一些问题, 比如MongoDB 的扩容非常不方便, 扩容需要很长的时间, 因为MongoDB 扩容的过程是将一条一条的数据写入的. 我们开发的时候考虑到这些问题, 因此Bada 使用的是leveldb, 当需要扩容的时候, 只要将某一个分片下面的数据文件拷贝过去即可. 前提是初始化的时候分片设置的足够大, 我们现实默认的分片是1024&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;MongoDB 的数据膨胀度比较大, 因为MongoDB 毕竟是文档型数据库, 肯定会保持一些冗余信息. 我们底下使用leveldb, leveldb 本身的压缩功能基于snappy 压缩. 还是做的比较好. 线上实际的磁盘空间大小相对于MongoDB 4:1&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Bada 和 Cassandra 对比&lt;/h3&gt;

&lt;p&gt;Cassandra的定位和Bada是不一样的, 我们面向的是线上频繁访问的热数据, 因此我们偏向于存储小value数据, 热数据, 对latency 的要求会苛刻。&lt;/p&gt;

&lt;p&gt;比如在云盘的场景, 我们存储的就是文件的索引信息, 而Cassandra存储的是具体的Cassandra的数据, 也因此我们线上部署Bada的机器是挂载SSD盘的。&lt;/p&gt;

&lt;h3&gt;Bada 和 Redis 对比&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bada 的性能比Redis 低, 但是目前redis cluster 还没发展完善. 我们公司的DBA也在跟进Redis cluster之中. 所以当数据量比较大的时候, Redis可能就不适用于这么大量的数据存储。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bada 的多数据结构支持不如Redis来得完善. 因此我们也在逐步的支持Bada的多数据结构。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Redis 毕竟是内存型的服务. 因此假如用户是偏向于存储持久化数据, 可能Redis不太合适。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;一些非技术的经验&lt;/h3&gt;

&lt;p&gt;技术是为业务服务, 包括我们Bada在公司内部推广的过程中也发现, 我们很多业务很头疼的问题在于360的机房较多, 每一个小业务都需要维护在多个机房, 因此为了降低用户的开发试错成本, 我们将能标准化的事情都做了。包括我们组的定位也是专注底层技术, 加速产品团队开发效率, 尽可能降低业务对服务端集群架构的关注。&lt;/p&gt;

&lt;h3&gt;Q&amp;amp;A&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Q1:客户端访问Bada时，怎么确保数据的均衡？从qconf拿到的是一个ip列表吧?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;是的。从QConf 中获得是随机的一个节点的ip，所以对每一个节点的访问基本的均衡的。服务端这边, 因为我们是有主从结构的。但是我们的主从是分片级别的主从，这点和redis cluster 不一样。比如 Redis cluster 有Master 节点, slave节点，一般情况slave 节点不接受任何的线上访问，但是从下面的图中可以看到 Bada 每一个节点都有主, 从分片。 因为每一个节点的访é动机是什么？其他leveldb分支是否考虑过?**&lt;/p&gt;

&lt;p&gt;åksdb 和 leveldb 做过对比.在数据量小的情况下, l据量大的时候 Rocksdb 会有性能优势. 因为我们乇我们的读写都走的是 Master 节点. 只有当主节çhj.jpg)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/s3KleQC.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这个截囼以及摘除失效节点的处理?**&lt;/p&gt;

&lt;p&gt;![](http://i.imgur.c 图中可以看出, 我们会将新增的节点中, 均衡的将新的主节点迁移的新节点上。目前扩容的过程是这样 我们先把当前这个节点加入到集群。然后通过 rebalance 来进行平衡。我们一般预先分配1024 个分配。这个应该也是业内场景的做法, 之前对腾讯的CKV 也是这么做，Riak 也是这么做。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q4:迁移是直接对leveldb复制，延时会有多移是直接对 leveldb 的文件进行复制, 这个时候æ¹是我们比mongo扩容快的地方, mongo 在扩容的旸»操作, 就是将所有的主切走。那么这个时候是¹的网络有额外的开销，但是这个节点不是面向个时间段内，如果要访问原来主上的数据，怎 节点。 那么A节点上有主分片, 那么在迁移之前¹上的主让给其他节点。这里就涉及到追Binlog 的导致Binlog 一直追不齐。确实会导致无法迁移。¬身的功能，就是通过scp leveldb 对应的数据文ä不过太细了有机会下次讲。使用binlog 来同步的副本策略之中, 常见的问题比如，分布式系统ä¹开发了binlog merge 来减少这种问题带来的影响㸪迁移的时候怎么解决的？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这个没有影响。åog文件, 将里面的内容加载到内存中。&lt;/p&gt;

&lt;p&gt;**Q8 :¯做切换，然后再复制数据，最后再映射meta？责任何的分片, 因此没有任何数据在这个节点上；然后我们迁移的过程是节点上的一个个的¶且这个时候 10~20 这个分片是主, 那么依次我们®改meta信息。然后接下来是复制对应的数据文仸同在于 Bada 的主从是分片级别的主从, 不是节簽可能的均衡。&lt;/p&gt;

&lt;p&gt;**Q9:mnisa用来存储meta信息吗？¯选主的过程提供一个全局的锁, 一个是保存元äooKeeper 和 mnesia 对比, ZooKeeper 是一个服务, 而m»´护一套服务. 而mnesia可以直接集成在代码里面¨Erlang 来开发&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q10:meta信息是存储在单独的机¢然用mnesia，那你前端机器连在一个集群？规æ½忠和四正的校对，其他多位编辑组志愿者对æ，关注“高可用架构”公众号，查看更多架&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/hHvBkzN.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Summarize of mesos paper</title>
   <link href="http://baotiao.github.io//2015/06/mesos-paper/"/>
   <updated>2015-06-09T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2015/06/mesos-paper</id>
   <content type="html">&lt;p&gt;以下是看mesos paper 总结的一些东西.&lt;/p&gt;

&lt;p&gt;目前Mesos 已经支持Docker的调度, 不过从代码上面来看属于增加的额外的一种Container,
官方是系统对Docker的支持与原有的自带的MesosContainer有区别,
比如官方自带的Container的隔离和限制都是自己做的, 而DockerContainer
是在Docker启动的时候在命令行里面指定的, 也由于这样, 目前Mesos
支持的所有的限制必须是Docker本身支持才行.&lt;/p&gt;

&lt;p&gt;后续会加入和Borg 的对比, 以及我对资源调度领域的一些理解.&lt;/p&gt;

&lt;h3&gt;Mesos Paper&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Overview 资源调度过程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;slave 主动汇报资源给 master, framework 向master 申请资源&lt;/li&gt;
&lt;li&gt;master 返回资源给 framework, 这里面会描述这些资源的构成, 比如内存有多少, cpu 有多少, 以及这些机器是不是都是一个机器上面的等等&lt;/li&gt;
&lt;li&gt;然后framework&#39;s scheduler 告诉给master 具体它要的这些资源怎么分, 每个任务占多少的cpu, 内存. 总共几个任务等等&lt;/li&gt;
&lt;li&gt;master 将framework 告诉给自己的任务, 分配在对应的slave机器上,然后用framework&#39;s executor执行起来&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mesos 允许各个framework 设置filter,  比如说某些任务只在某些机器上面执行等等&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Resource Allocation&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mesos 提供两种资源分配的算法, 第一种是max-min fairness 调度算法, 另一种是严格的资源限制&lt;/li&gt;
&lt;li&gt;Mesos 提供两种删除资源的方法, 比如这个framework长时间的占用资源, 这个时候怎么办, 比如想MapReduce 这样的任务, 那么就直接杀死, 因为对MapReduce这种架构而言, 单个任务的影响是不大的. 但是对MPI这种任务就不行, 因为MPI里面任务都不独立.  所以对于想MPI这种任务, allocation module提供了guaranteed allocation 的保证. 现在Mesos的做法比较简单, 如果一个framework在它保证的资源一下, 都不杀死, 如果超过了 全部杀死&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Isolation&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;主要由isolation moduler 模块提供, 这个也是一个插件是模块, 就是LXC下面的那一套. 不过现在基本都被Docker取代了吧&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Making Resource offers scale and robust&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;因为framework 会拒绝某些资源, 比如来自某些机器上的资源不要等等, 为了让这个拒绝的更快, 以免影响资源的等待等等问题, 所以支持框架一些简单的资源判断来达到快速拒绝的目的, 比如框架可以配置只要那些机器上的资源, 或者资源必须来自某一个机架等等&lt;/li&gt;
&lt;li&gt;Mesos本身会计算自己有多少资源, 所以能够并行的提供资源, 也能够快速决定是否满足framework的需求&lt;/li&gt;
&lt;li&gt;如果一个framework 在一定的时间没有返回, 那么Mesos 会取消这个资源, 然后分配给其他framework&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fault Tolerance&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Mesos 实现了主的状态叫soft state, 也就是一个新的主能够根据slave的信息, framework scheduler信息, 完全恢复这个信息, 可以不用担心宕机, 其实mesos master 里面主要包含active slaves, active frameworks, running tasks. 所以一般mesos 的master 把所有的节点信息放在zk上, 通过zk来选主, 但是里面的信息是通过与slave, framework 来恢复的&lt;/li&gt;
&lt;li&gt;如果是其他节点的错误或者executor crash, 那么Mesos master 会返回各种错误信息给framework scheduler, 然后让framework scheduler来处理这个信息&lt;/li&gt;
&lt;li&gt;&lt;p&gt;如果是framework 挂了, 那么mesos 的建议是注册多个framework 在这个mesos上, 如果一个framework 挂了, 那么其他framework 来执行这个任务, marathon 就是这种做法, 把主信息放在zk里面&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mesos Behavior&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Placement Preferences&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每一个framework 都有自己希望运行任务在哪些节点上, 由于Two Level 调度不可能了解到其他Framework 的 Preference, 所以与中心和调度相比, 不能很好的分配&lt;/li&gt;
&lt;li&gt;虽然这样, 但是因为我们可以多次随机的分配任务给这个framework, 如果不满足. 那么重新在分配其他的资源给这个framework, 因为最后还是能满足不同framework 的 preferences. 只是可能需要多次的重试&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Homogeneous tasks 相同任务的框架&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;从对比可以看出, 支持任务弹性, 并且每一个任务运行时间都相同的框架, 任务的启动时间, 任务完成的时间, 资源的使用率都比较高. 相反, 需要一定资源才能运行, 并且需要的运行任务时间是指数级变化的框架 各项指标都比较低&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Heterogeneous tasks 各种不同任务的框架&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;因为存在有些任务运行时间长, 有些任务运行时间短, 因此会存在长的任务把短任务全部占用的情况&lt;/li&gt;
&lt;li&gt;为了减少这样的影响, Mesos 提供了一些策略, 比如当向框架提供资源的时候, 限制这些资源的使用时间, 如果超过这个使用时间, 就把上面的任务给杀掉. 当然, 肯定是部分机器上的资源是这样, 如果都是这样的话, 那么长任务就跑不完了&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Framework Incentives&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Mesos 毕竟是一个无中心化的调度系统, 肯定这个系统对某些framework 支持的比较好&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;运行时间短的任务, 因为运行时间短的任务在任务失败, 或者丢失任务的时候影响比较小&lt;/li&gt;
&lt;li&gt;支持弹性扩容的任务, 就是任务不需要全部的资源达到了才能运行起来, 因为这样的任务在资源还没有满足的情况下就能够运行起来, 不需要等待资源到位, 而且资源利用率也会比较高&lt;/li&gt;
&lt;li&gt;不接受不认识的资源, 这样就不会造成资源浪费, 因为Mesos 是会统计哪些资源被占用, 来统计是否有资源, 如果framework 占用了没有使用的资源, 肯定就兰妃 &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;满足了以上这些支持的framework, Mesos 会支持的更好, 资源的利用率肯定也会更高, 现在很多的开源框架基本满足这些需求, 比如Hadoop, Dryad. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Two Level 调度的局限性 &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;碎片化&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;由于不同framework需要的资源不一样, 这种Two level 调度, 不能把多种类型打包起来, 分配资源不够合理&lt;/li&gt;
&lt;li&gt;因为在运行各种不同类型的framework的时候, 容易把所有的资源碎片化, 导致有一些framework需要大任务运行不起来. 如果是中心化调度的话, 因为他知道所有的需要调度的请求, 因为不存在这个问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;互相依赖的framework 限制&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;有些framework 需要互相配合才能运行的好, 由于Two Level 调度各自的scheduler 都在各自里面, 因此很难很好的配合. 不过这种情况很少见&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;framework 的复杂性&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;中心化调度的话, framework 想中心申请资源就可以了. 而这种Two Level 调度需要Master offer resource 给framework, 然后framework来判断是否使用, 或者接受哪一个offer&lt;/li&gt;
&lt;li&gt;还有就是有些framework 不能预知这个任务的时间, 还有就是framework 必须处理任务失败等情况&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Libprocess source intorduce</title>
   <link href="http://baotiao.github.io//2015/05/libprocess-source/"/>
   <updated>2015-05-22T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2015/05/libprocess-source</id>
   <content type="html">&lt;p&gt;最近由于项目的需要, 在看Mesos 的代码,
把Mesos底下的进程管理库libprocess大概过了一遍&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;libprocess 主要包含&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;process and PID 跟erlang 类似, PID可以用来唯一追踪这个process, 然后每一个process其实是一个线程里面的任务&lt;/li&gt;
&lt;li&gt;local message via dispatch, delay, defer&lt;/li&gt;
&lt;li&gt;functional composition via promise/futures&lt;/li&gt;
&lt;li&gt;remote messaging via send, route and install&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;libprocess 这个库的基础是include/process/event.hpp 这个文件. 这个里面定义了两个基类&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Event&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;那么定义各种类型的Event 就是对应各种Process, 对应的Process 处理Event&lt;/li&gt;
&lt;li&gt;MessageEvent, HttpEvent, DispatchEvent 等等&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;EventVisitor&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;EventVisitor 会对Event进行访问, 延伸出来 进程就是一个Visitor&lt;/li&gt;
&lt;li&gt;EventVisitor -&gt; ProcessBase -&gt; Process -&gt; ProtobufProcess -&gt; ReqResProcess&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;libprocess 里面进程的创建是在spawn 的时候调用&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;spawn 在src/process.cpp 里面实现, spawn 调用dispatch 进行创建&lt;/li&gt;
&lt;li&gt;spawn 的时候会指定ProcessManager 里面创建, 指定的ProcessManager 会管理一组对应的进程&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;libprocess 定义了一些全局变量&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;static SocketManager* socket_manager = NULL;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;就是用来管理全局的Socket 信息的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;static ProcessManager* process_manager = NULL;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;全局唯一的Process Manager, 那么接下来所有的spawn 等等操作都在这个ProcessManager 里面&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;PID gc;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;全局唯一的负责做gc的进程&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;src/process.cpp 里面的schedule 是最主要的schedule 的函数, 负责调度所有的线程的执行&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;schedule 是在 process.cpp 里面的, schedule 的主要执行过程就是不断的从process_manager 里面取出这个process, 然后运行&lt;/li&gt;
&lt;li&gt;取出这个process 以后, 会运行process_manager-&gt;resume 函数, 这个resume 基本就是一个个任务的生命周期过程&lt;/li&gt;
&lt;li&gt;resume 里面主要就是调用 event-&gt;visit(&amp;amp;visitor); 对每一个process 里面的事件, 去让process具体执行这个事件. 这里事件分几种类型

&lt;ul&gt;
&lt;li&gt;HttpEvent&lt;/li&gt;
&lt;li&gt;MessageEvent&lt;/li&gt;
&lt;li&gt;DispatchEvent&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在visit 函数实现里面, 由ProcessBase 注册了handlers, 这个handlers 里面注册了message类型, http类型的所有的对应的处理方法&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;可以看出, 由于ProcessBase这个基类里面实现了所有对于消息的处理方法, 那么继承来的子类就不需要自己去实现如何访问这个Event的方法了&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;从这个resume 函数的设计可以看出, 假设这个工作的线程里面某一个Process 有很多任务, 那么会出现把其它的工作线程堵住的情况, 因为这里处理的时候是默认把某一个线程里面所有的任务都执行完的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ProcessManager 是负责管理所有的正在运行的线程的&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;list runq 是所有正在运行的线程的队列&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Gate 类是类似于futex 的实现&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;src/process.cpp 里面的initialize() 是整个cpp 的初始化函数, 应该一开始在某个地方就被调用, 初始化了主要的全局变量&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;那么这个initialize 是什么时候运行的呢?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过gdb可以看出这个函数运行的时间是当你定义了一个 class MyProcess : public Process, 那么在ProcessBase的构造函数里面, 默认都会去执行一下这个initialize函数的. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;那么initialize 里面是如何保证这个initialize 一次?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;这里简单的是通过一个变量, 每次运行前都执行一次来进行这个判断是否初始化过. 如果初始化过就直接返回了&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;至此这个最重要的initialize 就启动了, 也就是任意第一次建立这个进程的时候, 都会去执行这个初始化的操作&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;从initialize 里面可以看出启动的时候就会默认去启动线程, 然后线程数至少是8个, 可以猜出来, 以后运行的线程应该在这几个初始化出来的线程上面执行&lt;/li&gt;
&lt;li&gt;每个线程启动以后, 执行的函数就是schedule 函数, 那么接下来就是任务过来由这个schedule 来执行这个函数&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;include/process/future.hpp 主要定义了future这个类&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;其中有定义了一个叫 promise 这个类, 这个类其实就是一个has-a的关系, 就是这个类只包含一个future这个对象, 所以是一个has-a的关系,  然后对future 这个类进行了操作包装&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在future这个类里面, 主要的几个 onReady, onFailed, onDiscarded, onAny 这几个函数都在某一个state的时候调用, 比如 &lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;``&quot;&gt;    future
      .onReady(std::tr1::bind(&amp;amp;Future&amp;lt;T&amp;gt;::set, f, std::tr1::placeholders::_1))
      .onFailed(std::tr1::bind(&amp;amp;Future&amp;lt;T&amp;gt;::fail, f, std::tr1::placeholders::_1))
      .onDiscarded(std::tr1::bind(&amp;amp;Future&amp;lt;T&amp;gt;::discard, f));
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;就是说在Ready 这个state的时候, 这行Future&lt;T&gt;::set 这个函数. 这里因为onReady, onFailed等等状态都是返回的*this指针, 所以就是相当于注册了个各种判断的返回&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在注册这个执行方法的时候, 如果不是这个状态的话, 那么就把这个执行方法放入到这个执行队列里面去, 比如onFailedCallbacks 等到了相应状态再去执行&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;如何做到调用这个Callbacks 队列里面的方法呢?

&lt;ul&gt;
&lt;li&gt;onAnyCallbacks-&gt;front()(*this);&lt;/li&gt;
&lt;li&gt;&lt;p&gt;std::queue&lt;AnyCallback&gt;* onAnyCallbacks; 是一个队列, 而里面的元素 就是一个个的AnyCallback, 而AnyCallback 其实是一个function&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;typedef std::tr1::function&amp;lt;void(const Future&lt;T&gt;&amp;amp;)&gt; AnyCallback;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;dispatch 实现&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在dispatch 低下都会执行到internal::dispatch, 主要做的就是生成一个DispatchEvent, 然后由process_manager 进行deliver(pid, event, &lt;strong&gt;process&lt;/strong&gt;); 这样就把对应的事情发送给对应的进程去处理了&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;defer 实现&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;defers a dispatch to current process, 就是向这个process本身发送任务的一个方法, 比如 defer(self(), &amp;amp;Self::_launch, containerId)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Mesos libprocess Synchronized Implement</title>
   <link href="http://baotiao.github.io//2015/05/Mesos-Synchronized/"/>
   <updated>2015-05-14T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2015/05/Mesos-Synchronized</id>
   <content type="html">&lt;p&gt;最近由于项目的需要, 在看Mesos 的代码&lt;/p&gt;

&lt;p&gt;觉得libprocess 里面实现的Synchronized 实现的挺有意思就摘取出来了&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;#include &amp;lt;pthread.h&amp;gt;
#include &amp;lt;iostream&amp;gt;

class Synchronizable
{
public:
    Synchronizable()
        : initialized(false) {}

    explicit Synchronizable(int _type)
        : type(_type), initialized(false)
    {
        initialize();
    }

    Synchronizable(const Synchronizable &amp;amp;that)
    {
        type = that.type;
        initialize();
    }

    Synchronizable &amp;amp; operator = (const Synchronizable &amp;amp;that)
    {
        type = that.type;
        initialize();
        return *this;
    }

    void acquire()
    {
        if (!initialized) {
        }
        pthread_mutex_lock(&amp;amp;mutex);
    }

    void release()
    {
        if (!initialized) {
        }
        pthread_mutex_unlock(&amp;amp;mutex);
    }

private:
    void initialize()
    {
        if (!initialized) {
            pthread_mutexattr_t attr;
            pthread_mutexattr_init(&amp;amp;attr);
            pthread_mutexattr_settype(&amp;amp;attr, type);
            pthread_mutex_init(&amp;amp;mutex, &amp;amp;attr);
            pthread_mutexattr_destroy(&amp;amp;attr);
            initialized = true;
        } else {
        }
    }

    int type;
    bool initialized;
    pthread_mutex_t mutex;
};


class Synchronized
{
public:
    explicit Synchronized(Synchronizable *_synchronizable)
        : synchronizable(_synchronizable)
    {
        synchronizable-&amp;gt;acquire();
    }

    ~Synchronized()
    {
        synchronizable-&amp;gt;release();
    }

    operator bool () { return true; }

private:
    Synchronizable *synchronizable;
};


#define synchronized(s)                                                 \
    if (Synchronized __synchronized ## s = Synchronized(&amp;amp;__synchronizable_ ## s))

#define synchronizable(s)                       \
    Synchronizable __synchronizable_ ## s

#define synchronizer(s)                         \
    (__synchronizable_ ## s)


#define SYNCHRONIZED_INITIALIZER                \
    Synchronizable(PTHREAD_MUTEX_NORMAL)

#define SYNCHRONIZED_INITIALIZER_DEBUG          \
    Synchronizable(PTHREAD_MUTEX_ERRORCHECK)

#define SYNCHRONIZED_INITIALIZER_RECURSIVE      \
    Synchronizable(PTHREAD_MUTEX_RECURSIVE)


/*
 * 这里是libprocess 里面锁的实现, 能够保证的力度是一个函数的力度.
 * 只要在这一个函数里面声明了不一样的synchronizable(prefixes), 那么就可以保证在
 * 执行 synchronized (prefixes) 的时候是只有一个线程能够执行到的
 * 可以 gcc -E % 可以看出这里宏到底做了什么
 *
 * string generate(const string&amp;amp; prefix)
 * {
 *     static map&amp;lt;string, int&amp;gt;* prefixes = new map&amp;lt;string, int&amp;gt;();
 *     static Synchronizable __synchronizable_prefixes = Synchronizable(0);
 * 
 *     int id;
 *     这里Synchronized(&amp;amp;__synchronizable_prefixes) 构造函数里面就对这个变量 
 *     pthread_mutex_lock(&amp;amp;mutex);
 *     那么如果有多个线程执行到这里, 其实只有一个线程能够执行的.
 *     那么这里为什么要赋值呢?
 *     因为这里这个变量没有copy assign 构造函数, 那么默认的就是全拷贝.
 *     那么这里这个变量的__synchronizedprefixes 的作用域就是 if()之间的内容
 *     而Synchronized 这个类型默认的析构函数就是释放这个锁,
 *     因此可以做到进入这个if 内容以后, 就把这个锁给释放了
 *     
 *     if (Synchronized __synchronizedprefixes = Synchronized(&amp;amp;__synchronizable_prefixes)) {
 *         int&amp;amp; _id = (*prefixes)[prefix];
 *         _id += 1;
 *         id = _id;
 *     }
 *     printf(&quot;%d\n&quot;, id);
 *     return prefix + &quot;(&quot; + &quot;)&quot;;
 * }
 *
 * 之前有文章介绍leveldb 锁的实现也很好看. 这里与leveldb 对比一下主要区别在于
 * leveldb 需要在类的内部定义mutex的变量. 不能想用的时候就用
 * mesos 这里可以想保证锁的时候就可以保证, 但是因为这个声明的变量是 static,
 * 存在于全局的.data段里面, 因此生命周期是整个进程的生命周期
 */

using namespace std;
#include &amp;lt;map&amp;gt;
#include &amp;lt;string&amp;gt;
string generate(const string&amp;amp; prefix)
{
    static map&amp;lt;string, int&amp;gt;* prefixes = new map&amp;lt;string, int&amp;gt;();
    static synchronizable(prefixes) = SYNCHRONIZED_INITIALIZER;

    int id;
    synchronized (prefixes) {
        int&amp;amp; _id = (*prefixes)[prefix];
        _id += 1;
        id = _id;
    }
    printf(&quot;%d\n&quot;, id);
    return prefix + &quot;(&quot; + &quot;)&quot;;
}


int main()
{
    printf(&quot;%s\n&quot;, generate(&quot;heihei&quot;).c_str());
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Bada系列- 副本策略</title>
   <link href="http://baotiao.github.io//2015/03/Bada-design-replicaset/"/>
   <updated>2015-03-10T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2015/03/Bada-design-replicaset</id>
   <content type="html">&lt;h2&gt;Bada系列 - 副本策略&lt;/h2&gt;

&lt;p&gt;Bada 是360开发的一套Nosql系统, 具备多机房架构, 低延迟, 结构化等特点.&lt;/p&gt;

&lt;p&gt;今天我们聊聊Bada的多副本策略&lt;/p&gt;

&lt;h3&gt;我们知道常见的分布式系统的多副本策略主要分成两类&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;以cassandra, dynamo 为主的, 没有主从结构的设计, 读写的时候满足W + R &gt; N, 因此写入的时候写入2个副本成功才能返回. 读的时候需要读副本然后返回最新的. 这里的最新可以是时间戳或者逻辑时间&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;以MongoDB, Bada为主的, 有主从结构的设计, 那么读写的时候, 客户端访问的都是主副本, 通过binlog/oplog 来将数据同步给从副本&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;两种设计都只能满足最终一致性. 那么我们再从&lt;a href=&quot;http://baotiao.github.io/2014/01/cap-theorem/&quot;&gt;CAP&lt;/a&gt;理论上看, 那么都是在哪些维度做了权衡&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;从性能上来看&lt;/p&gt;

&lt;p&gt;  有主从的设计很明显性能会由于无主从的, 因为有主从的设计只需要访问一个副本就可以返回, 而无主从的至少两个副本返回才可以.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;从一致性来看&lt;/p&gt;

&lt;p&gt;  有主从的设计如果挂掉一个节点, 如果这个节点是主, 那么就会造成由于数据同步的不及时, 这段时间写入的数据丢失. 如果挂掉的是从节点, 那么则对数据没有任何的影响. 只要这个节点在接下来的时间内能够起来即可.&lt;/p&gt;

&lt;p&gt;  无主从的设计如果挂掉一个节点, 理论上对结果是无影响的, 因为返回的时候会比较最新的结果.&lt;/p&gt;

&lt;p&gt;  有主从的结构由于写入都在一个节点, 因此不存在冲突. 而无主从的结构由于写入的是任意的两个副本, 会存在对同一个key的修改在不同的副本, 导致客户端读取的时候是两个不一致的版本, 这个时候就需要去解决冲突, 常见的方案就涉及到vector clock, 时间戳等等.&lt;/p&gt;

&lt;p&gt;  不过, 总体来看无主从的设计一致性应该优于有主从的设计.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;从分区容错来看&lt;/p&gt;

&lt;p&gt;  两边都必须有一半以上的节点存活才能够对外提供服务, 因为有主从的设计中必须获得超过一半节点的投票才能成为主节点. 而无主从的结构, 常见在W = 2, R = 2的情况下, 必须2个副本以上才能对外提供服务&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;从可靠性来看&lt;/p&gt;

&lt;p&gt;  有主从的设计因为只访问一个副本, 性能优于无主从的设计. 而且无主从的设计中, 因为对单条数据必须有两次读取, 因此对系统的访问压力也会比无主从的来的多.&lt;/p&gt;

&lt;p&gt;  当然有主从的设计容易造成主落在同一个机器上, 造成负载不均的情况, 但是这里只要将主平均到所有的机器, 就可以解决这个问题.&lt;/p&gt;

&lt;p&gt;  但是有主从的设计在切换主从的时候, 必然有一段时间无法对外提供服务, 而无主从的设计则不存在这样的问题&lt;/p&gt;

&lt;p&gt;  总体来说, 笔者认为从可靠性的角度来说, 有主从的设计应该比无主从来的可靠.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;那Bada最后采取的是怎样的副本策略呢?&lt;/h3&gt;

&lt;p&gt;Bada采用的是有主从的设计, 主要考虑如下几个情况&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Bada主要的应用场景对性能的要求比较高, 大部分的请求需要在1ms左右的时间返回, 因此有主从的设计, 性能更满足需求&lt;/li&gt;
&lt;li&gt;线上服务的可靠性是我们另外一个考虑的因素&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;那么数据的一致性如何解决?&lt;/p&gt;

&lt;p&gt;有主从的设计最大的问题在于挂掉一个主副本, 就会造成数据的丢失. 因为后续从副本起来后, 之前主副本的未同步的数据就丢失了. MongoDB 就存在这样的问题. Bada在早期版本也是这样.&lt;/p&gt;

&lt;p&gt;这里笔者一直觉得设计就是在做折衷, 比如这里就是一个一致性和性能,可靠性的一个折衷. 在牺牲少量可靠性和性能的情况下, 尽可能的提高数据的一致性.&lt;/p&gt;

&lt;p&gt;因此后续我们又做了改进, 比如Binlog Merge等来提高数据的一致性, 只要机器挂了以后能够重启, 那么我们就可以将由于同步不及时的数据找回. 后续可以介绍Bada Binlog/Binlog Merge的实现&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Storage type</title>
   <link href="http://baotiao.github.io//2015/01/storage-define/"/>
   <updated>2015-01-27T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2015/01/storage-define</id>
   <content type="html">&lt;h2&gt;常见的存储类型&lt;/h2&gt;

&lt;p&gt;我的观点是, 这些存储类型的分类其实是人工的分类, 不存在一个绝对正确的分类.
分类只是为了有一个系统的认识
这里主要以aws上面提供的服务来进行分类&lt;/p&gt;

&lt;h3&gt;kv&lt;/h3&gt;

&lt;p&gt;Example: dynamo, simpleDB, DynamoDB, Bada&lt;/p&gt;

&lt;h3&gt;object storage&lt;/h3&gt;

&lt;p&gt;Example: amazon s3, amazon glacier&lt;/p&gt;

&lt;p&gt;object storage 与 kv的区别是, kv是数据库, 而object storage 面向的则是存储, 所以在amazon的观点看来, object storage 存的是冷数据, 访问以对象形式, 常见的比如云盘存音频, 图片等等信息, amazon glacier 更是可以用来存归档很久的历史信息. 而且存储更是要存储海量的数据, 因此一般都是用普通的saas盘来存储&lt;/p&gt;

&lt;p&gt;kv常见的用途就是用来提供线上的快速的存储服务, 以提供高性能的服务为目的, 所以一般配合SSD盘来使用, 比较适合存储小value的数据, 因为比较节省资源&lt;/p&gt;

&lt;p&gt;当然两者也有同样的地方, 比如多副本保证数据的可靠性, 不过这里在我看来kv比较适合用多副本, 而object storage比较适合用erasure code方法&lt;/p&gt;

&lt;p&gt;object storage 可能包含一些历史版本的数据, 因为作为storage 可能必须提供回滚等方案, 因此需要保留历史版本信息&lt;/p&gt;

&lt;h3&gt;block storage&lt;/h3&gt;

&lt;p&gt;Example: amazon EBS&lt;/p&gt;

&lt;p&gt;Block storage is a type of data storage typically used in storage-area network (SAN) environments where data is stored in volumes, also referred to as blocks.&lt;br/&gt;
Each block acts as an individual hard drive and is configured by the storage administrator. These blocks are controlled by the server-based operating system, and are generally accessed by Fibre Channel (FC), Fibre Channel over Ethernet (FCoE) or iSCSI protocols.&lt;br/&gt;
Because the volumes are treated as individual hard disks, block storage works well for storing a variety of applications such as file systems and databases. While block storage devices tend to be more complex and expensive than file storage, they also tend to be more flexible and provide better performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;block level storage devices are accessible as volumes and accessed directly by the operating system, they can perform well for a variety of use cases.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;block storage 是把所有的原生的硬盘连接在一起, 然后有一个server提供对所有硬盘的访问, 访问的协议是ECoe, iSCSI等. 然后每一个硬盘的部分或者多个硬盘可以组成一个block, 这个block就可以安装任何的文件系统, NFS, VMFS.&lt;br/&gt;
其实可以看成block storage 时间裸磁盘进行分隔, 然后一块磁盘上可以使用多种格式, 比如nfs,smb 等等.&lt;br/&gt;
对应的file storage, 就是一块磁盘上面本身就设定了一个格式, 然后再在上面进行应用.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Aerospike note</title>
   <link href="http://baotiao.github.io//2015/01/aerospike-note/"/>
   <updated>2015-01-14T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2015/01/aerospike-note</id>
   <content type="html">&lt;h2&gt;Aerospike 笔记&lt;/h2&gt;

&lt;blockquote&gt;&lt;p&gt;最近看了一下Aerospike的代码, 也做了一些测试, 记了点笔记, 有点乱, 后期再修整&lt;/p&gt;&lt;/blockquote&gt;

&lt;h3&gt;先贴一个测试的性能测试结果&lt;/h3&gt;

&lt;h4&gt;测试机器&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;CPU 24 * Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz&lt;/li&gt;
&lt;li&gt;内存 160G&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;测试结果&lt;/h4&gt;

&lt;p&gt;数据存储方式全内存, 单机模式
1000000 total time 110807419 average 0.110807419 ms&lt;/p&gt;

&lt;p&gt;数据存储方式全内存, 多机模式
10000 total time 2881574 average 0.288574ms&lt;/p&gt;

&lt;h3&gt;代码目录介绍&lt;/h3&gt;

&lt;h4&gt;主要代码在 as/src 下面&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;└─[$] ls as/src/
base  fabric  Makefile  storage
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;base 目录主要是对Aerospike 的所有的各个角色的工作线程的封装. Aerospike 启动以后会开启140多个线程. 比如

&lt;ul&gt;
&lt;li&gt;thr_rw.c 主要是处理用户读写请求的线程&lt;/li&gt;
&lt;li&gt;thr_info.c 保存的是用户的meta信息&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;fabric 这个目录主要出路网络相关的事情&lt;/li&gt;
&lt;li&gt;storage 这个目录是aerospike 的各个存储引擎&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;module 目录下面主要放的是Aerospike这个公司的一个公共的代码.&lt;/h4&gt;

&lt;p&gt;比如目前有&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;└─[$] ls modules/common/src/main/citrusleaf/
cf_alloc.c  cf_bits.c   cf_crypto.c  cf_hooks.c  cf_queue.c           cf_random.c  cf_shash.c
cf_b64.c    cf_clock.c  cf_digest.c  cf_ll.c     cf_queue_priority.c  cf_rchash.c  cf_vector.c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些包含 队列, 随机数, vector 等等东西&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;└─[$] ls modules/common/src/main/aerospike
as_aerospike.c                 as_buffer.c                  as_integer.c     as_module.c              as_rec.c         as_timer.c        internal.h
as_arraylist.c                 as_bytes.c                   as_iterator.c    as_msgpack.c             as_result.c      as_val.c
as_arraylist_hooks.c           as_hashmap.c                 as_list.c        as_msgpack_serializer.c  as_serializer.c  as_vector.c
as_arraylist_iterator.c        as_hashmap_hooks.c           as_logger.c      as_nil.c                 as_stream.c      crypt_blowfish.c
as_arraylist_iterator_hooks.c  as_hashmap_iterator.c        as_map.c         as_pair.c                as_string.c      crypt_blowfish.h
as_boolean.c                   as_hashmap_iterator_hooks.c  as_memtracker.c  as_password.c            as_stringmap.c   internal.c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个下面的东西更多了, 比如有buffer, 迭代器呀等等东西,
感觉像是把STL自己实现了一套&lt;/p&gt;

&lt;h3&gt;Server 代码&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Aerospike 有一个后台扫描线程存在, 每10秒扫描一次&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike 所有的线程之间的通信都是通过往对应的进程的队列发送信息来做到的&lt;br/&gt;
在队列的实现里面, 队列里面有mutex, 以及cond.&lt;br/&gt;
当队列是空的时候, 是阻塞在这个条件变量里面&lt;br/&gt;
然后往队列中插入数据的时候, 会触发这个条件变量, 然后阻塞在这个条件变量的线程重新工作的&lt;br/&gt;
所以这个队列还是封装的较好, 完全可以重新利用&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All of the nodes in the Aerospike system participate in a Paxos distributed consensus algorithm, which is used to ensure agreement on a minimal amount of critical shared state. The most critical part of this shared state is the list of nodes that are participating in the cluster. Consequently, every time a node arrives or departs, the consensus algorithm runs to ensure that agreement is reached.&lt;br/&gt;
所以 Aerospike 保存的是在这个集群里面, 运行的节点的信息, 如果运行的节点的信息发生了改变, 那么Aerospike 会通过走Paxos 来通知其他的节点&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike 里面所有的角色都是对等角色的存在, 通过心跳来保持联系.
Aerospike 有两种定义集群的方式&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike 的XDR这套基本有三个角色&lt;br/&gt;
Aerospike daemon process (asd)  这个是程序本身&lt;br/&gt;
XDR daemon process (xdr), which is composed of the following:&lt;br/&gt;
Logger module  (记录日志模块)&lt;br/&gt;
Shipper module (传输模块)&lt;br/&gt;
Failure Handler module (失败处理模块)&lt;br/&gt;
这里应该也是传输过去的日志 也可以加入压缩, 批量传输, 失败检查等功能, 然后写入到另一个IDC&lt;br/&gt;
然后不同IDC之间的关系也可以是多种, 比如主从, 主主, 有些主, 有些从. 等等. 但是并没有解决核心的不一致的问题&lt;br/&gt;
遇到不一致的问题的解决方案就是 想象不同的数据回去访问不同的机房, 但是不会再同一时刻. &lt;br/&gt;
或者也是primary key的解决方案&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike 确实是写直接写入到服务端, 走一遍socket. 不过有个优化是如果key很长, 那么会将key压缩成比较短的&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike 支持3中引擎&lt;br/&gt;
memory: 全内存的引擎&lt;br/&gt;
ssd: 直接写入到ssd盘的引擎&lt;br/&gt;
kv: 写入到文件的kv引擎&lt;br/&gt;
ssd_cold: 这个支持商业版的引擎&lt;br/&gt;
感觉添加一个新的引擎也不难, 实现as/src/storage.c 下面实现相应的函数就可以了&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike 就是一个c实现多态的例子, 支持多种引擎类型, 那么先实现基类storage.c, 然后每一个函数都有各自的不同的引擎的实现, 通过传进来的引擎类型来判断&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;thr_rw.c 是主要处理用户逻辑的线程&lt;br/&gt;
as_read_start 是开始读取数据的逻辑&lt;br/&gt;
as_write_start 是写入数据的逻辑&lt;br/&gt;
他们两个都会调用as_rw_start, 然后在as_rw_start里面具体的去进行数据的读写&lt;br/&gt;
然后as_rw_start 里面建立一个write_request_create(), 进行数据的读写&lt;br/&gt;
然后最后调用internal_rw_start, 然后再调用write_local 进行数据的写入&lt;br/&gt;
这个 thr_rw.c 是由主线程建立的&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;调用 thr_rw.c 的线程是 thr_tsvc.c 这个线程&lt;br/&gt;
在thr_tsvc.c这个线程里面 thr_tsvc()函数有一个循环不断去队列里面去取元素, 取到元素就进行处理&lt;br/&gt;
if (0 != cf_queue_pop(q, &amp;amp;tr, CF_QUEUE_FOREVER)) {&lt;br/&gt;
在thr_tsvc.c这个线程 as_tsvc_init() 是这个线程的建立函数,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;建立并调用thr_tsvc.c 这个线程的是主线程 as.c 来建立的, 所有的读写请求都经过这个thr_tsvc process_transaction 函数, 然后判断是读请求还是写请求, 然后交给thr_rw.c 来处理.&lt;br/&gt;
这个process_transaction 函数里面同时会判断这个数据是在本地处理, 还是要交给其他的节点去处理&lt;/p&gt;

&lt;p&gt;  其中读请求:&lt;br/&gt;
  thr_tsvc_read() 函数处理
  在处理完读请求以后, 可以看出, 如果是这个proxy的线程的请求的话, 那么会将请求转    发会给这个线程, 然后由这个线程去回复这个请求&lt;/p&gt;

&lt;p&gt;  写请求:&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike 基本的思路就是各个事情都有各自的线程去负责, 然后核心的逻辑分两块, fabric 网络,  storage 存储&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike 的heartbeat 里面有一个心跳, 方法是使用epoll监听这些事件的到来,
主要逻辑在hb.c里面的&lt;br/&gt;
&lt;code&gt;
nevents = epoll_wait(g_hb.efd, events, EPOLL_SZ,g_config.hb_interval / 3);
&lt;/code&gt;
这个函数, 然后下面有各个业务逻辑的处理&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike 的心跳的入口是 hb.c 下面的as_hb_init(), 然后里面建立一个线程执行mesh_list_service_fn这个函数&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;mesh host list 就是那个host的地址, 就是中心节点的地址&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;aerospike 里面的fabric节点之间的通信, income 和 send out 分别最多建立7个socekt, 然后超过的话就重复利用这7个socket&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;aerospike 里面的当有一个新节点到来的时候, mesh host 节点会调用fabric_accept_fn 来接受其他节点的请求, 然后在这个函数里面建立一个连接.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;基本aerospike 的实现里面都是一个线程有对应的队列, 然后去阻塞在去队列里面去数据的过程.&lt;br/&gt;
另一种就是 直接sleep的等待的过程, 这种线程就是定时跑的线程&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;aerospike 是40ms 做一次心跳&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;fabric_worker_fn 这个是fabric 的work thread 函数线程的主要工作, 在as_fabric_start的时候会把这个主要的工作线程启动,&lt;/p&gt;

&lt;p&gt;默认的fabric_worker 的线程数是64个
然后fabric 线程都阻塞在epoll 上, 实时处理fabric的请求&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;partition的个数是4096个, 默认的情况&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在启动Paxos以后, 会自动做一个partition平衡的事情&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;thr_demarshal 这个线程负责就是做的路由的事情, 建立一个epoll, 监听这个socket在thr_demarshal 这个线程里面, 在epoll_wait以后, 如果建立的fd等于当前这个sock的fd&lt;/p&gt;

&lt;p&gt;同样负责accept的这个线程建立连接以后, 用轮询的方式查看当前有哪一个线程的epoll_fd[id] 是空的, 如果是空的, 就是这个accept以后的fd给这个线程, 让这个线程的epoll_fd 去监听这个事件&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;是在thr_demarshal 这个线程里面打包好以后, 拼装成一个tranaction, 交给thr_tsvc这个线程来处理, 然后接下来就是具体的处理get, set请求了.
其中复制accept线程, 并且将accept以后评传成tranaction的线程有64个&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在thr_demarshal 里面, 第一个线程相当于负责accept的线程&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在thr_tsvc 里面的process_transaction() 会根据client的请求, 判断取的是哪一个namespace里面的内容&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在Aerospike 里面如何做分片的呢, 默认的分片数是4096个
也一样, 去获取这个partition的id, 就是一个Hash取模的操作&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;``&quot;&gt;  /* as_partition_getid
   * A brief utility function to derive the partition ID from a         digest */
  static inline as_partition_id
  as_partition_getid(cf_digest d)
  {
      return( (as_partition_id) cf_digest_gethash( &amp;amp;d,     AS_PARTITION_MASK ) );
  //     return((as_partition_id)((*(as_partition_id *)&amp;amp;d.digest[0])     &amp;amp; AS_PARTITION_MASK));
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike 的partition信息放在 datamodel.h里面  &lt;br/&gt;
关于一个namespace, 就是一个整个库的信息都是放在namespace_s里面, 包括副本个数, 分片个数, 冲突解决方案, 每一个namespace 里面也有partition的信息  &lt;br/&gt;
在datamodel.h 里面的partition 包含了主从信息, 其中partition 里面cf_node origin, target就是主从的信息, 包括waiting_for_master 等等&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在读取或者写入的流程里面, 有一个操作是 as_partition_getreplica_readall, 这个函数将负责服务某一个partition的所有机器找出来&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike里面的index 就是数据的索引, 存放在内存里面, 然后指向真正的数据的地址, 在base/index.h 这个文件里面, 一个index对应的就是一个record&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike 里面两个角色是master prole&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike vmapx 这个数据结果应该就是一个Hash结构, 然后存储的是一个records下面所有的Bin信息&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;最后写入到 namespace里面某一个record添加bin的是thr_rw.c 里面as_bin_create, 这就是写入一个新bin的最后插入到storage的操作&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;particle.c 这个文件里面存的是bin 所对应的具体的字段&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike 的某一个partition 启动的时候会有一个将主那边的数据拉过来的过程&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike 里面的事件中, 发完一个AS_PAXOS_MSG_COMMAND_HEARTBEAT_EVENT 事件以后, 下面紧接着是AS_PAXOS_MSG_COMMAND_RETRANSMIT_CHECK, 这个事件是对HEARTBEAT_EVENT的检查,
在RETRANSMIT_CHECK这个事件中, 主要是对集群中各个主的元信息的检查, 应该是HEARTBEAT_EVENT中带着集群的主的信息&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aerospike 里面的fabric模块中, 也是分成两种线程, 一种是server, 一种是worker线程
server线程负责accept连接, 然后将这个accept以后的连接放入到工作线程的fd里面去&lt;/p&gt;

&lt;p&gt;  这里 worker thread connect到这个server thread 建立的socket里面, 然后想这个server thread发送一个字节&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As 里面所有的heartbeat的信息都保持在g_hb这个信息里面, 比如有多少个节点需要传播heartbeat信息, 比如多少个节点是支持udp等等. 以及hb 的callback函数, 以及保存的mesh host的信息等等&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;hb主要用来做节点探测, 当节点探测成功以后, 就会把新加入的节点加入到对应node的fabric_node_element_hash(FNE)里面去&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在建立完这个fabric 端口accept的sockfd 以后, (这里这个connect的请求应该是从heartbeat里面来的) 这里用这个accept以后的sockfd 建立 fabric_worker_add 是将建立好的fabric_buffer 添加到某一个工作线程去  &lt;br/&gt;
目前好像是这样子, 一个node上面有多个进程, 然后有8个端口负责和其他的各个节点进行fabric消息的通知&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;hb 和 fabric之间是通过 fabric_heartbeat_event 这个函数进行交互的, hb获得的新节点或者减少的节点通过这个函数通知给其他的节点 &lt;br/&gt;
在fabric_heartbeat_event 后, 会交给在fabric里面注册的as_paxos_event模块去处理.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;as_fabric_register_msg_fn 注册着所以在定时发送fabric信息过程中要做的事情, 里面有18种类型, 包含要处理的 Transaction, proxy, info 等等信息, 每一个类型就注册一个处理函数去进行相应的处理. 主要的逻辑都在这里了&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;heartbeat 的定时触发是通过epoll_wait来做的
&lt;code&gt;
nevents = epoll_wait(g_hb.efd, events, EPOLL_SZ, g_config.hb_interval / 3);
&lt;/code&gt;
在heartbeat 启动以后, 轮询在这个epoll_wait的逻辑上&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;触发fabric建立连接的过程是
as_smd_init -&gt; as_smd_create -&gt; as_smd_thr -&gt; as_smd_process_event -&gt; as_smd_metadata_change -&gt; as_smd_proxy_to_principal -&gt; as_fabric_transact_start -&gt; as_fabric_send -&gt; fabric_connect&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在heartbeat 这块 as_hb_thr 是具体处理每一个节点发过来的消息, 是每 150 ms进行一次,  &lt;br/&gt;
然后在as_hb_thr 里面有可能会去修改 g_hb.adjacencies 也就是这个g_hb的全局的邻接表的信息,   &lt;br/&gt;
然后接下来是hb_monitor_thr 这个线程就是不断去检查邻接表有没有变化, 如果有变化的话, 那么会调用fabric.c里面的 fabric_heartbeat_event 这个函数, 这个函数里面又会调用注册的 paxos.c里面的as_paxos_event, 这个线程的时间是 150 ms 进行一次&lt;/p&gt;

&lt;p&gt;  至此一个节点的加入, 与退出都明显了..
就是不断的heartbeat 去各个节点, 维护一个邻接表, 然后另外一个线程不断检查邻接表的信息, 如果邻接表信息出现变化的话, 那么就走到了fabric 的逻辑, 然后fabric里面的逻辑回去走paxos的逻辑&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;as_partition 存的就是每一个partition对应的信息, 一个partition对应的就是一个数据分片里面所有的信息&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;迁移过程要考虑某一个partition 是否正在migrate中, 如果在迁移中, 要做相对应的处理&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在每一个partition的结构体里面, 有一个qnode字段. 这里字段指向某一个cf_node, 用来记录这个partition的负责查询的节点在哪&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在主要的as_partition_balance_new 里面, 主要的逻辑过程就是先算好每一个partition的node信息, 然后调整, 然后在迁移的过程中的节点要特殊处理一下&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;paxos 模块中, 有启动两个主要的线程, 一个是as_paxos_thr, 主要处理的是paxos的业务逻辑, 一个是as_paxos_sup_thr, 主要处理的逻辑就是不断的做 paxos_retransmit 然后往as_paxos_thr的队列里面发送数据&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;as_paxos_thr 是阻塞在不断从自己队列里面读取数据上面, 而as_paxos_sup_thr是周期性的往这个队列发送数据, 然后as_paxos_sup_thr是sleep 5秒一次&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;heart beat 进行以后, 会更新全局的g_config里面的 ha_paxos_succ_list_index, 以及hb_paxos_succ_list 这两个hv, hv_index 结构, 具体可以看partition.c 里面的解释, 然后在retransmit_check的时候, 会比较这两个list是否有变化&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;每一个节点在加入的时候都会执行 as_paxos_init(), 然后在节点加入自己的paxos信息后, 会执行as_partition_balance_new()这个操作&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在paxos模块发现需要改变节点信息以后, 那么就是通知system_metadata模块改变smd. 通知smd的函数就是&lt;br/&gt;
as_smd_paxos_state_changed_fn. 这个函数做的事情就是将一条internal + paxos的数据放到smd的队列里面, 那么不过轮询的这个队列就会发现有一个消息的到来, 然后在as_smd_thr函数里面, 将一条数据取出后, 执行as_smd_process_event() 去处理这个消息&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在 as_smd_process_event 里面处理两种类型的消息, 一种是普通的请求smd信息的API的请求, 还有一种就是Paxos发生改变的请求&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在system_metadata 模块, 将Paxos需要发送过来的消息执行以后, 会将这个结果返回给Paxos的principal节点, 在Paxos处理这些消息的过程中可以看到, 是需要等待所有节点返回以后. 才会表示这个操作成功的, 因此这里的操作可以看成是事务的&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;fabric 注册的两个模块, 一个处理到来的消息, 一个处理到来的事件&lt;/p&gt;

&lt;p&gt;as_fabric_register_msg_fn: 需要注册的消息函数在这块定义, 使用as_fabric_register_msg_fn 的地方是处理它队列里面的消息的时候
fabric_process_read_msg()
as_fabric_send()
这两块地方, 那么有消息过来的时候, 在相应的模块注册的事件就可以执行了&lt;/p&gt;

&lt;p&gt;  as_fabric_register_event_fn: 需要注册的事件处理函数在这块, 比如 paxos_event就进行了注册
  fabric_heartbeat_event 这里面就调用了注册的这些事件的函数, 相应的事件有相应的注册函数
  目前注册了这个事件的函数只有 as_paxos_event 函数&lt;/p&gt;

&lt;p&gt;  fabric_heartbeat_event 这个函数的调用是注册在 &lt;br/&gt;
  as_hb_register(fabric_heartbeat_event, fa);&lt;br/&gt;
  也就是注册在 hb的系统里面, hb某一个事件的时候会调用这个    fabric_heartbeat_event这块&lt;br/&gt;
那我们看看什么时候会调用注册在hb上面的这些事件&lt;br/&gt;
这个是在 as_hb_monitor_thr, 这个函数里面会调用到的. 这个函数是专门有一个线程在执行的
这个线程是as_hb_init() 的时候建立的, 这个线程执行一次以后sleep 150ms&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;Client 代码&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;aerospike client 主要的逻辑在citrusleaf:do_the_full_monte,
那么读取一个数据的过程是&lt;/li&gt;
&lt;li&gt;根据cluster, ns 就可以获得对应这个ns的table&lt;/li&gt;
&lt;li&gt;根据table, 要插入的key, 就可以获得对的partition id&lt;/li&gt;
&lt;li&gt;然后根据table-&gt;partitions[partition_id] 获得保存下来的master as_node, 如果读的话, 可能是prole node&lt;/li&gt;
&lt;li&gt;然后去获得这个node对应的fd, 这个node对应的fd可能有多个&lt;/li&gt;
&lt;li&gt;然后就是cf_socket_write_timeout 进行数据的写入&lt;/li&gt;
&lt;li&gt;这里如果出现问题的话就可能进行重试&lt;/li&gt;
&lt;li&gt;&lt;p&gt;写入以后还有一个读取的过程cf_socket_read_timeout, 用于读取写入数据的返回结果&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;客户端的基本的类的包含过程是aerospike-&gt;cluster-&gt;(as_nodes -&gt; as_node (as_node就是存的aerospike 服务端节点的具体信息了, 具体的Ip, partition的版本, 名字等等), as_partition_tables-&gt; as_partition_table(每一个namespace对应一个partition_table)-&gt;as_partitions(这里就记录了某一个partition的master 和 prole 的指针) -&gt; as_node(master, 以及prole) ) -&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;client 在什么时候更新这个元信息.
不断在后台执行的程序是 有一个线程执行 as_cluster_tend, 做的事情包括不断检查节点是否存活
as_lookup 做的事情就是检查某一个hostname:port 是否存活
通过不断检查node, 然后把这个node信息通过as_cluster_add_nodes_copy, 把这个node信息拷贝到cluster-&gt;nodes里面&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>Mario(a pipe line library)</title>
   <link href="http://baotiao.github.io//2014/10/introduce-mario/"/>
   <updated>2014-10-25T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2014/10/introduce-mario</id>
   <content type="html">&lt;p&gt;Introduce the mario pipe library.&lt;/p&gt;

&lt;p&gt;github address: &lt;a href=&quot;https://github.com/baotiao/Mario.git&quot;&gt;mario&lt;/a&gt;&lt;/p&gt;

&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/40708255&quot; width=&quot;476&quot;
height=&quot;400&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot;
scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;



</content>
 </entry>
 
 <entry>
   <title>CSAPP bomb lab</title>
   <link href="http://baotiao.github.io//2014/10/caspp-lab2/"/>
   <updated>2014-10-22T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2014/10/caspp-lab2</id>
   <content type="html">&lt;p&gt;做的是coursera上面这个 &lt;a href=&quot;https://class.coursera.org/hwswinterface-002/lecture&quot;&gt;The Hardware/Software Interface&lt;/a&gt; 这个课程的课后作业, 也就是CSAPP这本书上很有名的bonm实验. 感觉非常有意思, 对汇编和GDB的熟悉认真做完都能有很大的提高&lt;/p&gt;

&lt;p&gt;答案在我的github上有, 不过推荐自己做下来.&lt;br/&gt;
几个非常有用的GDB操作&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;info registers 查看所有的register信息&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;info frame 查看当前的栈信息, 非常的有用&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;display /10i $pc 这样就可以在nexti 执行以后, 显示当前10行的汇编代码&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;layout asm 线上汇编代码, 加当前指令的布局. 调试起来非常方便&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;print $eax 是显示某一个寄存器里面的内容&lt;br/&gt;
x/w $eax 是检查保存在寄存器里面的地址的内容, 也就是检查寄存器存的内容的内容&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;思路:&lt;/p&gt;

&lt;p&gt;首先objdump -d bomb &gt; bomb.s 得到汇编代码, 然后就可以gdb了&lt;/p&gt;

&lt;p&gt;bomb 1: 可以看到有一个 callq strings_not_equal, 然后进去就可以获得一个字符串, 然后出来是比较 %eax. 因此可以判断这里是要输入一个相应的字符串&lt;/p&gt;

&lt;p&gt;bomb 2: 可以看到 read_six_numbers, 主要考察的是loops操作&lt;/p&gt;

&lt;p&gt;bomb 3: 可以看到这里有一个 jmpq   *0x401b60(,%rax,8), 可以断定这是一个switch的case, 看汇编可以知道, 只有走到任意一个case, 就可以解这个bomb.
通过 x/7 0x401b60 就可以找到这个switch语句对应的跳转的表&lt;/p&gt;

&lt;p&gt;bomb 4: 这个bomb主要考察的是递归, 最后结果是一个斐波那契数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0x400f92 &amp;lt;func4+14&amp;gt;     mov    %edi,%ebx # 将%edi赋值给$ebx
0x400f94 &amp;lt;func4+16&amp;gt;     mov    $0x1,%eax
0x400f99 &amp;lt;func4+21&amp;gt;     cmp    $0x1,%edi # 判断%edi是否比1小
0x400f9c &amp;lt;func4+24&amp;gt;     jle    0x400fb2 &amp;lt;func4+46&amp;gt; # 比1小就退出
0x400f9e &amp;lt;func4+26&amp;gt;     lea    -0x1(%rbx),%edi # 其实这里 %edi = %rbx - 1 = %ebx - 1 = %edi - 1
0x400fa1 &amp;lt;func4+29&amp;gt;     callq  0x400f84 &amp;lt;func4&amp;gt;
0x400fa6 &amp;lt;func4+34&amp;gt;     mov    %eax,%ebp
0x400fa8 &amp;lt;func4+36&amp;gt;     lea    -0x2(%rbx),%edi # 这里就是 %edi = %edi - 2
0x400fab &amp;lt;func4+39&amp;gt;     callq  0x400f84 &amp;lt;func4&amp;gt;
0x400fb0 &amp;lt;func4+44&amp;gt;     add    %ebp,%eax
0x400fb2 &amp;lt;func4+46&amp;gt;     mov    0x8(%rsp),%rbx
0x400fb7 &amp;lt;func4+51&amp;gt;     mov    0x10(%rsp),%rbp
0x400fbc &amp;lt;func4+56&amp;gt;     add    $0x18,%rsp
0x400fc0 &amp;lt;func4+60&amp;gt;     retq
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bomb 5: 这里是对一个数组的不断跳转, 然后正好跳转12次, value是15. 数据的内容可以通过&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(gdb) x/16wd 0x401ba0
0x401ba0 &amp;lt;array.3014&amp;gt;:  10      2       14      7
0x401bb0 &amp;lt;array.3014+16&amp;gt;:       8       12      15      11
0x401bc0 &amp;lt;array.3014+32&amp;gt;:       0       4       1       13
0x401bd0 &amp;lt;array.3014+48&amp;gt;:       3       9       6       5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就可以获得这个数据的内容, 那么接下来就很容易了&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paxos introduction</title>
   <link href="http://baotiao.github.io//2014/09/paxos-introduction/"/>
   <updated>2014-09-09T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2014/09/paxos-introduction</id>
   <content type="html">&lt;p&gt;Introduction of Paxos protocol in my company.&lt;/p&gt;

&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/38862630&quot; width=&quot;512&quot; height=&quot;421&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&quot; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt; &lt;div style=&quot;margin-bottom:5px&quot;&gt; &lt;strong&gt; &lt;a href=&quot;https://www.slideshare.net/baotiao/paxos-38862630&quot; title=&quot;Paxos introduction&quot; target=&quot;_blank&quot;&gt;Paxos introduction&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&quot;http://www.slideshare.net/baotiao&quot; target=&quot;_blank&quot;&gt;宗志 陈&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>日志相关一些经验</title>
   <link href="http://baotiao.github.io//2014/07/log-experience/"/>
   <updated>2014-07-25T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2014/07/log-experience</id>
   <content type="html">&lt;p&gt;组内分享, 关于日志&lt;/p&gt;

&lt;iframe src=&quot;http://www.slideshare.net/baotiao/slideshelf&quot; width=&quot;615px&quot; height=&quot;470px&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:none;&quot; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt;&lt;/iframe&gt;

</content>
 </entry>
 
 <entry>
   <title>Leveldb env 对文件系统的封装</title>
   <link href="http://baotiao.github.io//2014/05/leveldb-env/"/>
   <updated>2014-05-15T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2014/05/leveldb-env</id>
   <content type="html">&lt;p&gt;LevelDB的Env主要封装了操作系统的文件接口, 后台线程的调度, 以及锁等实现
主要封装了如下三个文件类型&lt;/p&gt;

&lt;p&gt;为什么要封装不同的文件类型, 因为只有根据文件的类型不同, 进行不同的封装才可以把性能提高, 比如:随机写文件和顺序写文件, 顺序写我们就知道这个文件的修改只有在文件的末尾, 那么我们就可以Mmap文件末尾的部分空间, 然后进行文件写入, 这样既不会浪费空间, 性能也较高.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;RandomAccessFile  随机读文件 (sst文件的读取)&lt;/li&gt;
&lt;li&gt;SequentialFile 顺序读文件 (DB的日志文件, mainfest文件 这些文件的读取)&lt;/li&gt;
&lt;li&gt;WritableFile 顺序写文件 (DB的日志文件, sst文件, mainfest文件. 这些文件的写入都是这个WritableFile 封装的)&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;RandomAccessFile&lt;/h3&gt;

&lt;p&gt;RandomAccessFile 有两种实现, 一种是Mmap, 一种是pread&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Pread 的实现方式很简单, 之所以用Pread 就是为了防止多线程读写, lseek和read之间不是原子操作产生的问题&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; virtual Status Read(uint64_t offset, size_t n, Slice* result,
         char* scratch) const {
     Status s;
     ssize_t r = pread(fd_, scratch, n, static_cast&amp;lt;off_t&amp;gt;(offset));
     *result = Slice(scratch, (r &amp;lt; 0) ? 0 : r);
     if (r &amp;lt; 0) {
         // An error: return a non-ok status
         s = IOError(filename_, errno);
     }
     return s;
 }
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mmap 的实现方式是将新建的文件Mmap到虚拟内存空间, 然后在内存里面读取, 从代码里面可以看出默认的RandomAccessFile用的是Mmap的方式, 具体的原因肯定是Mmap的性能优于Pread 的方式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; virtual Status NewRandomAccessFile(const std::string&amp;amp; fname,
         RandomAccessFile** result) {
     *result = NULL;
     Status s;
     int fd = open(fname.c_str(), O_RDONLY);
     if (fd &amp;lt; 0) {
         s = IOError(fname, errno);
     } else if (mmap_limit_.Acquire()) { // 这里是判断是否还能够继续Mmap, 规定默认的Mmap文件的个数是1000个
         uint64_t size;
         s = GetFileSize(fname, &amp;amp;size);
         if (s.ok()) {
             void* base = mmap(NULL, size, PROT_READ, MAP_SHARED, fd, 0);
             if (base != MAP_FAILED) {
                 *result = new PosixMmapReadableFile(fname, base, size, &amp;amp;mmap_limit_);
             } else {
                 s = IOError(fname, errno);
             }
         }
         close(fd);
         if (!s.ok()) {
             mmap_limit_.Release();
         }
     } else {
         *result = new PosixRandomAccessFile(fname, fd);
     }
     return s;
 }
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;在Mmap的实现方式里面, 有一个MmapLimiter, 这个MmapLimiter主要用处就是防止你Mmap的文件过多, 造成虚拟内存空间被跑满, 或者是由于虚拟内存空间使用过多, 造成内核的性能问题. 所以这里最多允许Mmap 1000个文件. 其中在MmapLimiter里面需要用到Mutex, AtomicPointer主要为了保证修改这个Mmap的限制文件个数是原子操作&lt;/p&gt;

&lt;h3&gt;SequentialFile&lt;/h3&gt;

&lt;p&gt;SequentialFile 主要就是fread来实现&lt;/p&gt;

&lt;h3&gt;WritableFile&lt;/h3&gt;

&lt;p&gt;WritableFile 的主要封装是顺序写文件, PosixMmapFile就是用Mmap的封装来实现, 主要思路是因为是顺序写, 所以Mmap一个writefile大小的空间, 那么每次的写入就是Append()操作, 这个操作就直接通过memcpy来拷贝到内存中即可. 如果写入数据超过了writefile的大小, 那么就先ftruncate 将writefile的大小扩大(这里每次扩大的范围是一个增长因子, 从65535开始, 2倍的增长, 最大是1M),然后将原先Mmap的空间释放的, 把writefile新添加的地址Mmap到新的地址空间. 这样做的好处有&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;性能比write快. 具体的测试数据&lt;br/&gt;
每次写入1024字节, 总共写入1000000次&lt;br/&gt;
[----------debug--------][writable.cc:224]mmap cost time 6594890&lt;br/&gt;
[----------debug--------][writable.cc:239]write cost time 15244794&lt;br/&gt;
可以看出mmap 写1G的数据需要6.5s, 而write则需要15.2s&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;通过动态的扩展Mmap的空间的方式, 不会使用过多的虚拟内存空间.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;当然这个PosixMmapFile 也支持随时将写入的内存flush到磁盘上. 通过msync随时将结果Flush到磁盘&lt;/p&gt;

&lt;p&gt;主要的Append()函数实现方式:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    virtual Status Append(const Slice&amp;amp; data) {
        const char* src = data.data();
        size_t left = data.size();
        while (left &amp;gt; 0) {
            assert(base_ &amp;lt;= dst_);
            assert(dst_ &amp;lt;= limit_);
            size_t avail = limit_ - dst_;
            if (avail == 0) { // 这里判断当前Mmap的空间是否还有可用的空间, 也就是当前writefile是否被写满了
                if (!UnmapCurrentRegion() || // 被写满以后先Unmap将当前的Mmap去掉
                        !MapNewRegion()) { // 重新Mmap新的地址空间, 这里Mmap的初始地址从file_offset_开始, 也就是从
                                          // writefile文件新增的地址开始Mmap
                    return IOError(filename_, errno);
                }
            }

            size_t n = (left &amp;lt;= avail) ? left : avail;
            memcpy(dst_, src, n);
            dst_ += n;
            src += n;
            left -= n;
        }
        return Status::OK();
    }
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>一致性, 两阶段提交和事务提交的发展史(译)</title>
   <link href="http://baotiao.github.io//2014/04/consensus-history/"/>
   <updated>2014-04-17T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2014/04/consensus-history</id>
   <content type="html">&lt;p&gt;原文地址: http://betathoughts.blogspot.com/2007/06/brief-history-of-consensus-2pc-and.html&lt;br/&gt;
[注: 初次翻译, 这里面提到的论文可能理解不够, 有错误的地方感谢帮忙指出]&lt;/p&gt;

&lt;p&gt;这是关于一致性, 事务和两阶段提交的历史, 因为用词的变化导致阅读一致性相关的文章非常的麻烦(consensus 在较早以前都叫做agreement), 这些分布式相关的论文并没有按照逻辑的顺序产出, 并且整体的分布式算法是在并行发展的. 目前只有Lynch 的 Distributed Algorithm 这本书有这些主题&lt;/p&gt;

&lt;p&gt;接下来描述的Paper主要按他们的关联性排序, 并没有根据他们的出版时间进行排序.&lt;/p&gt;

&lt;p&gt;第一个我知道的关于一致性问题的实例是Lamport&#39;s &lt;a href=&quot;http://research.microsoft.com/users/lamport/pubs/time-clocks.pdf&quot;&gt;&quot;Time, Clocks and the Ordering of Events in a Distributed System&quot; (1978)&lt;/a&gt;, 虽然这篇文章没有严格的定义了一个一致性的问题. 在这个文章里面 Lamport 讨论了在有限的时间内消息是怎么在进程之间进行传输, 并且将进程之间传递消息和爱因斯坦的相对论进行的对比. 最近在博客上讨论分布式系统和爱因斯坦的相对论的关联非常的热门,但是在1978年的时候 Lamport就已经给出了一个完整的时空图的分析. 这个结论是在一个分布式系统里面, 只有A事件能够触发B事件的发生, 你才能够说明A事件发生在B事件前面, 否则是不行的. 不同的进程看到的事件的顺序是不一致的, 也就是在一个分布式系统里面, 只存在一个偏序关系, 不存在一个全序关系. Lamport 定义了&quot;happens before&quot; 关系和操作, 并且给出了一个在分布式系统里面全局的定义事件顺序关系的算法, 这样所有的进程看到的事件的顺序关系就是一致的&lt;/p&gt;

&lt;p&gt;Lamport还介绍了一个分布式的状态机: 所有的状态机以一个相同的初始状态启动, 并且保证所有的状态机处理的消息是一模一样的. 那么每一个状态机就是其他状态机的副本. 那么, 关键的问题是保证每一个副本约定好下一条要处理的消息: 一个一致性问题. 这个用上面给出的全局的定义事件顺序关系的算法可以做到的. 但是这个系统容错能力是不行的, 假如有一个进程失败了, 那么其他的进程必须等待他恢复.&lt;/p&gt;

&lt;p&gt;大约在这边论文发布的相同的时间, Gray 发布了两阶段提交协议的算法 &lt;a href=&quot;http://research.microsoft.com/%7EGray/papers/DBOS.pdf&quot;&gt;&quot;Notes on Database Operating Systems&quot; (1979)&lt;/a&gt;. 两阶段提交协议存在的问题是, 如果TM(Transaction Manager)在某些时刻如果失败了, 整个Transaction 就会阻塞. Skeen又发布了&lt;a href=&quot;http://www.cs.cornell.edu/courses/cs614/2004sp/papers/Ske81.pdf&quot;&gt;&quot;NonBlocking Commit Protocols&quot; (1981)&lt;/a&gt;这篇论文. 论文指出在一个分布式的事务里面, 需要一个三阶段的提交协议来避免在两阶段提交中存在的阻塞问题. 但是问题是这个一个完美三阶段提交协议需要将近25年的时间来完成一次提交!&lt;/p&gt;

&lt;p&gt;Fischer, Lynch and Paterson 在论文 &lt;a href=&quot;http://theory.lcs.mit.edu/tds/papers/Lynch/jacm85.pdf&quot;&gt;&quot;Impossibility of distributed consensus with one faulty process&quot; (1985)&lt;/a&gt;证明, 在一个异步系统里面, 只要有一个错误的进程, 一致性就不可能达成. 这个也就是著名的&quot;FLP&quot;结论. 在这个时间&quot;一致性&quot; 代表的是一堆进程同意同一个值的问题. 一个异步系统(异步系统指的是进程运行的速度不一定, 并且进程之间传递消息所需要的时间也不一定, 有可能无限长)在最完美的网络环境(这里的环境指的是所有的消息都会被送达, 消息送达的顺序和消息被发送的顺序是一致的, 并且消息是不会被多次的发送)下, 只要有一个进程出错, 这个一致性就无法达成. 这个问题的难点在于你不能判断一个进程是停止了还是跑的非常的慢. 处理一个在异步系统里面的错误几乎是不可能的. 这篇论文是非常重要的, 因为它告诉了我们什么事情是不可能的, 就像能量守恒定律对于永动机一样. 这篇文章证明过程是所有想要解决异步系统的一致性问题的算法必须有某些属性, 然后他们通过反证法证明了这些属性是不可能存在的.&lt;/p&gt;

&lt;p&gt;在这个阶段, 人们认为分布式系统分成同步的(进程运行在已知的速度, 并且在进程之间消息的传递在规定的时间到达) 和 异步的(进程运行在任意的速度, 并且进程间消息的传递时间不能保证). 异步的系统是一个更为常见的系统. 一个算法能够解决异步系统的一致性问题, 肯定也能够解决同步系统的一致性问题, 但是反过来不行.你可以把一个同步系统看成一个异步系统的特例, 同步系统是进程运行在规定速度, 进程之间传递消息有时间上线的一个异步系统.&lt;/p&gt;

&lt;p&gt;在FLP之前, 就有&lt;a href=&quot;http://research.microsoft.com/users/lamport/pubs/byz.pdf&quot;&gt;&quot;The Byzantine Generals Problem&quot; (1982)&lt;/a&gt;这篇论文. 在这个一致性问题里面, 进程可以传递错误信息, 并且他们积极的将这个错误信息传递给其他进程. 这个问题看起来比FLP结论更难, 但是在同步系统下, 还是有方法能够达成一致的(虽然在这篇文章发布的时候, 同步系统和异步系统的定义还没那么的明确). 但是这个方法在消息的传递的总量, 以及消息传递的轮数都非常的多, 因此性能不行. 这里Byzantine 将军问题最早的想法来自于航空业: 当一个飞机上的传感器捕获一个错误信息的时候该怎么办(这里的系统可以看成是同步系统).&lt;/p&gt;

&lt;p&gt;在1986年, 关注一致性的问题和事务的问题的人们聚集起来. 在那个时候, 最好的一致性算法就是上面提出的Byzantine Generals, 但是这个算法用来处理事务时间开销又太大. Jim Graw 在会议上发布了一篇文章 &lt;a href=&quot;http://research.microsoft.com/%7EGray/papers/TandemTR88.6_ComparisonOfByzantineAgreementAndTwoPhaseCommit.pdf&quot;&gt;&quot;A Comparison of the Byzantine Agreement Problem and the Transaction Commit Problem.&quot; (1987)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;这篇论文包含了这样一段话在开头 :-)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&quot;在这个会议之前, 大量的研究认为在分布式系统里面的事务提交是一个退化版本的Byzantine 将军问题. 可能这个会议最有用的结论是这两个问题没有任何的相关性&quot;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;最终分布式事务被看成了一个新版的一致性问题, 叫 uniform consensus (&lt;a href=&quot;http://infoscience.epfl.ch/getfile.py?recid=88273&amp;amp;mode=best&quot;&gt;&quot;Uniform consensus is harder than consensus&quot; (2000)&lt;/a&gt;). 在uniform consensus 里面, 所有的进程必须同意一个值, 包括哪些出错的进程. 一个事务在当且仅当所有的RMs(资源管理者)已经准备好要提交的时候, 才能够提交. 大部分的一致性只考虑到不出错的进程达成一致. Uniform consensus考虑的是所有的进程都达成一致, 所以Uniform consensus 是更难实现的&lt;/p&gt;

&lt;p&gt;最后 Lamport 在论文&lt;a href=&quot;http://research.microsoft.com/users/lamport/pubs/lamport-paxos.pdf&quot;&gt;&quot;The Part-Time Parliament&quot; (submitted in 1990, published 1998).&lt;/a&gt;提出了Paxos一致性算法. 不幸的是这篇论文用希腊明主投票做类比的写法让人们很难看懂, 所以这篇文章一直被忽略. 直到被Butler Lampson 在他的论文&lt;a href=&quot;http://research.microsoft.com/lampson/58-Consensus/Acrobat.pdf&quot;&gt;&quot;How to Build a Highly Availability System using Consensus&quot; (1996)&lt;/a&gt; 提起. 这篇论文介绍了如何使用Paxos来建立一个容错的系统. 后来Lamport 才又发布了&lt;a href=&quot;http://research.microsoft.com/users/lamport/pubs/paxos-simple.pdf&quot;&gt;&quot;Paxos Made Simple (2001)&quot;&lt;/a&gt;, 写的让大家都看的明白.&lt;/p&gt;

&lt;p&gt;Paxos最核心的问题是, 给定一个固定数目的进程, 这些进程的大部分定义为至少和其他的这个进程的大部分有一个相同的进程. 比如: 有A, B, C三个进程. 那么可能的这些进程大部分是AB, AC, BC. 如果有一个决定被某一个大部分通过, 比如AB. 那么任意一个时刻一个大部分的集合包含A或者B. 这样这些大部分的集合其它元素就可以从A, B那边获得这个决定的值.&lt;/p&gt;

&lt;p&gt;Paxos能够容忍信息丢失, 消息的延迟, 消息的重复发送以及消息的乱序到达. 如存在一个唯一一个在足够长的时间内能够对某一个大部分集合进行两次访问的Leader, 那么这个系统就能达到一致. 任何进程, 包括Leader都可以失败或者重启. 实际上所有的进程允许在同一个时刻同时失败, 这个时候这个算法仍然是安全. 并且这个算法容忍在某一个时候有多个主存在.&lt;/p&gt;

&lt;p&gt;Paxos是一个解决异步系统一致的算法; 这里并没有严格的超时限定. 然而只有当这个异步的系统表现出在同步的状态的情况下, 这个系统才能够达到一致. 在这个系统表现出在异步的状态的时候, 这个系统无法达成一致, 但是这个时候是安全的, 不会返回错误的结果. 有一个特殊的情形下Paxos是无法达成一致的, 这也是符合&quot;FLP&quot;的结论. 但是在实现的时候很容易避免这个情形.&lt;/p&gt;

&lt;p&gt;明确的将系统分成同步和异步有点太过宽泛. Dwork, Lynch 和 Stockmeyer定义了一个偏序的同步系统 &lt;a href=&quot;http://theory.lcs.mit.edu/tds/papers/Lynch/jacm88.pdf&quot;&gt;&quot;Consensus in the presence of partial synchrony&quot; (1988)&lt;/a&gt;. 存在两个版本的偏序同步系统: 一种是进程运行在某一个固定的速度并且消息在某一个规定的时间内传达, 但是这个某一个值是多少事先是不知道的. 另一种是进程运行的速度范围以及消息送达的时间是事先可以知道的, 但是他们保持这种状态仅仅是在未来的一段时间内, 这个时间有多长, 我们不知道. 比起同步系统和异步系统, 这个偏序的同步系统更接近现实世界.网络在绝大部分的情况下应该是可预测的, 但是偶然还是会变的不可预测.&lt;/p&gt;

&lt;p&gt;Lamport 和 Gray 在论文&lt;a href=&quot;http://research.microsoft.com/research/pubs/view.aspx?tr_id=701&quot;&gt;&quot;Consensus on Transaction Commit&quot; (2005)&lt;/a&gt;里继续尝试将Paxos应用到分布式事务系统里, 他们使用Paxos替换掉了两阶段提交里面的Transtraction Manager, 并且对每一个Resource Manager使用一个Paxos实例用来判断是否同意这个Resource Manager提交这个事务.表面上, 每一个Resource Manager 使用一个Paxos看过去成本特别高, 但是事实上不是的. 在没有错误的情况下, Paxos提交在两个阶段就会完成. 它和两阶段提交有着相同的消息延迟, 虽然可能会多传送一部分消息. 在有错误的情况下, Paxos的第三个阶段是需要的, 这个和Skeen 的结论也是一致的. 给定2n + 1个TM副本, Paxos的提交可以容忍n个错误的副本. Paxos提交不需要用Paxos直接解决事务提交的问题. Paxos 没有用来解决uniform consensus, 而是用来使这个系统容灾能力更强.&lt;/p&gt;

&lt;p&gt;因为两阶段提交是阻塞的是无效的, Paxos 提交忙于解决阻塞这个问题. 所以有些人认为分布式事务不应该被使用.&lt;/p&gt;

&lt;p&gt;最近有一些关于CAP(Consistency, Availability, Partition)猜想的讨论.这个猜想断言你不可能在一个分布式系统里面同时满足这三个属性, 也就是不存在一个系统, 它是一致的, 在有错误的进程存在并且有可能出现网络分区的情况下.&lt;/p&gt;

&lt;p&gt;我们通过把Consistency 看成Consensus来检查一下这个CAP猜想, 对于一个异步系统, 根据FLP理论, 只要存在一个进程错误的情况下, 这个系统就无法达成一致. 所以对于一个异步的系统我们不能同时有一致性和可用性.&lt;/p&gt;

&lt;p&gt;现在我们再看一下一个有三个节点的Paxos系统:A, B, C. 加入有两个节点在工作, 我们就能够达到一致. 那么我们就能实现一致性和可用性. 现在假如C被单独隔离了, 而这个时候有请求访问到C, 这个时候C是无法返回的, 因为必须有超过两个节点才能够返回结果, 而这个时候C无法和A, B节点通信的. C此刻是不知道是它自己被分区隔离了还是另外的两个节点都挂掉了, 还是说这个时候的网络特别慢的问题. 另外的两个节点能够继续的工作, 因为她们两个能够通信, 就可以超过半数的返回结果了. 所以对于CAP理论而言, Paxos并没有分区容错性, 因为这个时候C是无法对用户的请求回应的. 然而我们在工程上绕过这个问题. 假如在同一个数据中心里面, 我们可以使用两个独立的网络连接不同的节点, 因为Paxos可以容忍数据的重复发送.&lt;/p&gt;

&lt;p&gt;对于一个同步的网络,假如C被分区了, 假如C在规定的周期内没有收到信息, 那么C能够知道它自己被分区了. 然后C就能够从客户端的访问中摘除掉了.&lt;/p&gt;

&lt;p&gt;Paxos, Paxos Commit 和 HTTP/REST 已经被合并起来建立一个高可用的同配置网格计算系统. 详细的信息可以从这里获得&lt;a href=&quot;http://www.cct.lsu.edu/%7Emaclaren/HARC/&quot;&gt;HARC&lt;/a&gt;, 以及更多的在这个文章里面的讨论&lt;a href=&quot;http://www.allhands.org.uk/2006/proceedings/papers/624.pdf&quot;&gt;&quot;Co-Allocation, Fault Tolerance and Grid Computing&quot; (2006).&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>两阶段提交协议</title>
   <link href="http://baotiao.github.io//2014/03/two-phase-commit/"/>
   <updated>2014-03-28T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2014/03/two-phase-commit</id>
   <content type="html">&lt;p&gt;组内的一个分享, 两阶段提交协议&lt;/p&gt;

&lt;iframe src=&quot;http://www.slideshare.net/slideshow/embed_code/32843186&quot; width=&quot;476&quot; height=&quot;400&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

</content>
 </entry>
 
 <entry>
   <title>多IDC冲突常见解决方案</title>
   <link href="http://baotiao.github.io//2014/03/conflict-solution/"/>
   <updated>2014-03-14T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2014/03/conflict-solution</id>
   <content type="html">&lt;h2&gt;多IDC冲突常见解决方案&lt;/h2&gt;

&lt;p&gt;在分布式系统里面我们会经常遇到多个Client对同一个Key进行了修改, 如果系统不想解决冲突, 那么默认的解决方案是选取时间戳最新的那个结果. 不过有时候业务经常会对一个Key进行局部修改,然后保存. 这个时候其实业务想要的是几次操作的合并.&lt;/p&gt;

&lt;p&gt;比如Key=name:baotiao|age:18 一个Client更新了这个Key, 变成Key=name:chenzonghzi|age:18. 另外一个Client同时也更新了这个Key, 变成 Key=name:baotiao|age:20 这个时候常见的按照最后的时间戳的解决方案会带来问题是只能获得其中一个的结果. 有什么比较好的解决方案么?&lt;/p&gt;

&lt;p&gt;这里介绍三个解决方案.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;h3&gt;Dynamo Vector Lock解决方案&lt;/h3&gt;

&lt;p&gt; Vector Lock的核心思想就是Client对这个数据的了解是远远超过服务端的, 因为对于服务端而言, 这个Key 对应的Value 对于Server 端只是一个字符串. 而Client 端能够具体了解这个Value 所代表的含义, 对这个Value 进行解析. 那么对于这个例子. 当这两个不一样的Value写入到两个副本中的时候, Client进行一次读取操作读取了多个副本.&lt;/p&gt;

&lt;p&gt; Client 发现读到的两个副本的结果是有冲突的, 这里我们假设原始的Key的Vector Lock信息是[X:1], 那么第一次修改就是[X:1,Y:1], 另一个客户端是基于[X:1]的Vector Lock修改的, 所以它的Vector Lock信息就应该是[X:1,Z:1]. 这个时候我们只要检查这个Vector Lock信息就可以可以发现他们冲突, 这个就是就交给客户端去处理这个冲突.并把结果重新Update即可&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;h3&gt;Cassandra 的解决方案&lt;/h3&gt;

&lt;p&gt; Cassandra 的解决方案就是讲一个Key尽可能小粒度的拆分, 所以我们看到在Cassandra里面有RowKey的概念, 通常将一个data分成多个部分. 比如这里存的Key_name=name:baotiao, Key_age=age:18. 那么两次修改分别修改了两个表, 那么最后我们查询的时候在Key_name列里面我们看到的最新时间戳的肯定是Key_name=name:chenzongzhi, Key_age=age:20. 这样我们就可以得到这个Key最新的结果&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;h3&gt;Yahoo! Pnuts 的Primary Key解决方案&lt;/h3&gt;

&lt;p&gt; 这里我们对每一个Key 有一个Primary IDC, 也就是这个Key的修改删除等操作都只会在当前这个IDC完成, 然后读取可以有多个IDC去读取. 那么因为对于同一个Key的修改, 我们都在同一个IDC上. 我们通过给每一个Key加上一个Version信息, 类似Memcached的cas操作, 那么我们就可以保证做到支持单条数据的事务.&lt;br/&gt;
 如果这条数据的Primary IDC是在本机房, 那么插入操作很快.&lt;br/&gt;
 如果这条数据的Primary IDC不是本机房, 那么就有一个Cross IDC的修改操作, 延迟将会比较高.不过我们考虑一下我们大部分的应用场景, 90%的数据的修改应该会在同一个机房. 比如一个用户有一个profile信息, 那么和修改这个信息的基本都是这个用户本人, 90%的情况下应该就是在同一个地点改, 当然写入也会在同一个机房. 所以大部分的修改应该是同一个机房的修改.&lt;br/&gt;
 当然为了做优化, 有些数据可能在一个地方修改过了以后, 多次在其他地方修改, 那么我们就可以修改这个Key的Primary IDC 到另外这个机房&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;后话&lt;/h3&gt;

&lt;p&gt;这里提到的方案只是我个人的理解, 有不对的地方,还望大家指出
目前我觉得Yahoo!这套方案比较适合用来处理业务对数据的丢失比较敏感的方案, 虽然牺牲了10%写的性能不过我感觉能够接受&lt;br/&gt;
Dynamo 的方案问题在于有时候客户端虽然可以获得这个数据, 但是客户端也不知道如何处理这个冲突, 简单的方案可以做Merge, 复杂的结构就不好处理了.&lt;br/&gt;
Cassandra 的方案在读取方面性能可能有损失, 因为毕竟将一个Key 分成了多个Key以后, 每一次的读取操作都要合并多个Key的结果&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Leveldb write </title>
   <link href="http://baotiao.github.io//2014/02/leveldb-write/"/>
   <updated>2014-02-12T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2014/02/leveldb-write</id>
   <content type="html">&lt;pre&gt;&lt;code&gt;年前分享了LevelDB的时候遗留了一个问题
就是在LevelDB Write操作的时候, 如何做到线程安全的, 以及在代码里面为什么要同时通知这么多个的线程
    while (true) {
        Writer* ready = writers_.front();
        writers_.pop_front();
        if (ready != &amp;amp;w) {
            ready-&amp;gt;status = status;
            ready-&amp;gt;done = true;
            ready-&amp;gt;cv.Signal();
        }
        if (ready == last_writer) break;
    }

重新看了一下代码应该是这个样子的
Status DBImpl::Write(const WriteOptions&amp;amp; options, WriteBatch* my_batch) {
    // 这里用到的就是标准的 condition variable 配合 mutex 使用的例子,
    // 这里在这个while 里面添加的 w != writes_.front() 同时又保证了只有一个写
    Writer w(&amp;amp;mutex_); // 这个w锁是一个条件变量, 传入的mutex_是交给条件变量里面的mu_的
    w.batch = my_batch;
    w.sync = options.sync;
    w.done = false;

    // NICE
    // 这里写的也很精妙, 之所以用MutexLock 来实现, 是因为这样只要在中途退出就会自动
    // 触发这个MutexLock的析构函数, 析构函数里面写了unLock这个锁的操作, 那么就可以不用在
    // 每个中间的return 前面都加上这个l-&amp;gt;unLock()操作
    MutexLock l(&amp;amp;mutex_); // 这里的操作是在做pthread_cond_wait之前把mutex_锁住的操作, 这样保证pthread_cond_wait的时候不会死锁
    writers_.push_back(&amp;amp;w);
    //这里用一个队列, 并且只有在队列最头部的那个writeBatch才会被写. 所以进入到下面Write的过程只会有一个线程
    while (!w.done &amp;amp;&amp;amp; &amp;amp;w != writers_.front()) {
        w.cv.Wait(); //这里是condition varaible, 这里wait 的时候会同时把mu_这个锁放开
    }
    /* 之前解释说这里w.done 是写代码写的很小心, 是错误的, 具体解释见下面 */
    if (w.done) {
        return w.status;
    }
    // 接下来处理的就是这个writers_ 里面最头的那个的信息

    // May temporarily unlock and wait.
    // 这里是检查memtable有没有空间可以写入, 如果没有就换一个buffer 和 compaction等操作
    Status status = MakeRoomForWrite(my_batch == NULL);
    uint64_t last_sequence = versions_-&amp;gt;LastSequence();
    Writer* last_writer = &amp;amp;w;
    if (status.ok() &amp;amp;&amp;amp; my_batch != NULL) {  // NULL batch is for compactions

        /*  主要的地方就是这个BuildBatchGroup 函数, 这个函数做的是将这个队列里面前几个的Writer, 合并成一个Batch.
            这么做的原因我想主要也是为了性能考虑, 因为这里我们每一次的Put, 都是一个batch, 所以这里会将多个的batch
            合并成一个Batch来进行处理, 这样能明显的提高性能.
            所以这里将队列的前几个Batch合并成了一个Batch, 由当前的Batch处理了. 所以刚才上面那个代码会判断一下当前的这个
            Write 是否已经被处理好了

            代码见下面
        */

        WriteBatch* updates = BuildBatchGroup(&amp;amp;last_writer);

        WriteBatchInternal::SetSequence(updates, last_sequence + 1);
        last_sequence += WriteBatchInternal::Count(updates);

        // Add to log and apply to memtable.  We can release the lock
        // during this phase since &amp;amp;w is currently responsible for logging
        // and protects against concurrent loggers and concurrent writes
        // into mem_.
        {
            //因为到这里的时候 只有一个writers_里面的一个能到达这里. 所以这里可以保证这有一个线程到了可以AddRecord这一步了.
            //所以这里把锁release掉

            mutex_.Unlock();
            status = log_-&amp;gt;AddRecord(WriteBatchInternal::Contents(updates));
            if (status.ok() &amp;amp;&amp;amp; options.sync) {
                status = logfile_-&amp;gt;Sync();
            }
            if (status.ok()) {
                status = WriteBatchInternal::InsertInto(updates, mem_);
            }
            mutex_.Lock();
        }
        if (updates == tmp_batch_) tmp_batch_-&amp;gt;Clear();

        versions_-&amp;gt;SetLastSequence(last_sequence);
    }

    /*  这里循环判断已经被处理掉的batch, 设置done = true, 并从队列里面取出, 并Pop()掉, 可以看出
        这里都是因为上面做了Batch合并, 同时处理了多个Batch. 所以这里可以直接将这个done = true. 并
        触发这个线程, 然后线程进入刚才的Wait()判断成功. 然后
        if (w.done) {
        return w.status;
        }
        就直接退出了
        就相当于队里头部的这个线程, 完成多其他线程的几个的写操作
    */

    while (true) {
        Writer* ready = writers_.front();
        writers_.pop_front();
        if (ready != &amp;amp;w) {
            ready-&amp;gt;status = status;
            ready-&amp;gt;done = true;
            ready-&amp;gt;cv.Signal();
        }
        if (ready == last_writer) break;
    }

    // 这里之前已经把合并一起的Batch都处理完了, 并且已经处理的Batch都从队列里面Pop()出去了, 然后现在就
    // 唤醒当前队列最前面的线程
    if (!writers_.empty()) {
        writers_.front()-&amp;gt;cv.Signal();
    }

    return status;
}

WriteBatch* DBImpl::BuildBatchGroup(Writer** last_writer) {
    assert(!writers_.empty());
    Writer* first = writers_.front();
    WriteBatch* result = first-&amp;gt;batch;
    assert(result != NULL);

    size_t size = WriteBatchInternal::ByteSize(first-&amp;gt;batch);

    // Allow the group to grow up to a maximum size, but if the
    // original write is small, limit the growth so we do not slow
    // down the small write too much.
    size_t max_size = 1 &amp;lt;&amp;lt; 20; // 这个size 是设置合并的WriteBatch 的大小
    if (size &amp;lt;= (128&amp;lt;&amp;lt;10)) {
        max_size = size + (128&amp;lt;&amp;lt;10);
    }

    *last_writer = first;
    std::deque&amp;lt;Writer*&amp;gt;::iterator iter = writers_.begin();
    ++iter;  // Advance past &quot;first&quot;
    for (; iter != writers_.end(); ++iter) {
        Writer* w = *iter;
        if (w-&amp;gt;sync &amp;amp;&amp;amp; !first-&amp;gt;sync) {
            // Do not include a sync write into a batch handled by a non-sync write.
            break;
        }

        if (w-&amp;gt;batch != NULL) {
            size += WriteBatchInternal::ByteSize(w-&amp;gt;batch);
            if (size &amp;gt; max_size) {
                // Do not make batch too big
                break;
            }

            // Append to *reuslt
            if (result == first-&amp;gt;batch) {
                // Switch to temporary batch instead of disturbing caller&#39;s batch
                result = tmp_batch_;
                assert(WriteBatchInternal::Count(result) == 0);
                WriteBatchInternal::Append(result, first-&amp;gt;batch);
            }
            WriteBatchInternal::Append(result, w-&amp;gt;batch);
        }
        *last_writer = w;  // 同时更新最后的last_writer 到队列里面最新的last_writer
    }
    return result;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;总结:    LevelDB Write 的线程安全是通过将所有的写入插入到一个队列中, 然后有且仅有一个线程去消费这个队列来做到的. 消费这个队列的线程会把队列头部的几个Batch合并成一个大的Batch 一起消费掉, 也是为了提高效率.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>CAP theorem</title>
   <link href="http://baotiao.github.io//2014/01/cap-theorem/"/>
   <updated>2014-01-02T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2014/01/cap-theorem</id>
   <content type="html">&lt;h3&gt;CAP theorem (摘自维基百科)&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Consistency (all nodes see the same data at the same time)&lt;/li&gt;
&lt;li&gt;Availability (a guarantee that every request receives a response about whether it was successful or failed)&lt;/li&gt;
&lt;li&gt;Partition tolerance (the system continues to operate despite arbitrary message loss or failure of part of the system)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;这里 Availability 可以这么理解, 就是在单位的时间内, 这个分布式系统能否给你返回一个成功或者失败.&lt;/p&gt;

&lt;h3&gt;实际工作例子&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;数据的一致性
当客户端写入数据, 考虑可用性和一致性的折中&lt;br/&gt;
可以配置是要eventual consistency 还是 strict consistency.

&lt;ul&gt;
&lt;li&gt;方案一: 主写入BinLog, 直接返回成功. 然后是将记录插入到DB中, 然后同步给从BinLog, 然后从将数据插入到DB中&lt;/li&gt;
&lt;li&gt;方案二: 主写入BinLog, 然后写入DB成功后返回成功. (Dynamo 在W参数 = 1 的时候情形). 然后从同步BinLog, 然后从将数据插入到DB中&lt;/li&gt;
&lt;li&gt;方案三: 主写入BinLog, 写DB同步数据给从的BinLog. 然后返回成功. (Mola, BigTable, Dynamo在W参数= 2 是这个情形). 然后从将数据插入到DB中&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 可以看出方案一到三是 一致性越来越强, 可用性越来越弱的过程.(这里指的是用户的写会越来越慢, 只有之前的事物完成,才算完成) 我们最后选择的方案二, 因为我们这里对一致性的需求没有那么强烈, 如果等到将数据同步. 我们的性能是不允许的&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>How redis implement data structure</title>
   <link href="http://baotiao.github.io//2013/11/survey-redis/"/>
   <updated>2013-11-23T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2013/11/survey-redis</id>
   <content type="html">&lt;ol&gt;
&lt;li&gt;redis implement the append command by realloc the space need by the function.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;When we append the value. Redis first Makeroom for the result.  Redis control the memory by double the memory. The copy the chars to new space by memcpy.&lt;br/&gt;
2. use copy-on-write technology to make as less copy as possible.&lt;br/&gt;
3. &lt;strong&gt;how redis implement list?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;redid really implement two kind of list.&lt;/p&gt;

&lt;p&gt;the first is the zip list, it is also the default list.  every time we have the value the size, we will transfer the zip list the the second link list.&lt;/p&gt;

&lt;p&gt;Why redis implement this way. Because We know link list is the as fast as zip list. At the beginning of time, we just need small space. So we can first encode the chars and store the value. Then if we need the value, what we need to do is just to decode the memory. So It is very quickly. As the data grow, the space is not big enough, and every time the decoding is also waste of time .&lt;/p&gt;

&lt;p&gt;So we need to modify the way we save data, we just put the data in link list, and we travel the list by the point.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>vector clocks</title>
   <link href="http://baotiao.github.io//2013/11/vector-locks/"/>
   <updated>2013-11-12T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2013/11/vector-locks</id>
   <content type="html">&lt;p&gt;vector clocks 是Amazon 的Dynamo 论文提出的一个处理冲突的解决方案&lt;/p&gt;

&lt;p&gt;核心思想是由于server知道的信息有限, 如果发生了冲突, 能做的做大的办法
就是根据timestamp取最新的数据. 如果把冲突放在客户端解决, 由于客户端
知道数据所代表的含义, 那么冲突可以得到更好的解决&lt;/p&gt;

&lt;p&gt;vector clocks 很好理解, 可以看:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://basho.com/why-vector-clocks-are-easy/&quot;&gt;why-vector-clocks-are-easy&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;vector clocks 存在的问题:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://basho.com/why-vector-clocks-are-hard/&quot;&gt;why-vector-clocks-are-hard&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;为什么Cassandra 不用vector clocks. 然后Cassandra的解决方案是什么&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.datastax.com/dev/blog/why-cassandra-doesnt-need-vector-clocks&quot;&gt;why-cassandra-doesnt-need-vector-clocks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Cassandra的解决方案是将一行拆成多列, 然后分别更新. 这样就解决的vector clocks 最擅长解决的问题局部冲突解决的问题.
不过我感觉带来的问题也有就是 将一行拆成多列以后, 获取数据的时候如何保证获取到一致的版本? 还有存储的空间肯定加大了.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Basho levelDB 改进</title>
   <link href="http://baotiao.github.io//2013/11/leveldb-eleveldb/"/>
   <updated>2013-11-11T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2013/11/leveldb-eleveldb</id>
   <content type="html">&lt;h1&gt;eleveldb 对 leveldb 的改进&lt;/h1&gt;

&lt;h2&gt;整体改进目标&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;服务方面: Riak 需要在压力比较大的互联网环境使用. 所以增加了硬件的CRC校验, 增强了Bloom filter 的命中率, 还有默认的数据的完整性的检查&lt;/li&gt;
&lt;li&gt;多数据库支持: Riak 会同时打开 8-64个数据库. Google的leveldb也支持这些, 不过他的compaction 线程不支持这些.
具体的做法是当这个 compaction thread 有太多的事情要做的时候, 就停止让用户写入这些数据.
Basho 的leveldb 的改进包括多个线程同时锁住, 让优先级更高的的线程进行compaction 操作&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Basho 与 官方对比&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;官方: 只限制sst文件的大小&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Basho: 限制sst文件的大小同时限制sst文件key的个数&amp;lt;75000&lt;/p&gt;

&lt;p&gt;原因: 为了控制bloom filter中key的个数, 反正key过多bloom filter的命中率降低&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官方: LevelDB 的每个级别的sst文件大小&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Basho: 定制了每个级别的sst文件大小&lt;/p&gt;

&lt;p&gt;原因: 因为一个进程需要打开64个levelDB实例, 所以需要限制levelDB单个实例的open_files.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官方: 没有统计当前DB的key个数等方法&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Basho: 增加统计工具. 通过在sst文件的头部添加统计结构, 可以统计每一个sst文件中key 的个数&lt;/p&gt;

&lt;p&gt;原因: 方便管理统计&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官方: 没有DB的操作数的记录统计&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Basho: 在Leveldb进程加入shared memory segment, 用来统计Get, Put, OpenFile 等当前信息&lt;/p&gt;

&lt;p&gt;原因: 方便管理&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官方: 当Compaction线程落后很多的时候, 会不可写&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Basho: 增加Compaction线程, 每个线程有优先级.优先级最高的是imm_ 到 Level 0的Compaction&lt;/p&gt;

&lt;p&gt;原因: 因为当imm&lt;em&gt;满的时候, 写入是不允许的. 增加Compaction的优先级, 可以优先满足imm&lt;/em&gt;到Level 0 的Compaction
具体的做法是这样:
有4个线程 normal,  level0 =&gt; level1, background unmap (目前没用), imm_ =&gt; level0&lt;/p&gt;

&lt;p&gt;每个线程各自维护着自己要Compaction的Item, 当出现imm_到level0 的线程需要Compaction 而normal 线程正在Compaction的时候,&lt;/p&gt;

&lt;p&gt;会把normal这个线程里面的Item删除, 加入imm&lt;em&gt; =&gt; level0 的需要Compaction的Item. 这样能有效的提高了imm&lt;/em&gt; =&gt; level0 Compaction的效率&lt;/p&gt;

&lt;h2&gt;具体参数调整&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;write_buffer_size: Riak 会随机把这个write_buffer_size 设置在30MB ~ 60MB 之间, 而默认大小是 4MB.
这样带来的影响是log文件的大小变大, 同样level0 的文件的大小也相应的增大了&lt;/li&gt;
&lt;li&gt;max_open_files: Riak 考虑了max_open_files 对内存的影响, 因为这个 max_open_files 对应的是打开的table_cache的数量, 因此Riak 减少了这一部分的cache&lt;/li&gt;
&lt;li&gt;具体计算哪一个级别需要进行compaction的 score上进行了修改&lt;/li&gt;
&lt;li&gt;对每个级别的sst 文件的大小,  每个级别的MaxBytes, m_DesireBytesForlevel 进行了调整, 而原来leveldb 是每个级别的sst 的大小均为2M,

&lt;h4&gt;可以看出 eleveldb 调整了 write_buffer_size 的大小, 调整了每个级别的sst 文件的大小, 调整了每个级别能有MaxBytesForLevel 等等参数都是为了减少sst 文件的数目.因为Riak 的实现里面也是一个进程开了多个 eleveldb 的实例, 所以如果文件比较多, 那么打开的open_files 就比较多.很容易超出进程的open files.&lt;/h4&gt;&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>levelDB Compaction 相关</title>
   <link href="http://baotiao.github.io//2013/10/leveldb-compaction/"/>
   <updated>2013-10-15T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2013/10/leveldb-compaction</id>
   <content type="html">&lt;h1&gt;levelDB&lt;/h1&gt;

&lt;h2&gt;level DB 如何选择要Compaction的级别&lt;/h2&gt;

&lt;p&gt;这个计算级别的函数在version_set::Finalize() 里面&lt;/p&gt;

&lt;p&gt;在Finalize里面, 有一个算score的过程&lt;/p&gt;

&lt;p&gt;看了一下这个 算Finalive的过程, 根据官方配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;level 0: 差不多 4 个 sst 文件的时候分数 = 1
level 1: 差不多 5 个 sst 文件的时候分数 = 1
level 2: 差不多 50 个 sst 文件的时候分数 = 1
level 3: 差不多 500 个 sst 文件的时候分数 = 1
……
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finalize 只会在 LogAndApply 和 VersionSet::Recover() 的时候被调用, 也就是生成一个新的Version 的时候被调用.&lt;/p&gt;

&lt;p&gt;结论: 所以可以这么说 每次生成一个新的Version 的时候 我们都已经初始化好了这个分数, 判断这一个version 是否需要 compaction 以及那个级别需要compaction&lt;/p&gt;

&lt;h2&gt;level DB 会触发Compaction的操作&lt;/h2&gt;

&lt;p&gt;触发这个MaybeScheduleCompaction() 的地方应该就是有可能触发后台这个Compaction的地方了, 目前会调用到这个MaybeScheduleCompaction() 的地方有&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在进行了一次Compaction 以后, 也就是在DBImpl::Background()函数里面&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;为什么要做Compaction 因为可能会产生太多的新文件在新的一个级别, 所以会检查一下是否需要再进行一次Compaction&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在进行了一次 DBImpl::Get 操作了以后, 如果这个数据是在sst的文件里面找到的.
get的时候找到的这个key存在多个level 0 的文件里面那么就会触发compaction&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;为什么要做Compaction 因为如果一个key在多个文件里面找到,那么说明这个key在多个level 0的文件里面重复了, 所以检查一下是否需要进行compaction
    if (have_stat_update &amp;amp;&amp;amp; current-&gt;UpdateStats(stats)) { // 这里如果有更新, 那么会判断是否启动后台的Compaction() 进程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在 DBImpl::Write 的 MakeRoomForWrite 函数里面, 当immutable 生成一个level 0 文件的时候, 会检查一下是否需要Compaction, 这样会防止level 0 文件过多.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;为什么要做Compaction 这时候做Compaction, 主要为了防止不断的从immutable 生成到level 0 文件, 一直触发immutable到level0 过程, 而没有时间进行其他级别的合并 并且在MakeRoomFroWrite 的时候, 我们会检查一下 如果level0 的文件数 &gt; config::kL0_SlowdownWritesTrigger 这个数据的大小的话. 那么我们 就会sleep 一段时间, 也是为了让出时间给其他级别进行Compaction&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在DB::Open() 这里函数里面, 如果Recover 成功以后, 并且进行了
s = impl-&gt;versions&lt;em&gt;&gt;LogAndApply(&amp;amp;edit, &amp;amp;impl&gt;mutex&lt;/em&gt;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;为什么要做Compaction 这里会将可以上次DB关闭以后没有来得及写入的数据重新回放, 所以这里可能会生成新的level 0的文件, 所以这里也会进行 检查 MaybeScheduleCompaction().&lt;/p&gt;

&lt;h2&gt;具体的MaybeCompaction() 过程&lt;/h2&gt;

&lt;h4&gt;函数入口&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;void DBImpl::MaybeScheduleCompaction() {
mutex_.AssertHeld();
// 如果后台有Compaction 线程, 那么直接退出
if (bg_compaction_scheduled_) {
            // Already scheduled
            // 如果db 要被 shut_down, 直接退出
        } else if (shutting_down_.Acquire_Load()) {
            // DB is being deleted; no more background compactions
            // 如果 imm_ 这个文件还是空的, 并且是manual_compaction是空的, 这里
            // TODO
        } else if (imm\_ == NULL &amp;amp;&amp;amp;
manual_compaction\_ == NULL &amp;amp;&amp;amp;
\!versions_-&amp;gt;NeedsCompaction()) {
            // No work to be done
        } else {
            // 设置这个后台有compaction 线程已经启动
            bg_compaction_scheduled_ = true;
            env_-&amp;gt;Schedule(&amp;amp;DBImpl::BGWork, this); //调用下面的 BGWork函数. 这里虽然是env_, 当时这env_里面会调用这个函数指针, 调用DBImpl::BGWork 这个函数
        }
}

* 在PosixEnv::Schedule 这个函数里面

void PosixEnv::Schedule(void (*function)(void*), void\* arg) {
PthreadCall(&quot;lock&quot;, pthread_mutex_lock(&amp;amp;mu_));

// Start background thread if necessary
// 看是否有后台线程已经启动, 如果没有启动就启动这个后台线程, 并执行一个死循环
// 具体的执行是BGThreadWrapper \-&amp;gt; BGThread 这个函数,
// 在BGThread 函数就是一个死循环, 不断的从这个queue\_ 里面读出, 这个是FIFO的形式
// 读出, 先进先出. 没有做一个优先级的概念.
if (\!started_bgthread_) {
                started_bgthread_ = true;
                PthreadCall(
                        &quot;create thread&quot;,
                        pthread_create(&amp;amp;bgthread_, NULL,  &amp;amp;PosixEnv::BGThreadWrapper, this));
            }

// If the queue is currently empty, the background thread may currently be
// waiting.
// 如果这个queue\_ 里面的数据当前是空的, 则等待cond 锁让它起来
if (queue_.empty()) {
                PthreadCall(&quot;signal&quot;, pthread_cond_signal(&amp;amp;bgsignal_));
            }

// Add to priority queue
queue_.push_back(BGItem());
queue_.back().function = function; // 这里注册的函数是 &amp;amp;DBImpl::BGWork
queue_.back().arg = arg; // 这里arg 是 db-&amp;gt;this指针

PthreadCall(&quot;unlock&quot;, pthread_mutex_unlock(&amp;amp;mu_));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;接下来是具体执行 函数 BackgroundCall() -&gt; BackgroundCompaction().&lt;/h4&gt;

&lt;p&gt;在BackgroundCompaction() 函数里面&lt;/p&gt;

&lt;p&gt;优先 compaction immutable -&gt; level0 sst&lt;/p&gt;

&lt;p&gt;然后 我们都是!is_manual 的, 那么我们就要选择去Compaction() 那个级别的.&lt;/p&gt;

&lt;p&gt;在versions_-&gt;PickCompaction().&lt;/p&gt;

&lt;p&gt;这里我们有之前在Finalfize() 里面算出来的compaction_score_, 如果这个score &amp;lt; 1 就不进行compaction.&lt;/p&gt;

&lt;p&gt;这里可以看到并不是每次检查是否需要Compaction 的时候都会进行. 只有score &gt;= 1 的时候levelDB才会选一个级别进行Compaction()&lt;/p&gt;

&lt;p&gt;在选择好Compaction的级别以后. 就调用BackgroundCompaction&lt;/p&gt;

&lt;p&gt;如果生成的这个Compaction 是空的, 那么就不进行Compaction
选择是否能直接将这个文件移动到level + 1, 而不用与level + 1 的文件进行归并的计算
进行真正的Compaction DoCompactionWork() 函数&lt;/p&gt;

&lt;h4&gt;在DoCompactionWork()函数里面&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;对这些指针进行归并, 归并出一个MergeIterator input.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;具体的iterator 看leveldb iterator&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;遍历获得的需要合并的数据, 如果这个key以前是否出现.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;如果已经出现过了就不会再进行处理 因为leveldb 里面对相同的key是进行过排序的. 默认squencenumber 最大的排在最前面, 也就是最新的数据排在最前面.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;如果这个key 的squenctNumber &amp;lt; 当前快照的版本号, 说明这个key 是旧的了.
如果这个key 的类型是delete, 并且更高级别已经没有这个key的数据了, 那么这个key也会被drop掉
可以看出, levelDB 这里做了这些操作也都是尽可能的减少key的数量
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;接下来就把这些剩余的key插入到新的version里面&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>levelDB Get过程</title>
   <link href="http://baotiao.github.io//2013/09/leveldb-Get/"/>
   <updated>2013-09-05T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2013/09/leveldb-Get</id>
   <content type="html">&lt;pre&gt;&lt;code&gt;Status DBImpl::Get(const ReadOptions&amp;amp; options,
        const Slice&amp;amp; key,
        std::string* value) {
    Status s;
    MutexLock l(&amp;amp;mutex_); //这里初始化levelDB的锁, 默认把锁加上
    SequenceNumber snapshot; //这里就是定义一个最新的一个操作号, 有一个全局
    唯一的SequenceNumber
    if (options.snapshot != NULL) { //如果要求取的是某一个版本的数据
        snapshot = reinterpret_cast&amp;lt;const SnapshotImpl*&amp;gt;(options.snapshot)-&amp;gt;number_;
    } else {
        snapshot = versions_-&amp;gt;LastSequence(); //否则就是最新的数据, 也就是当
        前versions_里面最大的SequenceNumber的数据
    }

    MemTable* mem = mem_; //mem table
    MemTable* imm = imm_; //imm table
    Version* current = versions_-&amp;gt;current(); //当前的version
    mem-&amp;gt;Ref(); //对mem的引用+1, 这个ref主要是用来删除文件的时候判断, 如果这
    个ref引用为0了, 那么就可以删除掉.
    if (imm != NULL) imm-&amp;gt;Ref();
    current-&amp;gt;Ref(); //同样对当前版本的ref引用+1

    bool have_stat_update = false; //用来是否有更新, 如果有更新再判断是否启动compaction线程
    Version::GetStats stats;

    // Unlock while reading from files and memtables
    {
        mutex_.Unlock(); //把锁放开, 可以看到 正真加锁部分只有获得当前版本号
        , 以及获得当前最新的版本这一部分, 也就是说在具体的get操作之前就已经
        可以支持多个线程进行读取了. 为什么可以这么做呢? 首先获得了当前最新的
        current以后, 并把这个current 的引用+1, 就可以保证当前的这个version
        是不会被删除的, 同样对于当前的这个imm, mem ref+1 以后可以保证是不会
        呗删除掉得. 所以只要在这段时间锁住就可以. 如果这个时候又有新的key写
        入, 那么他这个时候写入的key 是一个新的SequenceNumber. 不会影响我们接
        下来读的结果.

        // First look in the memtable, then in the immutable memtable (if any).
        LookupKey lkey(key, snapshot);
        if (mem-&amp;gt;Get(lkey, value, &amp;amp;s)) { //从mem里面读取这个key的 value  这
        里要注意memtable里面的kv 是如何排序的. 这里面key的排序是 首先按照
        SquenceNumber排序, 然后是操作类型(删除排在最前面), 然后是key的大小(
        具体再看一下Compaction里面)
            // Done
        } else if (imm != NULL &amp;amp;&amp;amp; imm-&amp;gt;Get(lkey, value, &amp;amp;s)) {
            // Done
        } else {
            s = current-&amp;gt;Get(options, lkey, value, &amp;amp;stats); //如果mem 和
            imm 都找不到, 那么这个时候我们要从一个一个level里面去找.
            have_stat_update = true;
        }
        mutex_.Lock();
    }

    if (have_stat_update &amp;amp;&amp;amp; current-&amp;gt;UpdateStats(stats)) { //
    这里如果有更新, 那么会判断是否启动后台的Compaction() 进程
        MaybeScheduleCompaction();
    }
    mem-&amp;gt;Unref(); //分别把 mem, imm, current 的ref - 1
    if (imm != NULL) imm-&amp;gt;Unref();
    current-&amp;gt;Unref();
    return s;
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>levelDB thought</title>
   <link href="http://baotiao.github.io//2013/09/leverdb-thought/"/>
   <updated>2013-09-03T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2013/09/leverdb-thought</id>
   <content type="html">&lt;ol&gt;
&lt;li&gt;在ENV 里面可以把levelDB的结果从写入到本地 改成写入到 hdfs 来实现数据的备份, 复制等操作 具体的做法就是调用hdfs的写入这些库来实现.  这样实现levelDB的分布实话非常方便&lt;/li&gt;
&lt;li&gt;levelDB的VersionSet 是管理者 version. 然后 每一个version 有一个列表, 这个列表是这个version 的对应的所有的SST文件.  所以你要查找某一个Version的数据的时候.  先会在这个VersionSet里面查找一遍包含这个当前快照的一个版本, 然后再从这个version 的这个list里面去具体的文件查找具体的内容&lt;/li&gt;
&lt;li&gt;在一台机器上面getInstance()出来1000 levelDB的实例的话, 只会有一个compaction线程, 然后一个机器1000 个levelDB 实例和1个机器1个levelDB的实例的话 带来的好处是在机器挂掉得时候recovery的非常的快.  不过这样compaction起来就很费劲&lt;/li&gt;
&lt;li&gt;在将本地文件写入到hdfs的节点中, 因为hdfs的写入性能比较慢.  所以在本地应该是writrBranch. 然后20ms向hdfs写一次. 这样比较适合.&lt;/li&gt;
&lt;li&gt;levelDB 如何实现原子的getAndSet.  因为levelDB不是再内存层面实现这个对具体某一个key操作. 所以这个levelDB 的getAndSet操作不是通过汇编层面实现的.&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>levelDB中用到的迭代器模型</title>
   <link href="http://baotiao.github.io//2013/08/leveldb-iterator/"/>
   <updated>2013-08-22T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2013/08/leveldb-iterator</id>
   <content type="html">&lt;h3&gt;迭代器的设计模式是一种很常用的设计模式. leveldb的实现里面就用到了.&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;Iteartor模式:提供一种方法顺序访问一个聚合对象中的各个元素, 而又不暴露其内部的表示.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在leveldb 里面include/iterator.h 定义了 iterator.h 的基类, leveldb 里面有memtable, block 等数据格式. 都是通过定义一个自己的iterator来实现对这一个数据的访问.&lt;/p&gt;

&lt;p&gt;比如这里的block类:
在每一个类的里面都定义了一个&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Iterator* NewIterator(const Comparator* comparator);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在 NewIterator 的实现里面&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Iterator* Block::NewIterator(const Comparator* cmp) {
    if (size_ &amp;lt; 2*sizeof(uint32_t)) {
        return NewErrorIterator(Status::Corruption(&quot;bad block contents&quot;));
    }
    const uint32_t num_restarts = NumRestarts();
    if (num_restarts == 0) {
        return NewEmptyIterator();
    } else {
        return new Iter(cmp, data_, restart_offset_, num_restarts);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;会具体的返回一个在这个类内部的一个指针, 这个指针在这个block类的内部具体定义的.
这个Iter指针实现了需要的所有的操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Block::Iter : public Iterator {
    private:
        const Comparator* const comparator_;
        const char* const data_;      // underlying block contents
        uint32_t const restarts_;     // Offset of restart array (list of fixed32)
        uint32_t const num_restarts_; // Number of uint32_t entries in restart array


        // current_ is offset in data_ of current entry.  &amp;gt;= restarts_ if !Valid
        uint32_t current_;
        uint32_t restart_index_;  // Index of restart block in which current_ falls
        std::string key_;
        Slice value_;  // 这里就会直接存这个value_的值
        Status status_;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next(), Prev(), Value() 等等操作&lt;/p&gt;

&lt;p&gt;比如这里Next()的实现的时候, 没进行一个Next()操作, 就会调用ParseNextKey(), 然后这个函数就会更新value&lt;em&gt;的值. 所以调用Value()的时候直接返回 value&lt;/em&gt;即可.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;virtual void Next() {
    assert(Valid());
    ParseNextKey();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样在这个dbImpl这个类里面, dbImpl的iterator的定义就在db_impl.cc里面定义.&lt;/p&gt;

&lt;p&gt;使用这种迭代器模型, 我们调用的时候就可以不用知道这个具体的结构, 直接用一个Iterator, 就可以使用这个类&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Iterator* iter = mem-&amp;gt;NewIterator(); //这个是memtable 
Iterator* iter = table_cache_-&amp;gt;NewIterator(ReadOptions(), output_number, current_bytes); //table_cache 这个类
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>learn c++ from levelDB</title>
   <link href="http://baotiao.github.io//2013/08/leveldb-c++/"/>
   <updated>2013-08-22T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2013/08/leveldb-c++</id>
   <content type="html">&lt;h3&gt;LevelDB是一个学习c++很好的一个代码. 里面有很多写代码的好习惯值得我们学习.&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;assert 断言, 在一些判断的地方直接用 assert 来提示出错 比如&lt;br/&gt;
      char operator&lt;a href=&quot;size_t%20n&quot;&gt;&lt;/a&gt; const { assert(n &amp;lt; size()); return data_[n]; }&lt;/li&gt;
&lt;li&gt;尽量如果一个成员函数的方法不会改变成员变量的话, 添加上const 结尾&lt;/li&gt;
&lt;li&gt;如果想让一个类不被复制,那么就设置它的copy construct 和 copy assign 为 私有函数,&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;并且你要实现的类继承它就可以了, leveldb 里面的WritableFile 就是这么设置的
标注的定义一个类的基类的方法:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    class WritableFile {
        public:
            WritableFile() { }
            virtual ~WritableFile();
            virtual Status Append(const Slice&amp;amp; data) = 0;
            virtual Status Close() = 0;
            virtual Status Flush() = 0;
            virtual Status Sync() = 0;

        private:
            // No copying allowed
            WritableFile(const WritableFile&amp;amp;);
            void operator=(const WritableFile&amp;amp;);
    };
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;在设计中多用迭代器模型, 可以抽象更多的细节.&lt;/li&gt;
&lt;li&gt;未完待续...&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Mac 换上SSD硬盘</title>
   <link href="http://baotiao.github.io//2013/07/add-ssd/"/>
   <updated>2013-07-18T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2013/07/add-ssd</id>
   <content type="html">&lt;p&gt;给Mac换上的SSD硬盘步骤&lt;/p&gt;

&lt;p&gt;[前期准备]&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;支持SATA的移动硬盘盒&lt;/li&gt;
&lt;li&gt;一套电子维修专用改锥&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;[第一步:安装克隆软件]&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在mac上安装Carbon copy cloner软件（简称CCC,请支持正版）&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;[第二步:克隆磁盘]&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;将SSD装入移动硬盘,接到mac上&lt;/li&gt;
&lt;li&gt;使用CCC将HDD的内容克隆到SSD,一切按提示操作,注意要创建恢复分区,否则SSD无法引导&lt;/li&gt;
&lt;li&gt;等待30~60分钟&lt;/li&gt;
&lt;li&gt;重启电脑,并按住option,选择从SSD启动,检查是否成功启动&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;[第三步:安装SSD]&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;将SSD装入mac,外面10颗螺丝,里面6颗螺丝,注意硬盘上有四颗内五角螺丝需要专用改锥&lt;/li&gt;
&lt;li&gt;启动电脑,体验非一般的感觉吧&lt;/li&gt;
&lt;/ol&gt;


&lt;pre&gt;&lt;code&gt;    all: TRIM SMS noatime reboot 

    checkroot:
        @echo -n checking privilege...
        @touch / &amp;amp;&amp;gt;/dev/null || ( echo error!;exit 1; )
        @echo ok!


    SMS: checkroot
        @echo -n disabling Sudden Motion Sensor...
        @pmset -a sms 0 || ( echo error!;exit 1; )
        @echo done!

    TRIM: checkroot
        @echo -n enabling TRIM...
        @cp /System/Library/Extensions/IOAHCIFamily.kext/Contents/PlugIns/IOAHCIBlockStorage.kext/Contents/MacOS/IOAHCIBlockStorage /System/Library/Extensions/IOAHCIFamily.kext/Contents/PlugIns/IOAHCIBlockStorage.kext/Contents/MacOS/IOAHCIBlockStorage.bak
        @perl -pi -e &#39;s|(\x52\x6F\x74\x61\x74\x69\x6F\x6E\x61\x6C\x00).{9}(\x00\x54)|$1\x00\x00\x00\x00\x00\x00\x00\x00\x00$2|sg&#39; /System/Library/Extensions/IOAHCIFamily.kext/Contents/PlugIns/IOAHCIBlockStorage.kext/Contents/MacOS/IOAHCIBlockStorage
        - @kextcache -system-prelinked-kernel &amp;amp;&amp;gt;/dev/null
        @kextcache -system-caches
        @echo done

    noatime: checkroot
        @echo -n disabling atime...
        @echo &#39;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt; &amp;lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot; &quot;http://www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&amp;gt; &amp;lt;plist version=&quot;1.0&quot;&amp;gt; &amp;lt;dict&amp;gt; &amp;lt;key&amp;gt;Label&amp;lt;/key&amp;gt; &amp;lt;string&amp;gt;com.noatime.root&amp;lt;/string&amp;gt; &amp;lt;key&amp;gt;ProgramArguments&amp;lt;/key&amp;gt; &amp;lt;array&amp;gt; &amp;lt;string&amp;gt;mount&amp;lt;/string&amp;gt; &amp;lt;string&amp;gt;-uwo&amp;lt;/string&amp;gt; &amp;lt;string&amp;gt;noatime&amp;lt;/string&amp;gt; &amp;lt;string&amp;gt;/&amp;lt;/string&amp;gt; &amp;lt;/array&amp;gt; &amp;lt;key&amp;gt;RunAtLoad&amp;lt;/key&amp;gt; &amp;lt;true/&amp;gt; &amp;lt;/dict&amp;gt; &amp;lt;/plist&amp;gt;&#39; &amp;gt; /Library/LaunchDaemons/com.noatime.root.plist || ( echo error!;exit 1; )
        @echo done!

    reboot: checkroot
        @echo Finished !
        @echo CAUTION: we are going to reboot, press Ctrl-C to abort and you can reboot manually later.;sleep 5;
        @for i in `seq 10 1`; do clear; echo CAUTION: we are going to reboot in $$i seconds;sleep 1; done;
        @reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行方式为:sudo make -f ssd_opt.makefile&lt;/p&gt;

&lt;h4&gt;执行优化包括:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;打开TRIM&lt;/li&gt;
&lt;li&gt;禁用atime&lt;/li&gt;
&lt;li&gt;禁用磁盘保护&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;注意:&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;打开TRIM时,会误报segment fault,忽略即可&lt;/li&gt;
&lt;li&gt;由于系统版本原因,TRIM可能打开失败,如何检测成功与否,以及失败了怎么办,请参见:http://www.cnbeta.com/articles/219752.htm&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>test markdown</title>
   <link href="http://baotiao.github.io//2013/07/test-markdown/"/>
   <updated>2013-07-01T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2013/07/test-markdown</id>
   <content type="html">&lt;p&gt;first time use in markdown&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Linux内核设计与实现 进程地址空间 实际中使用的内存</title>
   <link href="http://baotiao.github.io//2012/11/linux-kernel-process/"/>
   <updated>2012-11-16T10:28:59+08:00</updated>
   <id>http://baotiao.github.io//2012/11/linux-kernel-process</id>
   <content type="html">&lt;pre&gt;&lt;code&gt;int main(int argc, char *argv[])
{
    while (1) {
    };
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用pmap 进程号 -d 来查看进程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Address           Kbytes Mode  Offset           Device    Mapping
0000000000400000       4 r-x-- 0000000000000000 4f9:2c566 a.out
0000000000500000       4 rw--- 0000000000000000 4f9:2c566 a.out
000000302ad00000      84 r-x-- 0000000000000000 008:00002 ld-2.3.4.so
000000302ae14000       8 rw--- 0000000000014000 008:00002 ld-2.3.4.so
000000302af00000    1196 r-x-- 0000000000000000 008:00002 libc-2.3.4.so
000000302b02b000    1020 ----- 000000000012b000 008:00002 libc-2.3.4.so
000000302b12a000      12 r---- 000000000012a000 008:00002 libc-2.3.4.so
000000302b12d000      12 rw--- 000000000012d000 008:00002 libc-2.3.4.so
000000302b130000      16 rw--- 000000302b130000 000:00000   [ anon ]
00002ae829a73000       4 rw--- 00002ae829a73000 000:00000   [ anon ]
00002ae829a87000       8 rw--- 00002ae829a87000 000:00000   [ anon ]
00007fff81022000      84 rw--- 00007fff81022000 000:00000   [ stack ]
ffffffffff600000       4 r-x-- 0000000000000000 000:00000   [ anon ]
mapped: 2456K    writeable/private: 136K    shared: 0K

这里的顺序就是进程的地址空间中各个区域的顺序
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一行 a.out 的代码段 执行权限是r-x
第二行 a.out 的数据段 执行权限是rw-
然后是mapped file
ld的代码段 执行权限r-x
ld的数据段 执行权限rw-
libc库代码段 执行权限r-w
libc库这个我也不太清楚
libc库....
libc库的数据段 执行权限rw-&lt;/p&gt;

&lt;p&gt;接下来有三行的anon, 分别是a.out, ld, libc 的BSS段, BSS段映射文件的内存区域的设备标志为000:00000 这个区域是0页,0页映射的内容全部为零.因为bss段是未初始化的全局变量区,在进程里面未初始化数据区存的都是0.操作系统有写时复制机制,这样子真正有修改的时候,才会从物理内存中获得空间.&lt;/p&gt;

&lt;p&gt;该进程全部地址空间2456K, 可写空间136K.不可写区域可以和其他进程共享,所以多个进程都共享libc库,以及内核的代码.所以物理内存里面只需要保存一份libc库的代码节省了大量的空间&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Linux内核的设计与实现:块I/O层</title>
   <link href="http://baotiao.github.io//2012/10/linux-kernel-io/"/>
   <updated>2012-10-18T23:31:12+08:00</updated>
   <id>http://baotiao.github.io//2012/10/linux-kernel-io</id>
   <content type="html">&lt;h1&gt;块设备&lt;/h1&gt;

&lt;h3&gt;扇区:&lt;/h3&gt;

&lt;p&gt;扇区是块设备中最小的可寻址的单元,大小一般是2的整数倍.扇区的大小是设备的物理属性,扇区是所有块设备的基本单元--块设备无法对比它还小的单元进行寻址和操作.&lt;/p&gt;

&lt;h3&gt;块:&lt;/h3&gt;

&lt;p&gt;块是自己的最小逻辑可寻址单元.块是文件系统的一种抽象--只能及愉快来范围文件系统.虽然物理磁盘寻址是按照扇区级进行的,但是内核执行的所有的磁盘操作都是按照块进行的.由于扇区是设备的最小可寻址单元,所有块不能比扇区还小,只能数倍于扇区大小.&lt;/p&gt;

&lt;h1&gt;I/O调度程序&lt;/h1&gt;

&lt;h2&gt;I/O调度程序&lt;/h2&gt;

&lt;p&gt; IO调度程序将磁盘I/O资源分配给系统中所有挂起的块I/O请求.具体的说这种资源分配是通过将请求队列中挂起的请求合并和排序来完成的.&lt;/p&gt;

&lt;p&gt; 与进程调度的区别&lt;/p&gt;

&lt;p&gt; 进程调度程序的作用是将处理器资源分配给系统中的运行进程.&lt;/p&gt;

&lt;p&gt; 进程调度程序和I/O调度程序都是将一个资源虚拟给多个对象,对进程调度来说,处理器被虚拟并被系统中的运行进程共享.这种虚拟提供给用户的就是多任务和分时操作系统,像Unix系统.相反,I/O调度程序虚拟块设备给多个磁盘请求,以便降低磁盘寻道的时间, 确保磁盘性能的最优化&lt;/p&gt;

&lt;h2&gt;I/O调度程序的工作&lt;/h2&gt;

&lt;p&gt;  IO调度程序的工作是管理块设备的请求队列.它觉得队列中的请求排列顺序以及在什么时候派发请求到块设备.这样做有利于减少磁盘寻址时间,从而提高全局吞吐量.&lt;/p&gt;

&lt;p&gt;  IO调度两种方法减少寻址时间:合并和排序&lt;/p&gt;

&lt;p&gt;  合并:将两个或多个请求结合成一个新请求.&lt;/p&gt;

&lt;p&gt;  这种情况,文件系统提交请求到请求队列--从文件中读取一个数据区,如果这时文件中已经存在一个请求,它访问的磁盘扇区和当前请求访问的磁盘扇区相连(比如同一个文件中早些时候被读取的数据区),那么这两个请求可以合并为一个对单个和多个相邻磁盘扇区操作的新请求.通过合并请求,I/O调度程序将多次请求的开销压缩成一次请求的开销,而且只需要传递给磁盘一条寻找命令,就可以访问到合并前多次寻址才能访问完的磁盘区域.(寻址是I/O里面最费时的操作)&lt;/p&gt;

&lt;p&gt;  排序:I/O调度程序是的整个请求队列按扇区增长方向有序排序.使所有秦秋按磁盘扇区排列的顺序尽可能有序排列.这样不只是缩短一次请求的寻址时间,更重要的,通过保存磁盘头以直线方向移动,缩短了所有请求的磁盘寻址时间.该排序算法类似于电梯调度&lt;/p&gt;

&lt;h2&gt;Linux 电梯&lt;/h2&gt;

&lt;p&gt;  当一个请求加入到队列中是,有可能发生4中操作
   1. 如果队列中已存在一个对相连磁盘扇区操作的请求,那么新请求将和这个已存在的请求合并成一个请求
   2. 如果队列中存在一个驻留时间过长的请求,那么新请求将被插入到队列尾部,已防止其他旧的请求发生饥饿
   3. 如果队列中以扇区方向为序存在合适的插入位置, 那么新的请求将被插入到该位置,保证队列中的请求是以被访问磁盘物理位置为序进行排列
   4. 如果队列中不存在合适的请求插入位置,请求将被插入到队列尾部&lt;/p&gt;

&lt;h2&gt;其他4中I/O调度程序&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt; . 最终期限I/O调度程序&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt; . 预测I/O调度程序&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt; . 完全公正的排队I/O调度程序&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt; . 空操作的I/O调度程序&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>c++对象模型 构造函数语义学</title>
   <link href="http://baotiao.github.io//2012/09/c++-constuct/"/>
   <updated>2012-09-12T15:16:14+08:00</updated>
   <id>http://baotiao.github.io//2012/09/c++-constuct</id>
   <content type="html">&lt;h1&gt;1. C++ 不会为所有的class构造默认的constructor&lt;/h1&gt;

&lt;p&gt;(我们经常认为c++会为所有的class默认添加构造函数)
只有满足4个条件之一会添加构造函数
     1) 包含带有默认构造函数的对象成员的类
      2) 继承自带有默认构造函数的基类的类
      3) 带有虚函数的类
      4) 带一个虚基类的类
这4中情况会导致编译器必须为未申明构造函数的class, 生成一个默认的constructor. 更多情况下是不会去生成默认的构造函数的.&lt;/p&gt;

&lt;h1&gt;2. copy constructor 构造语义学&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt; C++并不是为所有的类都够着copy constructor. 默认的情况是如果没有定义copy constructor, 那么一般情况下如果有copy constructor 操作, 默认的是执行bitwise copy semantics. 也就是位逐次拷贝. 当然 bistwise copy 是有条件的,当不满足bitwise copy 的条件的时候,就不能进行bitwise copy. 那么C++就会为该类添加默认的copy constructor. ( 这里我感觉就是深拷贝和浅拷贝的区别,很明显bitwise 是浅拷贝,直接将一个对象的data member 拷贝给同一个类的另一个对象. 而如果这里不能直接内存拷贝,则需要深拷贝.就是需要构造一个copy constructor了)
 4种情况不满足bitwise copy. 也就是会调用构造函数
 1)当一个class 内的一个member object,而这个object 声明了一个copy constructor时
 2)当class 继承自一个bass class, 而这个bass class 有 copy constructor
 3)当class申明了一个或多个 virtual functions 时
 4)当class 派生自一个继承串链,其中一个或多个virtual base classes
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;3. copy assign constructor 构造语义学&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt; 跟上面的copy constructor 构造语义学一样, 如果能进行浅拷贝就浅拷贝, 不行的时候才会去生成assign operator
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;4. 对象析构语义学.&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt; 只有在基类拥有析构函数,或者object member 拥有析构函数的时候,编译器在为类合成析构函数,否则都被视为不需要.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这几个默认生成的函数都是在需要的时候才会被编译器生成出来, 默认情况下是不会生成这些函数的.所以如果class 当成一个存结构体使用, 效率是和C 的stuct 是一样的&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>c++ 对象模型</title>
   <link href="http://baotiao.github.io//2012/08/c++-class/"/>
   <updated>2012-08-25T17:15:01+08:00</updated>
   <id>http://baotiao.github.io//2012/08/c++-class</id>
   <content type="html">&lt;ol&gt;
&lt;li&gt;C++的对象模型里面,Nonstatic data members 配置于每一个class object之内, static data members 则被存放在所有的class object之外. static 和 non static function members 也被放在所有的所有的class object 之外. Virtual function 则以两个步骤支持&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;     1)每一个class 产生出一堆执行virtual functions 的指针,放在表格之中.这个表格被称为virtual table(vtbl)&lt;/p&gt;

&lt;p&gt;     2)每一个class object 被添加了一个指针,指向相关的virtual table, 通常这个指针被称为vptr. vptr的设定和重置都有每一个class的constructor, destructor 和copy assignment运算法自动完成. 如下图:(为什么采用这种模型是我们应该考虑的,是基于性能和空间的综合考虑.可以看一下简单对象模型和表格驱动对象模型)&lt;/p&gt;

&lt;p&gt; &lt;img src=&quot;http://chenzongzhi.info/wp-content/uploads/2012/08/3e55c34139de2e6b58bc737a91b41905.gif&quot; alt=&quot;3e55c34139de2e6b58bc737a91b41905&quot; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在继承的时候,对象的内存布局是这样的&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;基类:&lt;/p&gt;

&lt;p&gt;class ZooAnimal {&lt;/p&gt;

&lt;p&gt;public:&lt;/p&gt;

&lt;p&gt;   ZooAnimal();&lt;/p&gt;

&lt;p&gt;   virtual ~ZooAnimal();&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;   // ...&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;   virtual void rotate();&lt;/p&gt;

&lt;p&gt;protected:&lt;/p&gt;

&lt;p&gt;   int loc;&lt;/p&gt;

&lt;p&gt;   String name;&lt;/p&gt;

&lt;p&gt;};&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://chenzongzhi.info/wp-content/uploads/2012/08/ee21eaea173ebe93e1da4a422db9004e.gif&quot; alt=&quot;Ee21eaea173ebe93e1da4a422db9004e&quot; /&gt;&lt;/p&gt;

&lt;p&gt;子类:&lt;/p&gt;

&lt;p&gt;class Bear : public ZooAnimal {&lt;/p&gt;

&lt;p&gt;public:&lt;/p&gt;

&lt;p&gt;   Bear();&lt;/p&gt;

&lt;p&gt;   ~Bear();&lt;/p&gt;

&lt;p&gt;   // ...&lt;/p&gt;

&lt;p&gt;   void rotate();&lt;/p&gt;

&lt;p&gt;   virtual void dance();&lt;/p&gt;

&lt;p&gt;   // ...&lt;/p&gt;

&lt;p&gt;protected:&lt;/p&gt;

&lt;p&gt;   enum Dances { ... };&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;   Dances dances_known;&lt;/p&gt;

&lt;p&gt;   int cell_block;&lt;/p&gt;

&lt;p&gt;};&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Bear b( &quot;Yogi&quot; );&lt;/p&gt;

&lt;p&gt;Bear *pb = &amp;b;&lt;/p&gt;

&lt;p&gt;Bear &amp;amp;rb = *pb;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://chenzongzhi.info/wp-content/uploads/2012/08/c4613a1593002490b4ee517f61fcd5fc.gif&quot; alt=&quot;C4613a1593002490b4ee517f61fcd5fc&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到,子类是直接将基类的data member 空间直接拷贝过来的.&lt;/p&gt;

&lt;p&gt;然后在它的下面添加上自己的data menber&lt;/p&gt;

&lt;p&gt;3.一个类的对象的内存大小包括：  &lt;/p&gt;

&lt;p&gt;    1) 所有非静态数据成员的大小。&lt;/p&gt;

&lt;p&gt;    2) 由内存对齐而填补的内存大小。&lt;/p&gt;

&lt;p&gt;    3) 为了支持virtual有内部产生的额外负担,一个指针的大小.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://erektilepillenonline.com/products/viagra.htm&quot;&gt;viagra&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>c++ inline</title>
   <link href="http://baotiao.github.io//2012/07/c-inline/"/>
   <updated>2012-07-29T23:46:19+08:00</updated>
   <id>http://baotiao.github.io//2012/07/c-inline</id>
   <content type="html">&lt;p&gt;一个函数被定义成inline 函数. 那么就跟macro 宏一样, 在编译器编译的时候,如果编译器发现你的函数是inline类型的,就把你编译成inline code.&lt;/p&gt;

&lt;p&gt;编译器编译原文件以后,会单独把inline函数列出来.然后当有地方调用inline函数的时候,就会直接把编译好的inline code 插入到调用的这个地方.&lt;/p&gt;

&lt;p&gt;普通的函数调用一个函数,在汇编成面,会有一个函数调用,找到这个函数的地址,然后传递参数给这个函数,然后执行这个函数,最后从函数出返回值.然后堆栈又要恢复调用这个函数前的样子.因此每次调用一次函数都有时间和空间的开销. 如果要调用的函数仅仅是一个小函数,那么每次都重复这样的一个过程肯定非常的浪费时间,并且浪费堆栈的空间,如果是一个递归调用,使用堆栈的空间就更多了.
如果要调用的函数非常长,那么花这个函数调用的时间还是值得的.如果要调用的函数非常的短,那么就可以直接将要调用的函数复制到要调用这个函数的地方.如果函数过长,可以想象每次都要把一个这么长的函数复制到其他地方,是不能接受的. 所以如果一个函数过长,编译器会放弃使用内联的方式.&lt;/p&gt;

&lt;p&gt;总结:内联函数本身通过减少函数调用开销来起到优化目的.内联过大的函数不但起不到优化效果,还会导致代码膨胀,增加内存换页次数,得不偿失.&lt;/p&gt;

&lt;p&gt;所以并不是所有地方都用inline 函数,但是可以用macro的地方必定可以用inline替换.&lt;/p&gt;

&lt;p&gt;inline 是一个对函数的定义修饰的一个修饰符.
所以如果是
inline int max(int a, int b); 是没用的, 这里是函数的声明,不是函数的定义.&lt;/p&gt;

&lt;p&gt;inline 即使函数的声明没声明inline, 函数的定义声明是Inline就可以了.&lt;/p&gt;

&lt;p&gt;tips: 注意在类定义里给出定义的方法会自动内联.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>c++泛型单例模式</title>
   <link href="http://baotiao.github.io//2012/06/c++-single-model/"/>
   <updated>2012-06-19T02:23:13+08:00</updated>
   <id>http://baotiao.github.io//2012/06/c++-single-model</id>
   <content type="html">&lt;p&gt;单例模式是在设计模式里面很常见的一种,用来确保一个类只有一个实例. 首先是原生态版本的实现.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;
#ifndef SING_H
class sing {
    private:
        static sing *m_sing;
    public:
        int num;
        static void open();
        static void close();
        static sing *getInstance();
        void func();
    protected:
        ~sing();
        sing();
};

#define SING_H
#endif


#include &amp;lt;iostream&amp;gt;
#include &amp;lt;string&amp;gt;
#include &quot;sing.h&quot;
#include &quot;assert.h&quot;
using namespace std;

sing *sing::m_sing = NULL;
sing::sing()
{
    printf(&quot;sing construct\n&quot;);
}

sing::~sing()
{
    printf(&quot;sing destruct\n&quot;);
}

sing *sing::getInstance()
{
    if (m_sing == NULL) {
        printf(&quot;m_sing is NULL\n&quot;);
        m_sing = new sing();
        assert(m_sing != NULL);
    }
    return m_sing;
}

void sing::close()
{
    if (m_sing == NULL) {
        delete m_sing;
        m_sing = NULL;
    }
}

void sing::func()
{
    printf(&quot;sing::func\n&quot;);
}

#include &amp;lt;iostream&amp;gt;
#include &quot;sing.h&quot;

int main()
{
    sing *s = sing::getInstance();
    s-&amp;gt;num = 10;

    sing *s1 = sing::getInstance();
    printf(&quot;%d\n&quot;, s1-&amp;gt;num);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;c++ 的单态实现, 这里getInstance()获得那个单态的指针对象.&lt;/p&gt;

&lt;p&gt;以下是c++ 的泛型的单态的实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#ifndef TSING_H
#define TSING_H
template 
class sing {
private:
    static T *m_sing;

public:
    static void close();
    static T* getInstance();
    sing();
    ~sing();
};

template  T *sing::m_sing = NULL;

template  sing::sing() {
    printf(&quot;sing construct\n&quot;);
}

template  sing::~sing() {
    printf(&quot;sing destructor\n&quot;);
    close();
}

template  T* sing::getInstance() {
    if (m_sing == NULL) {
        printf(&quot;m_sing is NULL\n&quot;);
        m_sing = new T();
    }
    return m_sing;
}

template  void sing::close() {
    if (m_sing != NULL) {
        delete m_sing;
    }
}
#endif
#include &amp;lt;iostream&amp;gt;
#include &quot;tsing.h&quot;

struct node {
    int num;
    bool flag;
    int type;
};

int main()
{
    node *s = sing::getInstance();
    s-&amp;gt;num = 10;
    s-&amp;gt;flag = 0;
    s-&amp;gt;type = 100;

    //s-&amp;gt;num = 10;

    node *s1 = sing::getInstance();
    printf(&quot;%d %d %d\n&quot;, s1-&amp;gt;num, s1-&amp;gt;flag, s1-&amp;gt;type);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在我理解以来,在没有用泛型以前这里的静态指针sing,既是容器也是sing类型对象,也就是说之前这个单态只能设计来给sing这个类来使用,泛型的sing类,这里把sing只是这个容器的名字,T想要单态的类型,任意的类型对象. 而前面这种没有实现泛型是将sing即是这个类的容器也是这个类的对象.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>vim复制插件</title>
   <link href="http://baotiao.github.io//2012/05/vim-copy-plugin/"/>
   <updated>2012-05-03T03:34:33+08:00</updated>
   <id>http://baotiao.github.io//2012/05/vim-copy-plugin</id>
   <content type="html">&lt;p&gt;vim复制的时候不发复制到c-v的复制缓冲区,很麻烦,自己动手,丰衣足食&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nnoremap y :set operatorfunc=Pbcopyg@
vnoremap y y:call Pbcopy()
function! s:Pbcopy()
    execute &#39;call system(&quot;pbcopy&quot;, getreg(&quot;\&quot;&quot;))&#39;
    execute &#39;call system(&quot;xsel --clipboard --input&quot;, getreg(&quot;\&quot;&quot;))&#39;
endfunction
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将代码保存再.vim/plugin/copy.vim的文件即可
ubuntu环境下 默认得安装一下xsel这个东西&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>vim grep plugin</title>
   <link href="http://baotiao.github.io//2012/05/vim-grep/"/>
   <updated>2012-05-03T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2012/05/vim-grep</id>
   <content type="html">
</content>
 </entry>
 
 <entry>
   <title>Ternary Search Trees 三分树</title>
   <link href="http://baotiao.github.io//2012/03/ternary-search-trees/"/>
   <updated>2012-03-26T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2012/03/ternary-search-trees</id>
   <content type="html">&lt;p&gt;经常碰到要存一堆的string, 这个时候可以用hash tables, 虽然hash tables 查找很快,但是hash tables不能表现出字符串之间的联系.可以用binary search tree, 但是查询速度不是很理想. 可以用trie, 不过trie会浪费很多空间(当然你也可以用二个数组实现也比较省空间). 所以这里Ternary Search trees 有trie的查询速度快的优点,以及binary search tree省空间的优点.&lt;/p&gt;

&lt;p&gt;实现一个12个单词的查找&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;data:image/png;base64,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&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这个是用二分查找树实现,n是单词个数,len是长度,复杂度是O(logn * n),空间是n*len&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAATIAAACTCAYAAAAN8ZU6AAAD8GlDQ1BJQ0MgUHJvZmlsZQAAKJGNVd1v21QUP4lvXKQWP6Cxjg4Vi69VU1u5GxqtxgZJk6XpQhq5zdgqpMl1bhpT1za2021Vn/YCbwz4A4CyBx6QeEIaDMT2su0BtElTQRXVJKQ9dNpAaJP2gqpwrq9Tu13GuJGvfznndz7v0TVAx1ea45hJGWDe8l01n5GPn5iWO1YhCc9BJ/RAp6Z7TrpcLgIuxoVH1sNfIcHeNwfa6/9zdVappwMknkJsVz19HvFpgJSpO64PIN5G+fAp30Hc8TziHS4miFhheJbjLMMzHB8POFPqKGKWi6TXtSriJcT9MzH5bAzzHIK1I08t6hq6zHpRdu2aYdJYuk9Q/881bzZa8Xrx6fLmJo/iu4/VXnfH1BB/rmu5ScQvI77m+BkmfxXxvcZcJY14L0DymZp7pML5yTcW61PvIN6JuGr4halQvmjNlCa4bXJ5zj6qhpxrujeKPYMXEd+q00KR5yNAlWZzrF+Ie+uNsdC/MO4tTOZafhbroyXuR3Df08bLiHsQf+ja6gTPWVimZl7l/oUrjl8OcxDWLbNU5D6JRL2gxkDu16fGuC054OMhclsyXTOOFEL+kmMGs4i5kfNuQ62EnBuam8tzP+Q+tSqhz9SuqpZlvR1EfBiOJTSgYMMM7jpYsAEyqJCHDL4dcFFTAwNMlFDUUpQYiadhDmXteeWAw3HEmA2s15k1RmnP4RHuhBybdBOF7MfnICmSQ2SYjIBM3iRvkcMki9IRcnDTthyLz2Ld2fTzPjTQK+Mdg8y5nkZfFO+se9LQr3/09xZr+5GcaSufeAfAww60mAPx+q8u/bAr8rFCLrx7s+vqEkw8qb+p26n11Aruq6m1iJH6PbWGv1VIY25mkNE8PkaQhxfLIF7DZXx80HD/A3l2jLclYs061xNpWCfoB6WHJTjbH0mV35Q/lRXlC+W8cndbl9t2SfhU+Fb4UfhO+F74GWThknBZ+Em4InwjXIyd1ePnY/Psg3pb1TJNu15TMKWMtFt6ScpKL0ivSMXIn9QtDUlj0h7U7N48t3i8eC0GnMC91dX2sTivgloDTgUVeEGHLTizbf5Da9JLhkhh29QOs1luMcScmBXTIIt7xRFxSBxnuJWfuAd1I7jntkyd/pgKaIwVr3MgmDo2q8x6IdB5QH162mcX7ajtnHGN2bov71OU1+U0fqqoXLD0wX5ZM005UHmySz3qLtDqILDvIL+iH6jB9y2x83ok898GOPQX3lk3Itl0A+BrD6D7tUjWh3fis58BXDigN9yF8M5PJH4B8Gr79/F/XRm8m241mw/wvur4BGDj42bzn+Vmc+NL9L8GcMn8F1kAcXjEKMJAAAAItElEQVR4nO3d0baDNBCF4dTl+79yvVCUYmgDmZnMHv7vxqXHJhDCJgktvN7v97sBgLA/Vm8AAMwiyADII8gAyCPIAMgjyADII8gAyCPIAMgjyADII8gAyCPIAMgjyADII8gAyCPIAMgjyADII8gAyCPIAMgjyADII8gAyCPIAMj7c/UGoLbX6zX1eV4pgREEGVx5BNHr9SLg8IEgg6T9SI9QA0GGaVEjpK2eY12EGggy3LIqPHqhuf/345ocwfYML17QixG/AsIj2M5GeltdI/UwWnsGRmToGh3Z/AoVj2nnVt5ISDFaewaCDP+6MnoZGRV5r531Qupbfayt1cXU8sHujFBGp3VWIXa1nCvTzt7n7nwW6zEie5C7U6urJ/nK73ldmXb2Pnf87OjnsRZBVtzMSOPO6CbLl1WvTjvPPrv//JUyEIsgK8ZiNDEzPct4oh9HaVe3kdFafqyRFWA1Yrh7om+f9epK1mVbjrAYreXAiEyQ5ajA4kTMOhI7MzPtHCmr9zf4IsgEeJwgsyfwvhzlE3Z22tkra8NoLQ5BlpTXSWAVYFtZVU7Qu3c7R8o8lmtRNj6xRpaEd0e3DLCtvNW/sYyoszW/RxFtOAXnMSJbJOoK7XEyVhqJfWM57Twre19+728YQ5AFiroKc7W35THt7JW/4fhdR5A5ir7Sek6F9nU89eSyvNt5tZ7e3/Af1sgMrep0EQG21bOyu6yuvyeq7Y/1RdapgBHZpJUdK/IkyhgiGXhPO8/qO9bpXW92BNlFqzvPiuAkxH6Lmnae1bmvN6LubAiyARk6SPQUZl/v006KWZ53O0fq3dcdWf9KrJF1rB517a0KsK3ubN0j4zb9sjpUMvVnL4zIWs4DvTLAtvoztEMFK6adZ/XvtyF6Ozw9NsgyHsws20SI+Vk17extw347en9T8pggy3zAVo++9gixGNF3O39tx2b19txVdo0sc3BtMgVYazohprKdV2XsD3tZtqun1IhM5WqSrcO2VjcclGSYdu4pjdakg0zpitFang6qrnr7ZZl2HmVeW5ObWmY7uKMIMczI3n9WB5tEkK1uJCCL7IG2ikSQAfhEoH2SXiMDnooA+/TH6g2o5DgFzib79l1VbX9wH0EGWYxKcou80BBkAOSlWiOrcncy+1dEKrWz6rZbytjftm2KOkZpgqy3w4od9bjN2fahSjvjb1n72/v9Dt0WppbGjgduO6BZZOjksNELimz9LUqaIPv2uy4A+CbN1LK1/8/1CTMAI9IEWZa5PQA9aaaWAHBX2iBTnVYet5uRJrz0ll+e2t/STC2PB2X/TCalA3O2H4CHzP0t8isYPP0CgLy0U0sAGEWQAZCXJsh6i5ZKXq+X1D5k3rZv9u18/OfTnLVFlvaI3I4Ui/1qC/pnKuxDZlX6iYWztnjqF8mXj8gqHBBOMH/f2lipr1j41d+e1h6tJQiyyp7YoTyMXCi2tq7e3qMXzaf1vaVBVuHKUmEfstqCaXS0+36/y7b31bZo7Tnh3lqSNTJVTCn9zLRt9LOwvM22xWwZCpaNyJ40RK6wD5EsTroqbW4VQFXa48ySILs7RM6k+hVuFct2zdhvrrDuY+rt8Q2L/UEqdyIrHhcH1XUirwtl1X4YHmR3D1CmA1BhHzK5s5B9hdJNAO+2aE033L9hsf8ippS2Itsz+02A6LaIrtNT6IhsttFUrqrfVNgHKytOoqztvypQsrbHVWFBVuHuS5WrVwYr2zLbybu6X2VrjztY7F+gQseZsfrEbS3POlGGtmhNv0+GBFmF28gV9mG1iIXsK1beBMjWFq3lCfc7WOwfkK3DKcrchtE3AbK3RWu5t7HHfUTG92HOVdiHEQonRdSxUGiL1vT6pvQamVpjn6mwDxUoT608KJ1fvHwEgDzpERkAtEaQASiAIAMgjyADAqksnqspEWR0Dniw7lcqX71QVCLIADyb+dcvjlcx7yvQvj6PujzL38o+lnv23z3q9qpnG3149QfvfmbdPt79yKutj2VfKdO7j33UZRlkvaFzxHDaq45juR71rGizqP1qrR/Ss3VFtZl1mZ79tDW/tu6VPfI57z62x9TyRK/hlb7pfCZyv1gPiuP9RNkrVpw7pkHmOZWo6thmLAj/Rj/DkfnTL46dTLmDqY++zlTYr0r9DPNcHuOzfxSIsqonR5X9qtLPMM90atkb7lfrZF5rSasftKd0nJ7Qz3ANi/0neidHhfWrqvuFPFb0MdOp5apFWK8nfEZf6aPCRH0EE9XPop8cW0n0GibPI0uCEwa4j6klAHm8fGSxiJ8jAdURZIsRYMA8ppYA5BFkAOQRZADkEWQA5BFkAOQRZADkEWQA5BFk/4j87WFEXer7o1JmhrpQOMjoSHG8frDtXaZXH3n672ZXnHtlgwzAc5j+RGnk0SoWj1/5Vcb29ztXxpFHj1g9nqRXjvUr4q4ek7vHY7/9Fo/ZOXtTklWZXn1kptyRsq0eYXSlnjuvgOu9RWlme38xC7KRV3RZvMZrpIy7z5G6s70zHbZXzrdnhVmFwbftz/q6NusyvfrIzDPM7p4vs+eQZdv29j/i1XBMLXd+vcLK8jVXEa/L+tVZvF7bVXV9yPM1Z6Nle1xkPB9UGfVqOLMgG1lItWiwqAXblUYC9GlWPX0YGtzWyM5S1+IRuCP1eKkWnEr7E/34ZOhwXSM7c1xw9arHQ7WTR21/ZvoO6gpdI6v6Gi+PfdjaZkVwZzwmVfsObLgFWVQns6znWNbZHalv/49VXREs9+cJPNsr6lhEH/Oo+symlmfrF/uNtlrs/1XP/v+7Uv7IGozVSGB0vcc7VFRGNh6L/V59ZHZ7PMqOrOfYrhH7xevgEmN0BIzhe2QA5PEWpYTu/DQEeDKCLCECDLiGqSUAeQQZAHkEGQB5BBkAeQQZAHkEGQB5BBkAeQQZAHkEGQB5BBkAeQQZAHkEGQB5BBkAeQQZAHkEGQB5BBkAeX8BAI07Ilm244AAAAAASUVORK5CYII=&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这个是用trie实现,复杂度O(n), 空间是 这里是18 * 26(假设只有26个小写字符),随着单词长度的增长等,需要的空间就更多&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;data:image/png;base64,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&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这个是Ternary search tree, 可以看出空间复杂度和binary search tree 一样, 复杂度近似O(n),常数上会比trie差点.&lt;/p&gt;

&lt;h1&gt;介绍&lt;/h1&gt;

&lt;p&gt;Ternary search tree 有binary search tree 省空间和trie 查询快的优点.
Ternary search tree 有三个只节点,在查找的时候,比较当前字符,如果查找的字符比较小,那么就跳到左节点.如果查找的字符比较大,那么就跳转到友节点.如果这个字符正好相等,那么就走向中间节点.这个时候比较下一个字符.
比如上面的例子,要查找&quot;ax&quot;, 先比较&quot;a&quot; 和 &quot;i&quot;, &quot;a&quot; &amp;lt; &quot;i&quot;,跳转到&quot;i&quot;的左节点, 比较 &quot;a&quot; &amp;lt; &quot;b&quot;, 跳转到&quot;b&quot;的左节点, &quot;a&quot; = &quot;a&quot;, 跳转到 &quot;a&quot;的中间节点,并且比较下一个字符&quot;x&quot;. &quot;x&quot; &gt; &quot;s&quot; , 跳转到&quot;s&quot; 的右节点, 比较 &quot;x&quot; &gt; &quot;t&quot; 发现&quot;t&quot; 没有右节点了.找出结果,不存在&quot;ax&quot;这个字符&lt;/p&gt;

&lt;h1&gt;构造方法&lt;/h1&gt;

&lt;p&gt;这里用c语言来实现
节点定义:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;typedef struct tnode *Tptr;
typedef struct tnode {
    char s;
    Tptr lokid, eqkid, hikid;
} Tnode;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先介绍查找的方法:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int search(char *s) // s是想要查找的字符串
{
    Tptr p;
    p = t; //t 是已经构造好的Ternary search tree 的root 节点.
    while (p) {
        if (*s &amp;lt; p-&amp;gt;s) { // 如果*s 比 p-&amp;gt;s 小, 那么节点跳到p-&amp;gt;lokid
            p = p-&amp;gt;lokid;
        } else if (*s &amp;gt; p-&amp;gt;s) {
            p = p-&amp;gt;hikid;
        } else {
            if (*(s) == &#39;\0&#39;) { //当*s 是&#39;\0&#39;时候,则查找成功
                return 1;
            } //如果*s == p-&amp;gt;s,走向中间节点,并且s++
            s++;
            p = p-&amp;gt;eqkid;
        }
    }
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;插入某一个字符串:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Tptr insert(Tptr p, char *s)
{
    if (p == NULL) {
        p = (Tptr)malloc(sizeof(Tnode));
        p-&amp;gt;s = *s;
        p-&amp;gt;lokid = p-&amp;gt;eqkid = p-&amp;gt;hikid = NULL;
    }
    if (*s &amp;lt; p-&amp;gt;s) {
        p-&amp;gt;lokid = insert(p-&amp;gt;lokid, s);
    } else if (*s &amp;gt; p-&amp;gt;s) {
        p-&amp;gt;hikid = insert(p-&amp;gt;hikid, s);
    } else {
        if (*s != &#39;\0&#39;) {
            p-&amp;gt;eqkid = insert(p-&amp;gt;eqkid, ++s);
        } else {
            p-&amp;gt;eqkid = (Tptr) insertstr; //insertstr 是要插入的字符串,方便遍历所有字符串等操作
        }
    }
    return p;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同binary search tree 一样,插入的顺序也是讲究的,binary search tree 在最坏情况下顺序插入字符串会退化成一个链表.不过Ternary search Tree 最坏情况会比 binary search tree 好很多.&lt;/p&gt;

&lt;p&gt;肯定得有一个遍历某一个树的操作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//这里以字典序输出所有的字符串
void traverse(Tptr p) //这里遍历某一个节点以下的所有节点,如果是非根节点,则是有同一个前缀的字符串
{ 
    if (!p) return; 
    traverse(p-&amp;gt;lokid); 
    if (p-&amp;gt;s != &#39;\0&#39;) { 
        traverse(p-&amp;gt;eqkid); 
    } else { 
        printf(&quot;%s\n&quot;, (char *)p-&amp;gt;eqkid); 
    } 
    traverse(p-&amp;gt;hikid); 
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;应用&lt;/h1&gt;

&lt;p&gt;这里先介绍两个应用,一个是模糊查询,一个是找出包含公共前缀的字符串, 一个是相邻查询(哈密顿距离小于某个范围)
模糊查询
psearch(&quot;root&quot;, &quot;.a.a.a&quot;) 应该能匹配出baxaca, cadakd 等字符串&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void psearch1(Tptr p, char *s)
{
    if (p == NULL) {
        return ;
    }
    if (*s == &#39;.&#39; || *s &amp;lt; p-&amp;gt;s) { //如果*s 是&#39;.&#39; 或者 *s &amp;lt; p-&amp;gt;s 就查找左子树
        psearch1(p-&amp;gt;lokid, s); 
    }
    if (*s == &#39;.&#39; || *s &amp;gt; p-&amp;gt;s) { //同上
        psearch1(p-&amp;gt;hikid, s); 
    }
    if (*s == &#39;.&#39; || *s == p-&amp;gt;s) { // *s = &#39;.&#39; 或者 *s == p-&amp;gt;s 则去查找下一个字符
        if (*s &amp;amp;&amp;amp; p-&amp;gt;s &amp;amp;&amp;amp; p-&amp;gt;eqkid != NULL) { 
            psearch1(p-&amp;gt;eqkid, s + 1);
        }
    }
    if (*s == &#39;\0&#39; &amp;amp;&amp;amp; p-&amp;gt;s == &#39;\0&#39;) {
        printf(&quot;%s\n&quot;, (char *) p-&amp;gt;eqkid);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解决在哈密顿距离内的匹配问题,比如hobby和dobbd,hocbe的哈密顿距离都是2&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void nearsearch(Tptr p, char *s, int d) //s 是要查找的字符串, d是哈密顿距离
{
    if (p == NULL || d &amp;lt; 0)
        return ;
    if (d &amp;gt; 0 || *s &amp;lt; p-&amp;gt;s) {
            nearsearch(p-&amp;gt;lokid, s, d);
    }
    if (d &amp;gt; 0 || *s &amp;gt; p-&amp;gt;s) {
            nearsearch(p-&amp;gt;hikid, s, d);
    }
    if (p-&amp;gt;s == &#39;\0&#39;) {
        if ((int)strlen(s) &amp;lt;= d) {
            printf(&quot;%s\n&quot;, (char *) p-&amp;gt;eqkid);
        }
    } else {
        nearsearch(p-&amp;gt;eqkid, *s ? s + 1 : s, (*s == p-&amp;gt;s) ? d : d - 1);
    }   
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;搜索引擎输入bin, 然后相应的找出所有以bin开头的前缀匹配这样类似的结果.比如bing,binha,binb 就是找出所有前缀匹配的结果.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void presearch(Tptr p, char *s) //s 是想要找的前缀
{
    if (p == NULL)
        return;
    if (*s &amp;lt; p-&amp;gt;s) {
        presearch(p-&amp;gt;lokid, s);
    } else if (*s &amp;gt; p-&amp;gt;s) {
        presearch(p-&amp;gt;hikid, s);
    } else {
        if (*(s + 1) == &#39;\0&#39;) {
            traverse(p-&amp;gt;eqkid); // 遍历这个节点,也就是找出包含这个节点的所有字符
            return ;
        } else {
            presearch(p-&amp;gt;eqkid, s + 1);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;总结&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Ternary search tree 效率高而且容易实现&lt;/li&gt;
&lt;li&gt;Ternary search tree 大体上效率比hash来的快,因为当数据量大的时候hash出现碰撞的几率也会大,而Ternary search tree 是指数增长&lt;/li&gt;
&lt;li&gt;Ternary search tree 增长和收缩很方便,而 hash改变大小的话则需要拷贝内存重新hash等操作&lt;/li&gt;
&lt;li&gt;Ternary search tree 支持模糊匹配,哈密顿距离查找,前缀查找等操作&lt;/li&gt;
&lt;li&gt;Ternary search tree 支持许多其他操作,比如字典序输出所有字符串等,trie也能做,不过很费时.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;strong&gt;参考&lt;/strong&gt;:http://drdobbs.com/database/184410528?pgno=1&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Memcache initialization</title>
   <link href="http://baotiao.github.io//2012/03/memcached-initialization/"/>
   <updated>2012-03-07T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2012/03/memcached-initialization</id>
   <content type="html">&lt;pre&gt;&lt;code&gt;1.signal(SIGINT, sig_handler);

2.seetings_init(); //初始化默认的设置,初始化全局变量struct setting settings
static void settings_init(void) {
    settings.use_cas = true;
    settings.access = 0700;
    settings.port = 11211;
    settings.udpport = 11211;
    /* By default this string should be NULL for getaddrinfo() */
    settings.inter = NULL;
    settings.maxbytes = 64 * 1024 * 1024; /* default is 64MB */
    settings.maxconns = 1024;         /* to limit connections-related memory to about 5MB */
    settings.verbose = 0;
    settings.oldest_live = 0;
    settings.evict_to_free = 1;       /* push old items out of cache when memory runs out */
    settings.socketpath = NULL;       /* by default, not using a unix socket */
    settings.factor = 1.25;
    settings.chunk_size = 48;         /* space for a modest key and value */
    settings.num_threads = 4;         /* N workers */
    settings.num_threads_per_udp = 0;
    settings.prefix_delimiter = &#39;:&#39;;
    settings.detail_enabled = 0;
    settings.reqs_per_event = 20;
    settings.backlog = 1024;
    settings.binding_protocol = negotiating_prot;
    settings.item_size_max = 1024 * 1024; /* The famous 1MB upper limit. */
    settings.maxconns_fast = false;
    settings.hashpower_init = 0;
}

3. while (-1 != (c = getopt(argc, argv, //对输入的参数进行相应的设置,这种demon,item_max_size 等等

4.getrlimit,setrlimit 是系统调用,用来修改最大连接数 struct rlimit .

5.去掉root权限. 如果是root 通过 -u 参数来启动

6.init_sasl() 是否开启 sasl

7.daemonize(maxcore, setting.verbose) 开启 daemonize 模式

8.main_base = event_init(); 初始化主线程

9.stats_init() 初始化全局变量stats.

10.assoc_init() 初始化hashtable, 默认calloc(2^16, sizeof(void *))个指针

11.conn_init() 初始化连接池conn, 默认的空闲连接池的个数是200 声明calloc(200, sizeof(conn *))

12.slabs_init(settings.maxbytes, setting.factor, preallocate) 初始化memcached. setting.maxbytes 是最大内存的大小,setting.factor 成长因子, preallocate是否预先声明空间 . 默认的最小的chunk_size 是48,成长因子是1.25.

13.sigignore(SIGPIPE) 忽略SIGPIPE信号。 在Linux下写socket的程序的时候，如果尝试send到一个disconnected socket上，底层将抛出一个SIGPIPE信号。如果程序没有处理或没有忽略，这个信号将导致程序退出。这不是我们需要的.

14.thread_init(settings.num_threads, main_base); //初始化线程池模型

15.start_assoc_maintenance_thread() //在assoc里面,有一个独立的线程来维护从old_hashtable中的数据到相应的primary_hashtable中.开启这个线程

16.clock_handler(0, 0, 0); //初始化libevent的时间处理

17.server_socket_unix(settings.socketpath,settings.access) //建立tcp 或者 udp的socket连接sfd,dispatch_thread开始监听这个sfd连接.
server_sockets(settings.port, tcp_transport,portnumber_file) //可以让dispatch同时监听多个连接的端口. 到此 服务器初始化结束,等到client的连接

18.if (event_base_loop(main_base, 0) != 0) //循环main_base事件

19.stop_assoc_maintenance_thread(); //退出在assoc里面的维护hashtable线程
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Memcache stats mt 线上使用情况</title>
   <link href="http://baotiao.github.io//2012/03/memcached-meituan/"/>
   <updated>2012-03-05T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2012/03/memcached-meituan</id>
   <content type="html">&lt;p&gt;美团网memcached的使用情况,线上有3个memcached服务器,默认的内存空间是256M.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; stats &lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cmd_get累计的get命令数量 33408439710, cmd_set累计的set命令数量是 4037110852
get_hits 和 get_misses 分别是 32901704378 和 506735332 我们的命中率大概是98.5%
incr_misses 和 incr_hists 都是0 我们美团并没有用到incr这部分功能
decr_misses 和 decr_hists 也都是0 .
cas_misses 和 cas_hits 也都是0
bytes_read 8160828819650 bytes_written 82925274065843 是这个server 读到的数据的数量和这个服务器写的数据的数量
limit_maxbytes 268435456 = 256M 这个服务器开的内存的大小
threads 4 当前线程数
curr_items 504204  //当前使用的item的数量
total_items 422927023 //总共使用过的item的数量
bytes 190848020 //用来存目前的item所使用的内存那么空闲的内存就是 268435456 - 190848020 =77587436 = 28%的空闲内存
evictions 1494520 // 根据LRU算法淘汰的item的数量 可以看出evictions的数量远小于total_items.说明服务器的内存空间足够使用,很少有通过LRU重新使用item.
reclaimed 177784574 // 从item 上面的slots上重新使用items的数量. 这个数量将近total_items的一半.说明几乎又一半的item是存在重新使用的items里. 这里可以看出memcached的slabclass的不将空闲的item返回内存池,而是放在空闲链表是非常有用的
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;stats setting 查看默认的配置信息&lt;/p&gt;

&lt;p&gt;stats items 和 stats slabs
stats slabs 返回具体的每一个slabclass的信息
stats items 返回格式 items:: \r\n&lt;/p&gt;

&lt;p&gt;stats sizes 查看所有的items的大小和个数&lt;/p&gt;

&lt;p&gt;看一个stats slabs 的结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;STAT 17:chunk_size 3632 //一个chunk的大小
STAT 17:chunks_per_page 288 
STAT 17:total_pages 28
STAT 17:total_chunks 8064 //总的chunks数 = chunks_per_page * total_pages
STAT 17:used_chunks 4234 // 已经使用的chunks数
STAT 17:free_chunks 3830 // 空闲的chunks数
STAT 17:free_chunks_end 0 // 最后一次声明的那个pages现在有的空闲的chunk数. 发现这里为0.说明有大量的items 被使用,然后被放入到的slabclass的slots数组里面了. slots数组里面又将近一半的items.这也说明了memcached将空闲的item放入到slots链表,而不是返回给内存池是多么的有用
STAT 17:mem_requested 50004272
STAT 17:get_hits 5876286969
STAT 17:cmd_set 602661036
STAT 17:delete_hits 0
STAT 17:incr_hits 0
STAT 17:decr_hits 0
STAT 17:cas_hits 0
STAT 17:cas_badval 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这只是slabs中slabclass[17]的结构.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;STAT 1:chunk_size 96 STAT 1:total_pages 1
STAT 2:chunk_size 120 STAT 2:total_pages 3
STAT 3:chunk_size 152 STAT 3:total_pages 1
STAT 4:chunk_size 192 STAT 4:total_pages 42
STAT 5:chunk_size 240 STAT 5:total_pages 13
STAT 6:chunk_size 304 STAT 6:total_pages 84
STAT 7:chunk_size 384 STAT 7:total_pages 2
STAT 8:chunk_size 480 STAT 8:total_pages 2
STAT 9:chunk_size 600 STAT 9:total_pages 3
STAT 10:chunk_size 752 STAT 10:total_pages 4
STAT 11:chunk_size 944 STAT 11:total_pages 6
STAT 12:chunk_size 1184 STAT 12:total_pages 3
STAT 13:chunk_size 1480 STAT 13:total_pages 1
STAT 14:chunk_size 1856 STAT 14:total_pages 1
STAT 15:chunk_size 2320 STAT 15:total_pages 3
STAT 16:chunk_size 2904 STAT 16:total_pages 20
STAT 17:chunk_size 3632 STAT 17:total_pages 28
STAT 18:chunk_size 4544 STAT 18:total_pages 19
STAT 19:chunk_size 5680 STAT 19:total_pages 7
STAT 20:chunk_size 7104 STAT 20:total_pages 3
STAT 21:chunk_size 8880 STAT 21:total_pages 2
STAT 22:chunk_size 11104 STAT 22:total_pages 1
STAT 23:chunk_size 13880 STAT 23:total_pages 1
STAT 24:chunk_size 17352 STAT 24:total_pages 1
STAT 25:chunk_size 21696 STAT 25:total_pages 1
STAT 26:chunk_size 27120 STAT 26:total_pages 1
STAT 27:chunk_size 33904 STAT 27:total_pages 1
STAT 28:chunk_size 42384 STAT 28:total_pages 1
STAT 29:chunk_size 52984 STAT 29:total_pages 1
STAT 31:chunk_size 82792 STAT 31:total_pages 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;摘取的数据, 说明大部分的items的大小是304.(这跟我们框架将每一个对象都缓存,一个对象的大小差不多就是304)导致&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Memcache 内存分配</title>
   <link href="http://baotiao.github.io//2012/02/memcache-slabs/"/>
   <updated>2012-02-18T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2012/02/memcache-slabs</id>
   <content type="html">&lt;p&gt;内存分配主要在slab.c里面实现.
slabclass 的数据结构&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;typedef struct {
    unsigned int size;      /* sizes of items */  每一个chunk的大小
    unsigned int perslab;   /* how many items per slab */ 每一个slab包含的chunk的数目

    void **slots;           /* list of item ptrs */ 是当前slab class的空闲的chunk块指针数组    
    unsigned int sl_total;  /* size of previous array */ 已分配的slots数据的大小
    unsigned int sl_curr;   /* first free slot */ 当前可用的slots数组的索引

    void *end_page_ptr;         /* pointer to next free item at end of page, or 0 */ //这个指针指向最新的那个page可用的chunk处
    unsigned int end_page_free; /* number of items remaining at end of last alloced page */ //这个是新的那个page里面可用的chunk的个数,如果*end_page_prt != NUll 并且 end_page_free == 0 说明最后一个page也被填满数据了.

    unsigned int slabs;     /* how many slabs were allocated for this class */ //这个是当前已使用的slab_list的数量

    void **slab_list;       /* array of slab pointers */ //这个是所有的page的list, 每新添加一个page.
    unsigned int list_size; /* size of prev array */ //slab_list指针数组已分配的大小

    unsigned int killing;  /* index+1 of dying slab, or zero if none */
    size_t requested; /* The number of requested bytes */
} slabclass_t;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;slabs 模型. (网上找的,我觉得表示最好的一张图,结合图看下面的函数分析)
&lt;img src=&quot;./pic/memcache-slabs.gif&quot; alt=&quot;slabs&quot; /&gt;
&lt;a href=&quot;http://chenzongzhi.info/wp-content/uploads/2012/02/slabs.jpeg&quot;&gt;&lt;img src=&quot;http://chenzongzhi.info/wp-content/uploads/2012/02/slabs.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;﻿&lt;/p&gt;

&lt;p&gt;几个重要变量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static slabclass_t slabclass[MAX_NUMBER_OF_SLAB_CLASSES];
static size_t mem_limit = 0; //内存限制的大小
static size_t mem_malloced = 0;
static int power_largest;

static void *mem_base = NULL; //分配的整块大内存的位置
static void *mem_current = NULL; //目前可用的内存的位置
static size_t mem_avail = 0; //剩余可用的内存的大小
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;函数介绍:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/*
     就是根据传进来的size.找到第一个slab class的size比他大的slab class.返回slab class的id.
*/
unsigned int slabs_clsid(const size_t size) {
    int res = POWER_SMALLEST;

    if (size == 0)
        return 0;
    while (size &amp;gt; slabclass[res].size)
        if (res++ == power_largest)     /* won&#39;t fit in the biggest slab */
            return 0;
    return res;
}

/*
     slabs初始化函数,初始化了生成一个链表slabclass.
     slabclass 就是size大小不同的,然后size * perslab = 1M的一个链表,然后在这个链表里面每添加一个slab,这个slab也叫page.也就是不断往这个链表后面添加page.然后每个page里面的有perslab 数的chunk,然后真正的数据是叫item,item存在正好比item大一点的chunk上.
     以后每次添加新的page,都是先找到相应大小的那个类型的slab,然后插入在slabclass这个链表的slab_list的后面
*/

void slabs_init(const size_t limit, const double factor, const bool prealloc) {
    int i = POWER_SMALLEST - 1;
    unsigned int size = sizeof(item) + settings.chunk_size; //初始化最小的slabclass里面的size
    mem_limit = limit;

    if (prealloc) { //选择了prealloc就是预先分配一个大内存 然后初始化mem_base,mem_current,mem_avail.
        /* Allocate everything in a big chunk with malloc */
        mem_base = malloc(mem_limit); //分配mem_limit大小的内存
        if (mem_base != NULL) {
            mem_current = mem_base;
            mem_avail = mem_limit;
        } else {
            fprintf(stderr, &quot;Warning: Failed to allocate requested memory in&quot;
                    &quot; one large chunk.\nWill allocate in smaller chunks\n&quot;);
        }
    }

    memset(slabclass, 0, sizeof(slabclass));

    /*
        这里是初始化slabclass链表的过程,根据最小的size.然后size*factor 逐渐加上去
        直到size = 1M 为止
    */
    while (++i &amp;lt; POWER_LARGEST &amp;amp;&amp;amp; size &amp;lt;= settings.item_size_max / factor) {

    省略..
             if (pre_alloc == NULL || atoi(pre_alloc) != 0) {
            slabs_preallocate(power_largest);
          //这个函数是给没一个slabclass都预先分配一个slabs,也就是1M的大小.
          //所以每一个slabclass都会有一个1M大小的空间.不过这样有存在内存浪费,因为有可能有些
          //slabs不会被用到
        }
}

/*
     预先为所有的slabclass分配空间
*/
static void slabs_preallocate (const unsigned int maxslabs) {
    int i;
    unsigned int prealloc = 0;

    /* pre-allocate a 1MB slab in every size class so people don&#39;t get
       confused by non-intuitive &quot;SERVER_ERROR out of memory&quot;
       messages.  this is the most common question on the mailing
       list.  if you really don&#39;t want this, you can rebuild without
       these three lines.  */

    for (i = POWER_SMALLEST; i &amp;lt;= POWER_LARGEST; i++) {
        if (++prealloc &amp;gt; maxslabs)
            return;
        do_slabs_newslab(i);
    }
}

/*
    调整slabclass的slab_list指针数组,如果为空,这默认分配16个,如果存在,那么*2的增加slab_list数组*/
static int grow_slab_list (const unsigned int id) {
    slabclass_t *p = &amp;amp;slabclass;[id];
    if (p-&amp;gt;slabs == p-&amp;gt;list_size) {  //当slabclass里面的slabs(当前已使用的slab_list的数量)和list_size(已分配的slab_list的数量)相等时,
                                    //表示这个slab_list的已经用完了,需要生成新的slab_list
        size_t new_size =  (p-&amp;gt;list_size != 0) ? p-&amp;gt;list_size * 2 : 16; //如果list_size是空,自&amp;gt;动生成16个,否则就是*2的增长
        void *new_list = realloc(p-&amp;gt;slab_list, new_size * sizeof(void *)); //把这个list realloc到新的空间去
        if (new_list == 0) return 0;
        p-&amp;gt;list_size = new_size;
        p-&amp;gt;slab_list = new_list;
    }
    return 1;
}

/*
    为某一个slabclass分配一个新的slab.真正执行分配一个slab的是memory_allocate函数
    其实memori_alocate函数做的就是从mem_current找到len大小的空间,并返回mem_current的地址
*/
static int do_slabs_newslab(const unsigned int id) {
    slabclass_t *p = &amp;amp;slabclass;[id];
    int len = p-&amp;gt;size * p-&amp;gt;perslab;
    char *ptr;

    /*
        这里判断slab_list这个指针数据大小够不够,不够分配新的空间.
        memory_allocate分配新的大小的空间
    */
    if ((mem_limit &amp;amp;&amp;amp; mem_malloced + len &amp;gt; mem_limit &amp;amp;&amp;amp; p-&amp;gt;slabs &amp;gt; 0) ||
        (grow_slab_list(id) == 0) ||
        ((ptr = memory_allocate((size_t)len)) == 0)) { //在这里声明大小为 1M的page.

        MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id);
        return 0;
    }     

    memset(ptr, 0, (size_t)len);
    p-&amp;gt;end_page_ptr = ptr; //将end_page_ptr 也就是表示最新的可用的chunk的指针指向ptr
    p-&amp;gt;end_page_free = p-&amp;gt;perslab; //end_page_free 因为这里是一个全新的page 所以 end_page_free = p-&amp;gt;perslabs
    p-&amp;gt;slab_list[p-&amp;gt;slabs++] = ptr; //将ptr 加入道p-&amp;gt;slab_list里面去,并且 page 的个数++
    mem_malloced += len;
    MEMCACHED_SLABS_SLABCLASS_ALLOCATE(id);

    return 1;
}

/*
    这个是进行给slab分配内存的函数,会判断是否有足够的空间,已经然后将目前的mem_current返回,然后&amp;gt;调整mem_current的值
*/
static void *memory_allocate(size_t size) {
    void *ret;

    if (mem_base == NULL) {
        /* We are not using a preallocated large memory chunk */
        ret = malloc(size);
    } else {
        ret = mem_current;

        if (size &amp;gt; mem_avail) {
            return NULL;
        }

        /* mem_current pointer _must_ be aligned!!! */
        if (size % CHUNK_ALIGN_BYTES) {
            size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);
        }

        mem_current = ((char*)mem_current) + size;

        if (size &amp;lt; mem_avail) {
            mem_avail -= size;
        } else {
            mem_avail = 0;
        }
    }

    return ret;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到此处理slabs已经初始化结束了.memcached还提供了一个空闲item链表,叫slots.就是当一个slabs里面的item已经过期了,那么这个chunk不是退回给内存池,而是放入到这个空闲链表里面去,同样当申请一个新元素的时候也是先判断这个slots数组是否有空间,没有空间再去slabs里面去获得.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void *do_slabs_alloc(const size_t size, unsigned int id) {
     ..省略..
/*
        这里判断end_page_ptr是否指向可用的chunk, 当前slabclass的slots的空闲chunk个数
        以及是否可以新分配一块slab
        如果可以获得空间的话,先从slabclass的slots里面获得,然后才是从slab里面的end_page_ptr处去&amp;gt;获得
    */
    if (! (p-&amp;gt;end_page_ptr != 0 || p-&amp;gt;sl_curr != 0 ||
           do_slabs_newslab(id) != 0)) {
        /* We don&#39;t have more memory available */
        ret = NULL;
    } else if (p-&amp;gt;sl_curr != 0) {
        /* return off our freelist */
        ret = p-&amp;gt;slots[--p-&amp;gt;sl_curr];
    } else {
        /* if we recently allocated a whole page, return from that */
        assert(p-&amp;gt;end_page_ptr != NULL);
        ret = p-&amp;gt;end_page_ptr;
        if (--p-&amp;gt;end_page_free != 0) {
            p-&amp;gt;end_page_ptr = ((caddr_t)p-&amp;gt;end_page_ptr) + p-&amp;gt;size;
        } else {
            p-&amp;gt;end_page_ptr = 0;
        }
    }

    if (ret) {
        p-&amp;gt;requested += size;
        MEMCACHED_SLABS_ALLOCATE(size, id, p-&amp;gt;size, ret);
    } else {
        MEMCACHED_SLABS_ALLOCATE_FAILED(size, id);
    }

    return ret;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数主要是将slab里面的chunk设置成free. 设置free不是把他归还给内存池
而是把这个item放入到这个slabclass的slots中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void do_slabs_free(void *ptr, const size_t size, unsigned int id) {
    slabclass_t *p;

    assert(((item *)ptr)-&amp;gt;slabs_clsid == 0);
    assert(id &amp;gt;= POWER_SMALLEST &amp;amp;&amp;amp; id &amp;lt;= power_largest);
    if (id &amp;lt; POWER_SMALLEST || id &amp;gt; power_largest)
        return;

    MEMCACHED_SLABS_FREE(size, id, ptr);
    p = &amp;amp;slabclass;[id];

    /*
        刚开始看这里的时候不明白,为什么slots数组需要malloc空间,
        因为slots是指针数组,这里malloc分配的内存空间是给这个指针数组的.
        然后这个数组每一个都指向在slab里面被free的chunk.
    */
    if (p-&amp;gt;sl_curr == p-&amp;gt;sl_total) { /* need more space on the free list */
        int new_size = (p-&amp;gt;sl_total != 0) ? p-&amp;gt;sl_total * 2 : 16;  /* 16 is arbitrary */
        void **new_slots = realloc(p-&amp;gt;slots, new_size * sizeof(void *));
        //这里是给slots数组malloc了这么多数量的void指针.
        if (new_slots == 0)
            return;
        p-&amp;gt;slots = new_slots;
        p-&amp;gt;sl_total = new_size;
    }
    p-&amp;gt;slots[p-&amp;gt;sl_curr++] = ptr; //这里把sl_curr指向了这个ptr,这个ptr就是一个slab里面的一个空&amp;gt;闲的chunk处.所以这里空闲数组又获得了一个空间,这里sl_curr可用的slots数+1.
    p-&amp;gt;requested -= size;
    return;
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>event + 线程池模型的 server 类似 Memcache线程池模型</title>
   <link href="http://baotiao.github.io//2012/02/event-like-memcache/"/>
   <updated>2012-02-04T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2012/02/event-like-memcache</id>
   <content type="html">&lt;pre&gt;&lt;code&gt;/*
    main.h
    定义了三个数据结构 conn_queue_item,work_thread,dispatch_thread.
    conn_queue_item 只是存dispatch_thread accept 以后的描述符,然后
    dispatch_thread 将conn_queue_item 存入某一个work_thread.
    work_thread 真正负责work的thread.
    dispatch_thread 监听9877端口,并且将accept后的fd传给work_thread.
*/
#ifndef MAINH
#define MAINH

#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;event.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;netinet/in.h&amp;gt;
#include &amp;lt;sys/socket.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;pthread.h&amp;gt;
#include &amp;lt;errno.h&amp;gt;

typedef struct conn_queue_item CQ;

struct conn_queue_item {
    int sfd;
};

struct WORK_THREAD {
    pthread_t thread_id;
    struct event_base *base;
    struct event notify_event;
    int notify_receive_fd;
    int notify_send_fd;
    struct conn_queue_item cq;
};
typedef struct WORK_THREAD wk_thread;

struct DISPATCH_THREAD {
    pthread_t thread_id;
    struct event_base *base;
};
typedef struct DISPATCH_THREAD dh_thread;

#endif

主要的执行函数main.c
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;event.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;netinet/in.h&amp;gt;
#include &amp;lt;sys/socket.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;pthread.h&amp;gt;
#include &amp;lt;errno.h&amp;gt;
#include &quot;main.h&quot;

static struct event_base *main_base;

void call_accept(int fd, short event, void *arg)
{
    fputs(&quot;a socket has come\n&quot;, stdout);
    struct sockaddr_in cliaddr;
    socklen_t clilen;
    int connfd;
    connfd = accept(fd, (struct sockaddr *) &amp;amp;cliaddr, &amp;amp;clilen);
    dispatch_new_thread(connfd);
}



int main()
{

    int listenfd, connfd;
    struct sockaddr_in cliaddr, servaddr;
    socklen_t clilen;

    listenfd = socket(AF_INET, SOCK_STREAM, 0);
    memset(&amp;amp;servaddr, 0, sizeof(servaddr));
    servaddr.sin_family = AF_INET;
    servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
    servaddr.sin_port = htons(9877);
    bind(listenfd, (struct sockaddr *) &amp;amp;servaddr, sizeof(servaddr));
    listen(listenfd, 10);

    struct event_base *mb;
    struct event ev;
    mb = event_init();
    event_set(&amp;amp;ev, listenfd, EV_READ | EV_PERSIST, call_accept, &amp;amp;ev);
    //event_base_set(main_base, &amp;amp;ev);
    event_add(&amp;amp;ev, NULL);
    printf(&quot;add the event\n&quot;);

    thread_init(10, mb);

    printf(&quot;block before accept\n&quot;);
    event_base_loop(mb, 0);

    return 0;
}

线程池模型. 每一个work_thread监听自己的notify_receive_fd READ 事件,然后dispatch_thread 往notify_receive_fd 写入一字节的数据.接着 work_thread
就处理从dispatch_thread 传送过来的fd 的请求
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;event.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;netinet/in.h&amp;gt;
#include &amp;lt;sys/socket.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;pthread.h&amp;gt;
#include &amp;lt;errno.h&amp;gt;
#include &quot;main.h&quot;


static dh_thread dispatch_thread;

static wk_thread *threads;

static int last_thread = -1;

void dispatch_new_thread(int fd)
{
    int tid = (last_thread + 1) % 10;
    wk_thread *thread = threads + tid;

    thread-&amp;gt;cq.sfd = fd;
    write(thread-&amp;gt;notify_send_fd, &quot;&quot;, 1);
}


void thread_libevent_process(int fd, short which, void *arg)
{
    wk_thread *work_thread = arg;
    char unuse[1];
    if (read(fd, unuse, 1) != 1) {
        fprintf(stderr, &quot;Can&#39;t read from libevent\n&quot;);
    }
    char buf[100];
    int n;
    n = read(work_thread-&amp;gt;cq.sfd, buf, 100);
    write(work_thread-&amp;gt;cq.sfd, buf, n);
}

void setup_thread(wk_thread *work_thread)
{   
    work_thread-&amp;gt;base = event_init();
    if (!work_thread-&amp;gt;base) {
        fprintf(stdout, &quot;Can&#39;t allocate event base\n&quot;);
        exit(1);
    }

    event_set(&amp;amp;work_thread-&amp;gt;notify_event, work_thread-&amp;gt;notify_receive_fd, EV_READ | EV_PERSIST, thread_libevent_process, work_thread);
    event_base_set(work_thread-&amp;gt;base, &amp;amp;work_thread-&amp;gt;notify_event);
    if (event_add(&amp;amp;work_thread-&amp;gt;notify_event, 0) == -1) {
        fprintf(stdout, &quot;Can&#39;t add libevent notify pipe\n&quot;);
        exit(1);
    }
}

void worker_libevent(void *arg)
{
    wk_thread *work_thread = arg;
    event_base_loop(work_thread-&amp;gt;base, 0);
}

void create_worker(void *(*func)(void *), void *arg)
{
    pthread_t thread;
    pthread_attr_t attr;
    int ret;
    pthread_attr_init(&amp;amp;attr);

    if ((ret = pthread_create(&amp;amp;thread, &amp;amp;attr, func, arg)) != 0) {
        fprintf(stdout, &quot;Can&#39;t create thread: %s\n&quot;, strerror(ret));
        exit(1);
    }
}


void thread_init(int t_num, struct event_base *main_base)
{
    dispatch_thread.base = main_base;
    dispatch_thread.thread_id = pthread_self();
    int i;
    threads = calloc(t_num, sizeof(wk_thread));
    if (!threads) {
        perror(&quot;Can&#39;t alloc so many thread\n&quot;);
        exit(1);
    }

    for (i = 0; i &amp;lt; t_num; i++) {
        int fds[2];
        if (pipe(fds)) {
            perror(&quot;can&#39;t pipe\n&quot;);
            exit(1);
        }
        threads[i].notify_receive_fd = fds[0];
        threads[i].notify_send_fd = fds[1];

        setup_thread(&amp;amp;threads[i]);
    }

    for (i = 0; i &amp;lt; t_num; i++) {
        create_worker(worker_libevent, &amp;amp;threads[i]);
    }

}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Memcache threads analysis</title>
   <link href="http://baotiao.github.io//2012/02/memcached-thread-model/"/>
   <updated>2012-02-01T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2012/02/memcached-thread-model</id>
   <content type="html">&lt;p&gt;memcached 启动时线程处理流程&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://175.41.172.193/wp-content/uploads/2012/02/thread1.jpg&quot;&gt;&lt;img src=&quot;http://175.41.172.193/wp-content/uploads/2012/02/thread1-113x300.jpg&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;memcached 是利用libevent实现了一个线程池.有一个dispatch_thread 和 n 个work_thread构成.&lt;/p&gt;

&lt;p&gt;主要的数据结构以及简单操作
在thread.c里面有&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/* An item in the connection queue. */
typedef struct conn_queue_item CQ_ITEM;
struct conn_queue_item {
    int               sfd;  //对应每个connection的 fd
    enum conn_states  init_state;
    int               event_flags;
    int               read_buffer_size;
    enum network_transport     transport;
    CQ_ITEM          *next;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个CQ_ITEM 是对每一个connection的描述.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/* A connection queue. */
typedef struct conn_queue CQ;
struct conn_queue {
    CQ_ITEM *head;
    CQ_ITEM *tail;
    pthread_mutex_t lock;
    pthread_cond_t  cond;

};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个就是connection_queue 每个工作线程都有一个connection queue. CQ中的每个CQ_ITEM都是对一个socket连接的简单描述.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/* * Looks for an item on a connection queue, but doesn&#39;t block if there isn&#39;t
 * one.
 * Returns the item, or NULL if no item is available
 */
static CQ_ITEM *cq_pop(CQ *cq) {
    CQ_ITEM *item;

    pthread_mutex_lock(&amp;amp;cq-;&amp;gt;lock);
    item = cq-&amp;gt;head;
    if (NULL != item) {
        cq-&amp;gt;head = item-&amp;gt;next;
        if (NULL == cq-&amp;gt;head)
            cq-&amp;gt;tail = NULL;
    }
    pthread_mutex_unlock(&amp;amp;cq-;&amp;gt;lock);

    return item;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是对cq里面的item的pop操作. 可以看出cq只是指向了item,然后通过item之间的链表连接起来.从而获得每个工作线程需要处理的item.&lt;/p&gt;

&lt;p&gt;有1个空闲的CQ_ITEM链表&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static CQ_ITEM *cqi_freelist; //这是空闲的cqitem 链表
static pthread_mutex_t cqi_freelist_lock; //这个是链表的锁.用于对链表添加元素时加锁

/* * Returns a fresh connection queue item.
 */
static CQ_ITEM *cqi_new(void) {
    CQ_ITEM *item = NULL;
    pthread_mutex_lock(&amp;amp;cqi;_freelist_lock);
    if (cqi_freelist) {
        item = cqi_freelist;
        cqi_freelist = item-&amp;gt;next;
    }
    pthread_mutex_unlock(&amp;amp;cqi;_freelist_lock);

    if (NULL == item) {
        int i;

        /* Allocate a bunch of items at once to reduce fragmentation */
        item = malloc(sizeof(CQ_ITEM) * ITEMS_PER_ALLOC);
        if (NULL == item)
            return NULL;
/*其中 malloc的时候是直接malloc(sizeof(CQ_ITEM) * ITEMS_PER_ALLOC) ITEMS_PER_ALLOC个的CQ_ITEM 减少了每个item之间的碎片的产生.*/

        /*
         * Link together all the new items except the first one
         * (which we&#39;ll return to the caller) for placement on
         * the freelist.
         */
        for (i = 2; i &amp;lt; ITEMS_PER_ALLOC; i++)
            item[i - 1].next = &amp;amp;item;[i];

        pthread_mutex_lock(&amp;amp;cqi;_freelist_lock);
        item[ITEMS_PER_ALLOC - 1].next = cqi_freelist;
        cqi_freelist = &amp;amp;item;[1];
        pthread_mutex_unlock(&amp;amp;cqi;_freelist_lock);
    }

    return item;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是new 一个cq_item的操作, 其中 malloc的时候是直接malloc(sizeof(CQ_ITEM) * ITEMS_PER_ALLOC) ITEMS_PER_ALLOC个的CQ_ITEM 减少了每个item之间的碎片的产生.然后是将malloc出来的这么多的cq_item链接起来.然后再将其加入到 cqi_freelist中.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/*
 * Frees a connection queue item (adds it to the freelist.)
 */
static void cqi_free(CQ_ITEM *item) {
    pthread_mutex_lock(&amp;amp;cqi;_freelist_lock);
    item-&amp;gt;next = cqi_freelist;
    cqi_freelist = item;
    pthread_mutex_unlock(&amp;amp;cqi;_freelist_lock);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Free 一个cq_item 的时候是将其直接放入到cqi_freelist里面去.&lt;/p&gt;

&lt;p&gt;每一个work_thread在初始化的时候都有一个cq&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;typedef struct conn conn;struct conn {
    int    sfd;
    sasl_conn_t *sasl_conn;
    enum conn_states  state;
    enum bin_substates substate;
    struct event event;
    short  ev_flags;
    short  which;   /** which events were just triggered */

    char   *rbuf;   /** buffer to read commands into */
    char   *rcurr;  /** but if we parsed some already, this is where we stopped */
    int    rsize;   /** total allocated size of rbuf */
    int    rbytes;  /** how much data, starting from rcur, do we have unparsed */

    char   *wbuf;
    char   *wcurr;    int    wsize;
    int    wbytes;
    /** which state to go into after finishing current write */
    enum conn_states  write_and_go;
    void   *write_and_free; /** free this memory after finishing writing */

    char   *ritem;  /** when we read in an item&#39;s value, it goes here */
    int    rlbytes;

    /* data for the nread state */

    /**
     * item is used to hold an item structure created after reading the command
     * line of set/add/replace commands, but before we finished reading the actual
     * data. The data is read into ITEM_data(item) to avoid extra copying.
     */

    void   *item;     /* for commands set/add/replace  */

    /* data for the swallow state */
    int    sbytes;    /* how many bytes to swallow */

    /* data for the mwrite state */
    struct iovec *iov;
    int    iovsize;   /* number of elements allocated in iov[] */
    int    iovused;   /* number of elements used in iov[] */

    struct msghdr *msglist;
    int    msgsize;   /* number of elements allocated in msglist[] */
    int    msgused;   /* number of elements used in msglist[] */
    int    msgcurr;   /* element in msglist[] being transmitted now */
    int    msgbytes;  /* number of bytes in current msg */

    item   **ilist;   /* list of items to write out */
    int    isize;
    item   **icurr;
    int    ileft;

    char   **suffixlist;
    int    suffixsize;
    char   **suffixcurr;
    int    suffixleft;

    enum protocol protocol;   /* which protocol this connection speaks */
    enum network_transport transport; /* what transport is used by this connection */

    /* data for UDP clients */
    int    request_id; /* Incoming UDP request ID, if this is a UDP &quot;connection&quot; */
    struct sockaddr request_addr; /* Who sent the most recent request */
    socklen_t request_addr_size;
    unsigned char *hdrbuf; /* udp packet headers */
    int    hdrsize;   /* number of headers&#39; worth of space is allocated */

    bool   noreply;   /* True if the reply should not be sent. */
    /* current stats command */
    struct {
        char *buffer;
        size_t size;
        size_t offset;

    } stats;
    /* Binary protocol stuff */
    /* This is where the binary header goes */
    protocol_binary_request_header binary_header;
    uint64_t cas; /* the cas to return */
    short cmd; /* current command being processed */
    int opaque;
    int keylen;
    conn   *next;     /* Used for generating a list of conn structures */
    LIBEVENT_THREAD *thread; /* Pointer to the thread object serving this connection */
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;memcached 的conn 代表一个到memcached的链接.
里面的item是连接成功后,读入 set/add/replace 生成的cq_item.
conn-&gt;thread 是指向要处理的线程对象.然后将item加入到LIBEVENT_THREAD 对象的 struct conn_queue new_conn_queue中
同样也是有一个free list 叫 freeconns&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static conn **freeconns;
static int freetotal; //总的freeconns的大小
static int freecurr; //目前的空闲的 freeconn 的大小

/*
 * Adds a connection to the freelist. 0 = success.
 */
bool conn_add_to_freelist(conn *c) {
    bool ret = true;
    pthread_mutex_lock(&amp;amp;conn;_lock);
    if (freecurr &amp;lt; freetotal) {
        freeconns[freecurr++] = c;
        ret = false;
    } else {
        /* try to enlarge free connections array */
        size_t newsize = freetotal * 2;
        conn **new_freeconns = realloc(freeconns, sizeof(conn *) * newsize);
        if (new_freeconns) {
            freetotal = newsize;
            freeconns = new_freeconns;
            freeconns[freecurr++] = c;
            ret = false;
        }
    }
    pthread_mutex_unlock(&amp;amp;conn;_lock);
    return ret;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加一个 connection 到 freelist, 如果超过了freetotal的大小,那么就将freetotal*2 然后realloc一块新的是原来2倍内存的大小.然后把这个connection添加进去,发现好多地方都是这么做的&lt;/p&gt;

&lt;p&gt;memcached的多线程主要是通过实例化多个libevent实现的,分别是一个主线程和n个worker线程.无论是主线程还是worker线程全部通过libevent管理网络事件,实际上每个线程都是一个单独的libevent实例.
主线程负责监听客户端的建立连接请求,以及建立连接后将连接好后生成的connection发送给work_thread去负责处理&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://175.41.172.193/wp-content/uploads/2012/02/thread2.jpg&quot;&gt;&lt;img src=&quot;http://175.41.172.193/wp-content/uploads/2012/02/thread2-300x234.jpg&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;dispatcher_thread 和 worker_thread 的定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;typedef struct {
    pthread_t thread_id;        /* unique ID of this thread */
    struct event_base *base;    /* libevent handle this thread uses */
    struct event notify_event;  /* listen event for notify pipe */
    int notify_receive_fd;      /* receiving end of notify pipe */
    int notify_send_fd;         /* sending end of notify pipe */
    struct thread_stats stats;  /* Stats generated by this thread */
    struct conn_queue *new_conn_queue; /* queue of new connections to handle */
    cache_t *suffix_cache;      /* suffix cache */
} LIBEVENT_THREAD;

typedef struct {
    pthread_t thread_id;        /* unique ID of this thread */
    struct event_base *base;    /* libevent handle this thread uses */
} LIBEVENT_DISPATCHER_THREAD;

thread_init是启动所有的worker线程的核心方法.
void thread_init(int nthreads, struct event_base *main_base) {
     ......//加锁等操作
    for (i = 0; i &amp;lt; item_lock_count; i++) {
        pthread_mutex_init(&amp;amp;item;_locks[i], NULL);
    }

    threads = calloc(nthreads, sizeof(LIBEVENT_THREAD));
    if (! threads) {
        perror(&quot;Can&#39;t allocate thread descriptors&quot;);
        exit(1);
    }

    dispatcher_thread.base = main_base; //dispatcher_thread是静态的全局变量.dispatcher_thread 注册的事件是main_base时间,也就是负责监听socket请求的事件
    dispatcher_thread.thread_id = pthread_self(); 

    for (i = 0; i &amp;lt; nthreads; i++) { //这里是为每一个线程创建一个pipe,这个pipe被用来作为dispatch通知worker线程有新的连接到达
        int fds[2];
        if (pipe(fds)) {
            perror(&quot;Can&#39;t create notify pipe&quot;);
            exit(1);
        }

        threads[i].notify_receive_fd = fds[0];
        threads[i].notify_send_fd = fds[1];

        setup_thread(&amp;amp;threads;[i]);
        /* Reserve three fds for the libevent base, and two for the pipe */
        stats.reserved_fds += 5;
    }

    /* Create threads after we&#39;ve done all the libevent setup. */
    for (i = 0; i &amp;lt; nthreads; i++) {
        create_worker(worker_libevent, &amp;amp;threads;[i]);
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后是setup_thread 方法,setup_thread 方法主要是创建所有worker线程的libevent实例(主线程的libevent实例在main函数里面创建)
注册所有worker线程的管道读端的libevent的读事件,等待主线程的通知.然后初始化所有worker的CQ.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void setup_thread(LIBEVENT_THREAD *me) {
    me-&amp;gt;base = event_init();
    if (! me-&amp;gt;base) {
        fprintf(stderr, &quot;Can&#39;t allocate event base\n&quot;);
        exit(1);
    }

    /* Listen for notifications from other threads */
    event_set(&amp;amp;me-;&amp;gt;notify_event, me-&amp;gt;notify_receive_fd,
              EV_READ | EV_PERSIST, thread_libevent_process, me);
    event_base_set(me-&amp;gt;base, &amp;amp;me-;&amp;gt;notify_event);

    if (event_add(&amp;amp;me-;&amp;gt;notify_event, 0) == -1) {
        fprintf(stderr, &quot;Can&#39;t monitor libevent notify pipe\n&quot;);
        exit(1);
    }

    me-&amp;gt;new_conn_queue = malloc(sizeof(struct conn_queue));
    if (me-&amp;gt;new_conn_queue == NULL) {
        perror(&quot;Failed to allocate memory for connection queue&quot;);
        exit(EXIT_FAILURE);
    }
    cq_init(me-&amp;gt;new_conn_queue);

    if (pthread_mutex_init(&amp;amp;me-;&amp;gt;stats.mutex, NULL) != 0) {
        perror(&quot;Failed to initialize mutex&quot;);
        exit(EXIT_FAILURE);
    }

    me-&amp;gt;suffix_cache = cache_create(&quot;suffix&quot;, SUFFIX_SIZE, sizeof(char*),
                                    NULL, NULL);
    if (me-&amp;gt;suffix_cache == NULL) {
        fprintf(stderr, &quot;Failed to create suffix cache\n&quot;);
        exit(EXIT_FAILURE);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后 create_worker启动了所有线程,pthread_create调用worker_libevent方法,这个方法又调用event_base_loop() 启动该线程的libevent.&lt;/p&gt;

&lt;p&gt;在server_socket函数中,主要就是建立一个socket并且绑定到一个port上.然后调用conn_new(这里传入的事件是main_base)注册事件
event_set(&amp;amp;c-&gt;event, sfd, event_flags, event_handler, (void *)c);
事件为持久可读,所以dispatch_thread在当前listen的socket可读时,就会调用event_handler,进而调用driver_machine(c) 进入状态机.而在driver_machin中,如果是主线程(dispatch_thread)则会在accept socket 后调用dispatch_new_conn函数来给各个work_thread派发connection&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static int server_socket(const char *interface,
                         int port,
                         enum network_transport transport,
                         FILE *portnumber_file) {
     ....
            if (!(listen_conn_add = conn_new(sfd, conn_listening,
                                             EV_READ | EV_PERSIST, 1,
                                             transport, main_base))) { 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此 dispatch_thread 和 worker_threads的libevent都已开启&lt;/p&gt;

&lt;p&gt;thread_libevent_process是worker_thread的管道读端有事件的时候调用的方法.参数fd是这个worker_thread的管道读端的描述符.
首先将管道的1个字节读出,这一个字节是dispatch_thread写入的.用来通知表示有数据写入.然后从自己的CQ里面pop出一个item进行处理,这个item是被dispatch_thread丢到这个cq队列中的.
item-&gt;sfd是已建立的socket连接的描述符,通过conn_new函数为该描述符注册libevent的读事件,me-&gt;base 是 struct event_base 代表自己的一个线程结构体,就是说对该描述符的事件处理交给当前这个worker_thread处理.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/*
 * Processes an incoming &quot;handle a new connection&quot; item. This is called when
 * input arrives on the libevent wakeup pipe.
 */
static void thread_libevent_process(int fd, short which, void *arg) {
    LIBEVENT_THREAD *me = arg;
    CQ_ITEM *item;
    char buf[1];

    if (read(fd, buf, 1) != 1)
        if (settings.verbose &amp;gt; 0)
            fprintf(stderr, &quot;Can&#39;t read from libevent pipe\n&quot;);

    item = cq_pop(me-&amp;gt;new_conn_queue);

    if (NULL != item) {
        conn *c = conn_new(item-&amp;gt;sfd, item-&amp;gt;init_state, item-&amp;gt;event_flags,
                           item-&amp;gt;read_buffer_size, item-&amp;gt;transport, me-&amp;gt;base);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来是conn_new函数,前面是一系列的判断,从conn_from_freelist()取得连接.
这里注册了事件,由当前线程处理,(因为这里的event_base是改work_thread自己的)
当该连接有可读时会回调event_handler函数,event_handler调用memcached最核心的方法drive_machine.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conn *conn_new(const int sfd, enum conn_states init_state,
                const int event_flags,
                const int read_buffer_size, enum network_transport transport,
                struct event_base *base) {
     ......
    event_set(&amp;amp;c-;&amp;gt;event, sfd, event_flags, event_handler, (void *)c);
    event_base_set(base, &amp;amp;c-;&amp;gt;event);
    c-&amp;gt;ev_flags = event_flags;

    if (event_add(&amp;amp;c-;&amp;gt;event, 0) == -1) {
        if (conn_add_to_freelist(c)) {
            conn_free(c);
        }
        perror(&quot;event_add&quot;);
        return NULL;
    }

    STATS_LOCK();
    stats.curr_conns++;
    stats.total_conns++;
    STATS_UNLOCK();

    MEMCACHED_CONN_ALLOCATE(c-&amp;gt;sfd);

    return c;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面将dispatch_thread 会向work_thread写入一字节的数据.dispatch_thread注册的是监听socket可读的事件,然后当有建立连接请求时,dispatch_thread会处理,回调函数也是event_handler(因为dispatch_thread也是通过conn_new初始化监听socket的libevent可读事件)&lt;/p&gt;

&lt;p&gt;driven_machine 网络事件处理最核心函数 是所有线程在connection来到时都要调用的函数.
driven_machine 主要就是通过当前连接的conn的state来判断进行何种处理,因为libevent注册了读写事件回调的都是这个函数,所以实际上我们在注册libevent相应事件时,会同时把事件的状态写入到conn结构体里,libevent进行回调时会把该conn结构作为参数传过来&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void drive_machine(conn *c) {
     ...
    while (!stop) {
     ...
        switch(c-&amp;gt;state) {

        case conn_listening:// 该状态是conn_listening状态,只有dispatch_thread才会进入
          ....
          dispatch_conn_new(sfd, conn_new_cmd,EV_READ|EV_PERSIST,
                                     DATA_BUFFER_SIZE, tcp_transport);
          //这里就是dispatch_thread 通知 work_thread 的地方了.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;memcached.h里面conn的stat的声明&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/*
 * NOTE: If you modify this table you _MUST_ update the function state_text
 */
/**
 * Possible states of a connection.
 */
enum conn_states {
    conn_listening,  /**&amp;lt; the socket which listens for connections */
    conn_new_cmd,    /**&amp;lt; Prepare connection for next command */
    conn_waiting,    /**&amp;lt; waiting for a readable socket */
    conn_read,       /**&amp;lt; reading in a command line */
    conn_parse_cmd,  /**&amp;lt; try to parse a command from the input buffer */
    conn_write,      /**&amp;lt; writing out a simple response */
    conn_nread,      /**&amp;lt; reading in a fixed number of bytes */
    conn_swallow,    /**&amp;lt; swallowing unnecessary bytes w/o storing */
    conn_closing,    /**&amp;lt; closing this connection */
    conn_mwrite,     /**&amp;lt; writing out many items sequentially */
    conn_max_state   /**&amp;lt; Max state value (used for assertion) */
};

dispatch_conn_new 将一个新的connection给另外一个thread.这个函数只有dispatch_thread才会调用.
/* Which thread we assigned a connection to most recently. */
static int last_thread = -1;
/* 这里是静态变量 last_thread 记录的是上一次调用的thread.所以这里memcached并没有用高深的方法记录将connection 分发给哪一个thread.只是用轮询的方法实现
这里dispatch_thread 创建了一个ca_item,并插入到一个thread的cq里面.然后往对应的work_thread写入1字节的数据来通知他.这个时候work_thread立即回调了thread_libevent_process的方法来对数据进行读取. 然后work_thread线程取出这个item(item 里面包含了这次connection 的连接sfd),注册读时间,当该条连接上有数据时,最终也会回调drive_machine方法,也就是driven_machine 的 conn_read. 其他的conn 的状态全部由work_thread去处理,dispatch_thread 只负责将item 发到对应的work_thread中去.
*/

void dispatch_conn_new(int sfd, enum conn_states init_state, int event_flags,
                       int read_buffer_size, enum network_transport transport) {
    CQ_ITEM *item = cqi_new();
    int tid = (last_thread + 1) % settings.num_threads;
//这里是通过轮询找到目前要分配给的thread

    LIBEVENT_THREAD *thread = threads + tid;

    last_thread = tid;

    item-&amp;gt;sfd = sfd;
    item-&amp;gt;init_state = init_state;
    item-&amp;gt;event_flags = event_flags;
    item-&amp;gt;read_buffer_size = read_buffer_size;
    item-&amp;gt;transport = transport;

    cq_push(thread-&amp;gt;new_conn_queue, item);

    MEMCACHED_CONN_DISPATCH(sfd, thread-&amp;gt;thread_id);
    if (write(thread-&amp;gt;notify_send_fd, &quot;&quot;, 1) != 1) { //这里就是往work_thread的notify_fd 写入1字节的数据.来
        perror(&quot;Writing to thread notify pipe&quot;);
    }
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>libevent demo</title>
   <link href="http://baotiao.github.io//2012/01/libevent-demo/"/>
   <updated>2012-01-31T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2012/01/libevent-demo</id>
   <content type="html">&lt;p&gt;libevent 就是对select的封装.&lt;/p&gt;

&lt;p&gt;select 比一般的阻塞调用厉害的地方在于,它能够阻塞在多个调用上.比如read阻塞了以后,程序只能当有新的输入以后,程序才继续运行,socket accept阻塞以后,当socket有请求,这个阻塞才会停止. 而select的叫IO多路复用,就是说调用select 以后可以同时监听这些阻塞.调用select以后,当有新的输入或者有新的socket请求程序都会停止阻塞,继续运行.&lt;/p&gt;

&lt;p&gt;read/write 操作&lt;/p&gt;

&lt;p&gt;libevent 编译时候加-levent&lt;/p&gt;

&lt;p&gt;main 开始的时候event_init();&lt;/p&gt;

&lt;p&gt;event_set(struct event &lt;em&gt;ev, int fd, short event, void (&lt;/em&gt;fn)(int, short, void &lt;em&gt;), void &lt;/em&gt;arg);&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;construct struct event for event_add and event_del, the fourth parameter is the callback function we should implement.&lt;/li&gt;
&lt;li&gt;event type: EV_TIMEOUT,EV_SIGNAL,EV_READ,EV_WRITE&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;--&gt;The additional flag EV_PERSIST makes an event_add() persistent until event_del() has been called.
如果没有用EV_PERSIST那么这个时间触发一次以后,这个事件就不再被注册了.所以基本都有EV_PERSIST&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;event_add(struct event *ev, struct timeval *tv)
event_del(struct event *ev):
add or del an event.

event_dispatch():
In order to process events, an application needs to call it.This function only returns on error, and should replace the event core of the application program.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;表示监听多个event type的时候 用| EV_READ | EV_TIMEOUT&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/* 这个是libevent 的一个例子. 先建立一个FIFO,将其设置为非阻塞.注册这个event,
event_set(&amp;amp;evfifo, socket, EV_READ | EV_PERSIST, fifo_read, &amp;amp;evfifo);
意思是监听socket这个描述符的读请求,并且设置EV_PERSIST. 触发以后调用fifo_read这个函数.
等待管道的另一端写数据.
此时在另外一个终端执行echo &quot;heihei&quot; &amp;gt; event.fifo 这个就会输出这个heihei.

#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;sys/types.h&amp;gt;
#include &amp;lt;sys/stat.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;signal.h&amp;gt;
#include &amp;lt;event.h&amp;gt;
#include &amp;lt;errno.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;

void fifo_read(int fd, short event, void *arg)
{
    char buf[255];
    int len;
    struct event *ev = arg;
    static int count = 0;

    /* Reschedule this event */
//    event_add(ev, NULL);

    fprintf(stderr, &quot;fifo_read called with fd: %d, event: %d, arg: %p\n&quot;,
        fd, event, arg);
    if (++count == 20) {
        len = read(fd, buf, sizeof(buf) - 1);

        if (len == -1) {
            perror(&quot;read&quot;);
            return;
        } else if (len == 0) {
            fprintf(stderr, &quot;Connection closed\n&quot;);
            return;
        }

        buf[len] = &#39;\0&#39;;
        fprintf(stdout, &quot;Read: %s\n&quot;, buf);
    }
printf(&quot;over...\n&quot;);
}

int main()
{
    struct event evfifo;
    char* fifo = &quot;event.fifo&quot;;

    unlink(fifo);
    if (mkfifo(fifo, 0777) == -1) {
        perror(&quot;mkfifo&quot;);
        exit(1);
    }

    int socket = open(fifo, O_RDWR | O_NONBLOCK, 0);
    if (socket == -1) {
        perror(&quot;open&quot;);
        exit(1);
    }

    fprintf(stderr, &quot;Write data to %s\n&quot;, fifo);
    event_init();
    /* Initalize one event */
    event_set(&amp;amp;evfifo, socket, EV_READ | EV_PERSIST, fifo_read, &amp;amp;evfifo);
    /* Add it to the active events, without a timeout */
    event_add(&amp;amp;evfifo, NULL);

    event_dispatch();

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;今天做了一堆的实验 可以得出一些结论:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;libevent 的event_base是基于线程的.也就是说一个thread只能有一个event_base.如果有多个event_base 那么后来的event_base则会跑到栈的开头,等这个event_base结束以后,这个event_base会结束,原来的event_base又会顶上来. 表现出来的就是 如果在call_accept 里面新建立一个event_base.那么 当telnet成功连接一次,新过来的telnet的连接就不会进入到call_accept这个事件里面.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;event_base_loop 启动以后,就不能再往这个event_base添加新的事件了.添加了以后会segmentfault.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;基于上面这种情况,所以单线程实现不了能够为多个telnet 提供call_accept的服务. 只能一个telnet-&gt;call_accept-&gt;str_echo 然后再处理一个新的telnet的请求.&lt;/p&gt;

&lt;p&gt; 所以想实现这种为多个telnet 提供服务的,要做成多thread这种,然后每个thread有一个自己的event_base.然后就telnet 连接成功以后的 sfd 传给每个线程,让每个线程在自己的event_base里面监听这个sfd的输入.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;还有就是 在memcached里面,dispatch_thread 往 work_thread 写入1个字节的数据.这个时候如果work_thread在忙,没有办法立刻处理这个事件会怎么办. libevent是这样做的. 在dispatch_thread 往 work_thread notify_receive_fd 写入事件后, 如果work_thread 正在忙,那么这个事件会保存在改线程的内存空间里面,一旦该线程没有阻塞.那么又会立刻执行这个事件.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;TCP断开连接的过程中client会保持一段时间在TIME_WAIT状态. 因为最后的阶段是srv往cli发一个fin,然后cli返回一个ack给srv. 然后会保持一段时间的TIME_WAIT状态,这个时候如果cli又在这个端口建立新连接会出错.得等会.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;pre&gt;&lt;code&gt;    #include &amp;lt;stdio.h&amp;gt;
    #include &amp;lt;stdlib.h&amp;gt;
    #include &amp;lt;fcntl.h&amp;gt;
    #include &amp;lt;event.h&amp;gt;
    #include &amp;lt;unistd.h&amp;gt;
    #include &amp;lt;netinet/in.h&amp;gt;
    #include &amp;lt;sys/socket.h&amp;gt;
    #include &amp;lt;string.h&amp;gt;
    #include &amp;lt;fcntl.h&amp;gt;

    typedef struct sockaddr * SA;
    struct event_base *main_base;

    void str_echo(int fd, short event, void *arg)
    {
        char buf[100];
        int n;

        n = read(fd, buf, 100);
            fputs(&quot;a socket has come\n&quot;, stdout);
            write(fd, buf, n);
        if (n &amp;lt; 0) {
            return ;
        }
        return ;
    }
    void call_accept(int fd, short event, void *arg)
    {
        sleep(10);
        printf(&quot;come in a new accept\n&quot;);
        struct sockaddr_in cliaddr;
        socklen_t clilen;
        int connfd;
        connfd = accept(fd, (struct sockaddr *) &amp;amp;cliaddr, &amp;amp;clilen);
        printf(&quot;accept from cliaddr\n&quot;);
        char buf[100];
    //    read(fd, buf, 100);
        //str_echo(connfd);
    //    printf(&quot;%d\n&quot;, (void *)base);
    //    struct event read_ev;
    //    //fcntl(connfd, F_SETFL, O_NONBLOCK);
    //    event_set(&amp;amp;read_ev, connfd, EV_READ, str_echo, &amp;amp;read_ev);
    //    //event_base_set(main_base, &amp;amp;read_ev);
    //    event_add(&amp;amp;read_ev, NULL);

    //    event_base_loop(base, 0);
    }
    int main()
    {
        int listenfd, connfd;
        struct sockaddr_in cliaddr, servaddr;
        socklen_t clilen;

        listenfd = socket(AF_INET, SOCK_STREAM, 0);
        memset(&amp;amp;servaddr, 0, sizeof(servaddr));
        servaddr.sin_family = AF_INET;
        servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
        servaddr.sin_port = htons(9877);
        bind(listenfd, (struct sockaddr *) &amp;amp;servaddr, sizeof(servaddr));
        listen(listenfd, 10);

        struct event ev;
        main_base = event_init();
        event_set(&amp;amp;ev, listenfd, EV_READ | EV_PERSIST, call_accept, &amp;amp;ev);
        //event_base_set(main_base, &amp;amp;ev);
        event_add(&amp;amp;ev, NULL);
        printf(&quot;block before accept\n&quot;);
        event_base_loop(main_base, 0);    
        printf(&quot;after event_dispatch\n&quot;);
        return 0;
    }
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>network programming echo example</title>
   <link href="http://baotiao.github.io//2012/01/echo-server-client/"/>
   <updated>2012-01-08T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2012/01/echo-server-client</id>
   <content type="html">&lt;p&gt;socket 描述符就跟 file 描述符一样.&lt;/p&gt;

&lt;p&gt;1个字节有8位.也就是1个字节可以表示0~255(2&lt;sup&gt;8&lt;/sup&gt;). 所以short是2个字节,表示的就是0~(2&lt;sup&gt;16&lt;/sup&gt;). 因为int 是4字节,所以int表示的就是0~(2&lt;sup&gt;32&lt;/sup&gt;)
short s = 0x0100 0x是标志是16进制,所以00是第一个字节,01是第二个字节.所以这里表示的值是256
所有的套接字地址结构一般都要包含3个字段 sa_family_t sin_family, struct in_addr sin_addr, in_port_t sin_port. 也就是套接字的address family 地址协议,套接字监听的地址,监听的端口号.&lt;/p&gt;

&lt;p&gt;所有客户和服务器都从调用socket开始,它返回一个套接字描述符号.客户随后调用connect,服务器则调用bind,listen和accept.大多数TCP服务器是并发的,所以为每个待处理的客户连接建立一个fork出一个子进程来处理.&lt;/p&gt;

&lt;p&gt;socket 指定期望的通信协议类型(使用ipv4的tcp,使用ipv6的udp等)
int socket(int family, int type, int protocol)
其中family参数执行协议组,就是上面讲的address family地址协议,type参数指明套接字的类型.protocol参数为某个协议类型常值.
函数在成功时返回一个小的非负整数值,它与文件描述符类似,我们把它称为套接字描述符,简称sockfd.&lt;/p&gt;

&lt;p&gt;在server 与 client 交互过程会产生3个sockfd,client 产生一个fd.然后connect server的addr.
在server端先是产生一个fd,bind()绑定自己的ip地址和端口号. 然后接下来listen.然后accept 阻塞等待client接过来一个请求.等到client正好connect这个server后就生成一个新的connected的fd, 还有就是一个client_addr.这个就是server获得到的两个东西,然后就是写数据或者收数据都是直接对这个sockfd进行的,而不是对这个地址进行的.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// 这个是srv端程序.
#include&amp;lt;stdio.h&amp;gt;
#include&amp;lt;netinet/in.h&amp;gt;
#include&amp;lt;sys/socket.h&amp;gt;
#include&amp;lt;string.h&amp;gt;
void str_echo(int sockfd)
{
    ssize_t n;
    char buf[1000];
again:
    while ((n = read(sockfd, buf, 1000)) &amp;gt; 0) { //读已经建立连接的socket的数据.
        fputs(&quot;a socket has come\n&quot;, stdout);
        write(sockfd, buf, n+3); // 写入数据
    }
    if (n &amp;lt; 0)
        exit(0);
}
int main()
{
    int listenfd, connfd;  //这两个分别是监听时候的链接,和accept建立以后的链接.
    pid_t childpid; //子进程的id.
    socklen_t clilen; 
    struct sockaddr_in cliaddr, servaddr; // 基于ipv4的套接字地址结构
    listenfd = socket(AF_INET, SOCK_STREAM,0); //返回一个socket描述符
    memset(&amp;amp;servaddr, 0, sizeof(servaddr));// 把这个servaddr地址初始化
    servaddr.sin_family = AF_INET; // 设置 adress family
    servaddr.sin_addr.s_addr = htonl(INADDR_ANY); // 设置套接字监听的地址为通配地址
    servaddr.sin_port = htons(9877); // 设置监听的端口号  
    bind(listenfd, (struct sockaddr *) &amp;amp;servaddr, sizeof(servaddr));
     //将这个listenfd socket 描述符绑定到servaddr这个地址
    listen(listenfd, 10); //listenfd 这个socket端口开始监听
    for(; ; ) { // 一个死循环来server
        clilen = sizeof(cliaddr);
        connfd = accept(listenfd, (struct sockaddr *) &amp;amp;cliaddr, &amp;amp;clilen);
        // 这里阻塞accept 等待 cli 连接. cli 连接成功会一个新的connfd,这个connfd就是通向客户端的socket描述符,往这个描述符写东西就可以送到客户端.
        if ((childpid = fork() == 0)) {
            close(listenfd);
            str_echo(connfd);
            exit(0);
        }
        fputs(&quot;i am parent\n&quot;, stdout);
        close(connfd);
    }
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是cli端程序&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//cli 端的程序要做的事情也和srv端一样,不过初始化好地址以后,调用connect.试图与srv端连接.链接成功则将一个文件描述符指向srv端,就可以srv端写入或者读出数据了.
#include&amp;lt;stdio.h&amp;gt;
#include&amp;lt;netinet/in.h&amp;gt;
#include&amp;lt;unistd.h&amp;gt;
#include&amp;lt;sys/socket.h&amp;gt;
#include&amp;lt;string.h&amp;gt;
void str_cli(FILE *fp, int sockfd)
{
    char sendline[1000], recvline[1000];
    while(fgets(sendline, 1000, fp) != NULL) {
        write(sockfd, sendline, strlen(sendline));
        if (read(sockfd, recvline, 100) == 0)
            printf(&quot;server terminated\n&quot;);
        fputs(recvline, stdout);
    }
}
int main(int argc, char *argv[])
{
    int sockfd;
    struct sockaddr_in servaddr;
    sockfd = socket(AF_INET, SOCK_STREAM, 0);
    memset(&amp;amp;servaddr, 0, sizeof(servaddr));
    servaddr.sin_family = AF_INET;
    servaddr.sin_port = htons(9877);
    inet_pton(AF_INET, argv[1], &amp;amp;servaddr.sin_addr);
    connect(sockfd, (struct sockaddr *) &amp;amp;servaddr, sizeof(servaddr));
    str_cli(stdin, sockfd);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Beanstalk 学习</title>
   <link href="http://baotiao.github.io//2011/11/beanstalk/"/>
   <updated>2011-11-12T01:41:18+08:00</updated>
   <id>http://baotiao.github.io//2011/11/beanstalk</id>
   <content type="html">&lt;p&gt;看的Beanstalk第一版的代码,因为代码比较短.小组的分享&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://www.slideshare.net/baotiao/beanstalk&quot;&gt;Beanstalk&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;View more &lt;a href=&quot;http://www.slideshare.net/&quot;&gt;presentations&lt;/a&gt; from &lt;a href=&quot;http://www.slideshare.net/baotiao&quot;&gt;宗志 陈&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://francepharmacieenligne.com/products/kamagra.htm&quot;&gt;kamagra&lt;/a&gt; &lt;a href=&quot;http://erektilepillenonline.com/products/viagra.htm&quot;&gt;viagra&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>函数指针和指针函数</title>
   <link href="http://baotiao.github.io//2011/10/function-point/"/>
   <updated>2011-10-23T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2011/10/function-point</id>
   <content type="html">&lt;pre&gt;&lt;code&gt;函数指针和指针函数区别. 指针函数. 返回值是指针的函数叫指针函数. 函数的返回值可以是int,char,double,struct,也可以是指针,指针函数就是返回值是指针的函数,也就是返回的是一个地址.比如: 

char *cp(char *s, char *t) 
{ 
    t = s; return t; 
}
int main() 
{ 
    char s1[10] = &quot;hello&quot;; 
    char s2[10] = &quot;world&quot;; 
    printf(&quot;%s %sn&quot;, s1, s2); 
    printf(&quot;%sn&quot;,cp(s1,s2)); 
    return 0; 
}

这里返回值就是一个地址. 这里 char *cp(char *s, char *t)里面的第一个*我们可以看做是跟定义一个指针 char *p,里面的*是一个意思,就是只是表示这是一个指针而已.所以我们调用cp(s1,s2)返回的就是一个地址,printf(&quot;%s&quot;)的时候,传入的是一个地址或者字符串的名字.如果是一个 int *get(),那么获得返回值就是 printf(&quot;%dn&quot;,*get())就可以了.

函数指针 指向函数的指针叫函数指针. 在C语言中，函数也是一种类型，可以定义指向函数的指针。我们知道，指针变量的内存单元存放一个地址值，而函数指针存放的就是函数的入口地址(位于.text段). 下面看一个简单的例子: 例 23.3. 函数指针 

void say_hello(const char *str) 
{ 
    printf(&quot;Hello %sn&quot;, str); 
} 
int main(void) { 
    void (*f)(const char *) = say_hello; //注意这里*f必须加()否则就和上面说的函数指针混乱了. 
    f(&quot;Guys&quot;); 
    return 0; 
}


分析一下变量f的类型声明void (*f)(const char *)，f首先跟*号结合在一起，因此是一个指针。(*f)外面是一个函数原型的格式，参数是const char *，返回值是void，所以f是指向这种函数的指针。而say_hello的参数是const char *，返回值是void，正好是这种函数，因此f可以指向say_hello。注意，say_hello是一种函数类型，而函数类型和数组类型类似，做右值使用时自动转换成函数指针类型，所以可以直接赋给f，当然也可以写成void (*f)(const char *) = &amp;amp;say_hello;，把函数say_hello先取地址再赋给f，就不需要自动类型转换了。
可以直接通过函数指针调用函数，如上面的f(&quot;Guys&quot;)，也可以先用*f取出它所指的函数类型，再调用函数，即(*f)(&quot;Guys&quot;)。可以这么理解：函数调用运算符()要求操作数是函数指针，所以f(&quot;Guys&quot;)是最直接的写法，而say_hello(&quot;Guys&quot;)或(*f)(&quot;Guys&quot;)则是把函数类型自动转换成函数指针然后做函数调用。
就是我们平常调用函数的时候也是先吧函数类型转换程函数指针,然后在做函数调用,有了函数指针直接把指针指向这一个函数在.text段的位置,就直接做函数调用了.函数指针可以指向所有这一类的函数.
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>introduce awk</title>
   <link href="http://baotiao.github.io//2011/08/awk-introduce/"/>
   <updated>2011-08-19T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2011/08/awk-introduce</id>
   <content type="html">&lt;p&gt;由于平常的工作都在linux下面进行,所以经常用到linux下面的awk,sed,grep等工具.
组内的一次分享.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/baotiao/czzawk&quot;&gt;introduce awk&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Cola.php的默认配置以及url分发</title>
   <link href="http://baotiao.github.io//2011/08/cola-framework/"/>
   <updated>2011-08-17T15:10:42+08:00</updated>
   <id>http://baotiao.github.io//2011/08/cola-framework</id>
   <content type="html">&lt;p&gt;和cola.php 是一个叫付超群写得框架,可能对比较大型的框架理解不清楚,可以先看这个比较小型的.
0.1版就包含了框架里面最主要的几个文件,那些扩张功能还不包括在里面,非常适合让我们理解如何实现一个mvc框架的过程.
这里是对 0.1版代码的阅读记录.
1. 在demo下面的index.php 是所有动态请求的入口,也是框架的入口.
2. 在index.php里面require了Cola.php 和 config.inc.php.
其中Cola.php是最主要的函数,在Cola.php里面的&lt;em&gt;&lt;em&gt;construct函数里面定义的根目录,用spl_autoload_register 实现框架加载任意的类.
3. 其中具体的加载方法是在Cola.php下面的loadClass方法里面.
首先,有三个默认加载的方法
如果是Cola_Router 那么 就加载Cola 下的Router.php.
如果是Cola_View 那么就加载Cola 下的View.php.
如果是Cola_Controller 那么就加载Cola 下的Controller.php
如果不是上面上中情况,那么在建立一个新的类的时候,就是getInstance()的时候必须指定类名,以及目录.
那么loadClass也会找到相应的文件并包含他.
4.config.inc.php 就是包含的默认的配置文件.
$urls 是用于url 分发的时候,定义的规则,与$urls匹配以后再找到相应的文件来加载.
$dbConfig 包含的是db的一些信息.
index.php 会调用cola 的config 函数把规则保存在$&lt;/em&gt;config 变量里面.
5. index.php 会调用dispatch() 函数,就是实现url 分发的主要方法.
首先.dispatch 函数会默认去获得DispatchInfo()的信息,如果没有那么就去setDispatchInfo().
然后,在setDispatchInfo()里面就会先获得Router的信息.那么Router会把config.inc.php里面包含的$urls的匹配信息保存进去.
然后.在setDispatchInfo()里面会调用setPathInfo函数,函数根据$&lt;/em&gt;SERVER[&#39;PATH_INFO&#39;]来获得pathInfo.
之后会调用Router里面的match方法.改方法将config.inc.php里面定义的匹配规则一一与获得的pathInfo匹配,如果匹配成功就return出来.
其中如果$url里面包含里maps这个变量,那么就可以变量赋值,并且,如果maps里面包含的某些变量没有进行赋值,那么default则会给赋默认值.
当匹配成功后会得到file,class,action,以后如果有默认参数会有args 也就是变量的信息.
6.接下来在dispatch()函数里面回去找文件,并且包含他.接下来就是调用找到的class里面的action 并且调用call_user_func_array 来执行
这个方法.然后就跳到执行相应方法的controller里面了.&lt;/p&gt;

&lt;p&gt;后面就是demo 下面的controller方法都是继承自Cola_Controller, model 都是继承自Cola_Model.&lt;/p&gt;

&lt;p&gt;有新的再及时更新.
在google 的code 里面有最新更新  http://code.google.com/p/colaphp/ 还有代码获取 &lt;a href=&quot;http://cheaponlinegenericdrugs.com/products/provigrax.htm&quot;&gt;Men&#39;s Health&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>grep in awk</title>
   <link href="http://baotiao.github.io//2011/08/grep-in-awk/"/>
   <updated>2011-08-17T10:05:32+08:00</updated>
   <id>http://baotiao.github.io//2011/08/grep-in-awk</id>
   <content type="html">&lt;pre&gt;&lt;code&gt;awk &#39;BEGIN {while (( getline &amp;lt; &quot;f2&quot; &amp;gt; 0 )) { f2[lc] = $0 ;lc++;}} { for (i=1; i&amp;lt;lc; i++) { if (match (f2[i], $1)) print f2[i];}}&#39; f1

awk &#39;BEGIN {while (( getline &amp;lt; &quot;f2&quot; &amp;gt; 0 )) { f2[lc] = $0 ;lc++;}} { for (i=1; i&amp;lt;lc; i++) { if (f2[i] ~ $1) print f2[i];}}&#39; f1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; 其中 match 函数 match(s,r)              测试s是否包含匹配r的字符串
有 两个文件 f1 包含
czz 234
xyy 2ee
ghy 2g3
f2包含
au=xyygxh
ssssssssssssss
au=xyy;xyygxhgxh
au=czz;andxyy
au=czz;andguanxiaohong&lt;/p&gt;

&lt;p&gt;现在 要把 f2 搜索一次 如果有包含 czz 就列出来, 然后 再把f2搜索一次 有xyy就列出来
所以答案应该是
au=czz;andxyy
au=czz;andguanxiaohong
au=xyy;xyygxhgxh
au=czz;andxyy
au=xyy;xyygxhgxh&lt;/p&gt;

&lt;p&gt;刚开始想到的是 先 awk 然后 里面再用 grep 来做,不过后来试验了无数次 grep 总是报错, 不过也知道了 grep 里面包含awk的方法.
就是 grep &quot;$(awk &#39;{printf(&quot;%s&quot;), $1;}&#39; f1)&quot; f2 这样就是把  f2 中 包含 czz 或者 xyy 或者 ghy 的找出来.
后来grep 放弃了, 后来看到有这个 match 函数 试了一下,马上就可以了.
awk &#39;BEGIN {while (( getline &amp;lt; &quot;f2&quot; &gt; 0 )) { f2[lc] = $0 ;lc++;}} { for (i=1; i&amp;lt;lc; i++) { if (match (f2[i], $1)) print f2[i];}}&#39; f1
这里 在 BEGIN 的时候 吧 f2的文件都读进来,然后 存在f2数组里面,然后 接下来对每一个f1文件里面的每一行 都 与f2数组做比较.
match(s,r) 的意思是测试s是否包含匹配r 的字符串. 所以f2[i] 包含 $1 就会输出 f2[i].&lt;/p&gt;

&lt;p&gt;还有一种做法
awk &#39;BEGIN {while (( getline &amp;lt; &quot;f2&quot; &gt; 0 )) { f2[lc] = $0 ;lc++;}} { for (i=1; i&amp;lt;lc; i++) { if (f2[i] ~ $1) print f2[i];}}&#39; f1&lt;/p&gt;

&lt;p&gt;这里 匹配的时候要注意两点  if (f2[i] ~ $1) 就是 f2[i] 中 包含 $1 这个正则表达式
还有 就是 是否要用着 /&quot;&quot;/ 这个问题, 如果是变量就不要用,不是变量就必须用.
这样这里就不用grep  实际上也是实现了 grep 的功能.
线上实际用到这个命令的地方.
awk &#39;BEGIN { while (( getline &amp;lt; &quot;/var/sankuai/wwwlogs/www.meituan.com-110816-access_log&quot; &gt; 0))  { f2[lc] = $0; lc++;}} -F &quot;,&quot; { name = &quot;au=&quot;$2&quot;[;|\&quot;]&quot;; for (i = 1; i &amp;lt; lc; i++) {if (f2[i] ~ name) print f2[i];} printf(&quot;\n\n&quot;);}&#39; zhuagui.ouput &gt; zhuagui50&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>vim 一些命令</title>
   <link href="http://baotiao.github.io//2011/08/vim/"/>
   <updated>2011-08-12T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2011/08/vim</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;x dw是删除单词用的  还有d$是删除到当前行的末尾 U 是用来作回复对当前行的修改&lt;/li&gt;
&lt;li&gt;还有就是 dd 完以后可以直接按p就可以实现复制粘帖 不过貌似只能对一行用&lt;/li&gt;
&lt;li&gt;然后就是修改句子中的某一个字符  就可以直接 r然后一个字符就修改光标处的 字符&lt;/li&gt;
&lt;li&gt;cw 用来改变一个单词的部分或者全部  从光标在的那个地方修改&lt;/li&gt;
&lt;li&gt;同样c 和d 是一系列的 也有 c$ 就是修改的最后为止&lt;/li&gt;
&lt;li&gt;ctrl-g 显示当前光标和文件状态&lt;/li&gt;
&lt;li&gt;/ 是用来做正方向查找，然后?是用来作反方向查找&lt;/li&gt;
&lt;li&gt;%可以找到匹配的括号处&lt;/li&gt;
&lt;li&gt;替换的时候 如果要替换某一行，那么 直接 :s/thee/the/g 就可以直接替换thee为the了&lt;/li&gt;
&lt;li&gt;:s/thee/the/gc 会加一次询问而已&lt;/li&gt;
&lt;li&gt;也可以先选中某一个区域块里面的进行修改，系统默认会多出一些&amp;lt;&gt;什么东西，直接写上:s就可以了&lt;/li&gt;
&lt;li&gt;还有一种全屏替换的方法是  :%s/thee/the/g这样就可以了&lt;/li&gt;
&lt;li&gt;想执行外部命令直接加一个  :!就可以了&lt;/li&gt;
&lt;li&gt;:10,20 w test 是把从几行到几行存起来&lt;/li&gt;
&lt;li&gt;然后 :r test是从这个文件中载入这个test 的文件&lt;/li&gt;
&lt;li&gt;o是在本行下面打开新的一行，O是本行上面打开， Shif O 是在本行上面开一行，不过光标不转过去而已&lt;/li&gt;
&lt;li&gt;a是光标后面一个插入，A是在本行的末尾插入&lt;/li&gt;
&lt;li&gt;R是替换&lt;/li&gt;
&lt;li&gt;按住b是从后面往前面跳一个单词&lt;/li&gt;
&lt;li&gt;ct！，这会删除从光标位置到下一个叹号（但不包括）   比如  ct{ 就算删除到 { 之前为止..&lt;/li&gt;
&lt;li&gt;‘’ 直接是跳回到光标 上一次在的地方&lt;/li&gt;
&lt;li&gt;tabe 是在上面开一个东西,然后可以在vimrc 里面配置 就可以用 ctrl n ctrl p 来来回移动了 vim 在写 html 的时候有 比如 &lt;h 1&gt;sdfsdsdfsd &lt;/h 1&gt; 那么 你 直接  dit  t 是 tab 的意思  意味着标签,就可以把 中间的东西删掉了   如果是 dat 的话 就连h1也一起删掉了&lt;/li&gt;
&lt;li&gt;同样在写其他的时候也一样  用 &#39;dfdfdfd&#39;  那么 就是  di&#39; 就是中间的东西删除掉  用 da&#39;的话就连‘也一起删除了 fx移动光标到当前行的下一个 x 处。很明显，x 可以是任意一个字母，而且你可以使用 ; 来重复你的上一个 f 命令。&lt;/li&gt;
&lt;li&gt;tx 和上面的命令类似，但是是移动到 x 的左边一个位置。（这真的很有用）&lt;/li&gt;
&lt;li&gt;&quot; 这个符号是用来使用寄存器的 :reg 就可以列出来现在寄存器里面复制的东西&lt;/li&gt;
&lt;li&gt;我们要拷贝当前行到寄存器 k。你应该按 &quot;kyy &quot;kp 来粘贴寄存器 k 里面的内容到你想要的位置&lt;/li&gt;
&lt;li&gt;这个是用来设定搜索的时候要不要注意大小写（:set ic 和 :set noic）&lt;/li&gt;
&lt;li&gt;dG 就是删除某一行下面的所有行  dgg 就是删除某一行上面的所有行&lt;/li&gt;
&lt;li&gt;vim 里面复制的时候到某个地方 要把某几行先删除掉,然后再粘贴过去,应该是先复制好,然后在选择上要删除的那些行,然后p&lt;/li&gt;
&lt;li&gt;yw 是复制某一个单词.&lt;/li&gt;
&lt;li&gt;vim 里面 set ic 和 set noic 就是要不要大小写.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>iconv 脚本 转换字符编码脚本</title>
   <link href="http://baotiao.github.io//2011/08/iconv/"/>
   <updated>2011-08-10T10:34:58+08:00</updated>
   <id>http://baotiao.github.io//2011/08/iconv</id>
   <content type="html">&lt;p&gt;这个脚本碰到有些别字的中文的时候会报错,直接去文件里面该就可以了.iconv: 北京.csv:31:12: cannot convert&lt;/p&gt;

&lt;p&gt;iconv: 总部.csv:315:12: cannot convert&lt;/p&gt;

&lt;p&gt;iconv: 福州.csv:3:12: cannot convert
比如这样  就是  在 北京.csv 31 行 有错 改下就可以了.
`&lt;/p&gt;

&lt;h1&gt;!/bin/sh&lt;/h1&gt;

&lt;p&gt;MYPATH=&lt;code&gt;pwd&lt;/code&gt;;
files=&lt;code&gt;ls&lt;/code&gt;;
for filename in $files
do
iconv -f UTF-8 -t GBK $filename &gt; gbk.$filename;
done
`&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>vmware centos 安装</title>
   <link href="http://baotiao.github.io//2011/08/vmware-centos/"/>
   <updated>2011-08-07T22:29:07+08:00</updated>
   <id>http://baotiao.github.io//2011/08/vmware-centos</id>
   <content type="html">&lt;p&gt;分享一下今天在vmware 下 centos 的安装过程遇到的问题.
centos 在vmware 里面安装的时候 不要选择简单安装.然后就会进入到centos标准安装界面.
swap 就是linux下的虚拟内存分区,它的作用是在物理内存使用完之后,将磁盘空间(也就是SWAP分区)虚拟成内存来使用.
需要注意的是他的速度比物理内存慢多了,因为他是从磁盘中读取的.如果想更快速度,swap是没用的然后看鸟哥的centos安装教程 在centos 里面安装好以后要 提示插入第二个盘 那么 选择 CD&amp;amp;DVD 然后 勾上 Connected . 然后在Use disc image 选上那个镜像就可以了.&lt;/p&gt;

&lt;p&gt;在centos 里面 安装 VMTool 在 菜单栏 里面有 virtual Machine 然后 里面 &lt;a href=&quot;http://hollandslotscasino.nl/&quot;&gt;slot machines online&lt;/a&gt; install VMTool . 进去以后把 /media 下面的 VMTools 复制出来再安装&lt;/p&gt;

&lt;p&gt;在centos 里面可能无法上网, 首先得先装好 VMTool ,然后 在 系统-&gt;管理-&gt;网络-&gt;双击eth0 里面配置设置成自动获取就可以了.然后保存 /etc/init.d/network restart 就可以了.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>mysql join 学习</title>
   <link href="http://baotiao.github.io//2011/07/mysql-join/"/>
   <updated>2011-07-30T02:13:57+08:00</updated>
   <id>http://baotiao.github.io//2011/07/mysql-join</id>
   <content type="html">&lt;p&gt;join 的学习: 在SQL标准中规划的（Join）联结大致分为下面四种： 1． 内联结：将两个表中存在联结关系的字段符合联结关系的那些记录形成记录集的联结。 2． 外联结：分为外左联结和外右联结。 左联结A、B表的意思就是将表A中的全部记录和表B中联结的字段与表A的联结字段符合联结条件的那些记录形成的记录集的联结，这里注意的是最后出来的记录集会包括表A的全部记录。 右联结A、B表的结果和左联结B、A的结果是一样的，也就是说： Select A.name B.name From A Left Join B On A.id=B.id 和Select A.name B.name From B Right Join A on B.id=A.id执行后的结果是一样的。 还有其他的联结就不用管了. 这里 就是 内联结是只将符合条件的列出来,而外联接是将全部的都列出来,分左右外联接啦. 这里我有个比较简便的记忆方法，内外联结的区别是内联结将去除所有不符合条件的记录，而外联结则保留其中部分。外左联结与外右联结的区别在于如果用A左联结B则A中所有记录都会保 留在结果中，此时B中只有符合联结条件的记录，而右联结相反，这样也就不会混淆了比如表t1 有 2数据 1,2 表 t2 有一个数据 1. 那么 select t1.s1,t2.s1 from t1 inner join t2 on t1.s1=t2.s1; ------ ------ | s1 | s1 | ------ ------ | 1 | 1 | ------ ------ 这个时候 t1里面的 2 这个数据由于在t2表里面找不到 就会被除去. left join &lt;a href=&quot;http://britishcasino.org.uk/&quot;&gt;casino online&lt;/a&gt; 的时候 select t1.s1,t2.s1 from t1 left join t2 on t1.s1=t2.s1; ------ ------ | s1 | s1 | ------ ------ | 1 | 1 | | 2 | NULL | 这个时候是 t1为左表都保存下来. ------ ------ 同样 right join 的时候 select t1.s1,t2.s1 from t1 right join t2 on t1.s1=t2.s1; ------ ------ | s1 | s1 | ------ ------ | 1 | 1 |&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>OpenParty 第一次参加这种交流会</title>
   <link href="http://baotiao.github.io//2011/07/openparty/"/>
   <updated>2011-07-23T21:54:28+08:00</updated>
   <id>http://baotiao.github.io//2011/07/openparty</id>
   <content type="html">&lt;p&gt;来得好像挺多牛B人的.看见挺多个穿TopCoder的.
听了刘未鹏的暗时间
总结下吧,提醒自己以后要做到.
1.应该不断提高挑战自己.把自己当做敌人,而不是把别人当做敌人.
比如:打台球有2个目的.一个是每次都要求赢,还有一个是每次不是为了赢
而是要提高自己的水平,那么每次好几次以后的结果肯定不一样.
2.英文书一定要多看.多看外文书.如果你一直看中文书,那么你读英文书水平一定
不会前行.如果你一开始就读英文书,可能刚开始的效率不行,很痛苦,但是长久以后的好处是很多的.
3.抓住各种时间看书,暗时间就是不需要用头脑的逻辑.抓住这些暗时间能看书,得到的效益是非常大的.
4.就是逼自己做自己没尝试过的事情.在没有自信之前,先用你的勇敢.
后来就是阿稳分享了他对R语言的学习.
1.一个语言决定一个人的思维
2.一个语言用来做他最适合的事情就好了,你可以用R语言设计一整套系统,但是这不是他的强项. &lt;a href=&quot;http://www.profitphp.com/&quot;&gt;online casino bonuses&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>aws 免密码登陆</title>
   <link href="http://baotiao.github.io//2011/07/solve-aws/"/>
   <updated>2011-07-17T19:46:13+08:00</updated>
   <id>http://baotiao.github.io//2011/07/solve-aws</id>
   <content type="html">&lt;p&gt;aws一直要用密钥登录非常的麻烦.
可以在aws里面添加一个新的用户
具体过程:
Setup the new user
:~$ sudo useradd -d /home/username -m -s /bin/bash username
:~$ sudo passwd username
Enter new UNIX password: NEWPASSWORD
Retype new UNIX password: NEWPASSWORD
passwd: password updated successfully
You also need to add to the sudo file
:~$ sudo visudo
Go to the line ALL = (ALL) &lt;a href=&quot;http://britonlinecasino.org.uk/&quot;&gt;casino great britain&lt;/a&gt; ALL and add the new user following example of admin (the leading % not needed)
Give the user ssh access
:~$ sudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.original
:~$ sudo chmod a-w /etc/ssh/sshd_config Change PasswordAuthentication to yes
:~$ sudo /etc/init.d/ssh restart&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>暂时搞定住处</title>
   <link href="http://baotiao.github.io//2011/07/life-in-beijing/"/>
   <updated>2011-07-16T00:00:37+08:00</updated>
   <id>http://baotiao.github.io//2011/07/life-in-beijing</id>
   <content type="html">&lt;p&gt;上周把租房子的事情给搞定了,总之废了各种尽.
这一周工作唯一接触的新东西还是那个ldap.
把ldap 好好搞搞,争取在公司里面成为这个领域的专家才可以.
其他的东西,以后再搞搞吧.
一直想搞一个vimwiki 不过都没去搞. 感觉用google &lt;a href=&quot;http://polskojackpot.com/&quot;&gt;casino online polska&lt;/a&gt; 的 notebook
就可以不过 用着还是不爽. 找个时间整整这玩意儿....
还有就是把awk系统的复习起来,既然已经学了就应该好好总结起来.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>现在在青岛</title>
   <link href="http://baotiao.github.io//2011/07/qingdao/"/>
   <updated>2011-07-03T01:24:14+08:00</updated>
   <id>http://baotiao.github.io//2011/07/qingdao</id>
   <content type="html">&lt;p&gt;应该算是毕业旅行。来了10个人
陈宗志，官宏宇，赵安安，赵辰星，李华东，陈晓师，李闯，赵鹏阳，王超。
吴鹏。
嗯，就是这样。我们住奔奔旅店。&lt;/p&gt;

&lt;p&gt;今天去第二浴场，晚上在奔奔一群人在这扯屁呢。我在这用老板的破电脑记录着，
总觉得应该留下点什么东西。&lt;/p&gt;

&lt;p&gt;以前不是特别爱拍照，这次来不管什么时候总是嚷嚷着，照相，照相。
差不多就是这样，记录些什么，留下点什么。&lt;/p&gt;

&lt;p&gt;在一个陌生的地方，大家确实挺开心的，这样就好，其他的什么都好了。
暴跳是我见过最能说的时候了，在4个香港女孩面前，暴跳也爆发出青春了吧。
哈哈。
大爷的。。。。坑爹的。
老板很有趣，老板是个有故事的人，有故事的人是幸福的。希望多
年以后我也会在一个陌生的地方，和一群陌生的人一起，述说着
我年轻的时候的故事，希望那个时候我应该是自豪的。仅此而已
虽然下了些雨，不过在一起的时刻，此生难忘。&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>今天答辩结束</title>
   <link href="http://baotiao.github.io//2011/06/ending-examination/"/>
   <updated>2011-06-28T15:03:35+08:00</updated>
   <id>http://baotiao.github.io//2011/06/ending-examination</id>
   <content type="html">&lt;p&gt;答辩还是挺水的.忽悠着,忽悠着,就过了.唉. &lt;a href=&quot;http://hollandonlinecasinos.nl/&quot;&gt;casino nederland&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>和俱乐部的散伙饭</title>
   <link href="http://baotiao.github.io//2011/06/goodbye-club/"/>
   <updated>2011-06-27T20:25:15+08:00</updated>
   <id>http://baotiao.github.io//2011/06/goodbye-club</id>
   <content type="html">&lt;p&gt;和 俱乐部吃饭了散伙饭。。
俱乐部 &lt;a href=&quot;http://premiumonlinecasino.de/&quot;&gt;online slot maschinen&lt;/a&gt; 的白姐 给教了很多很多。。&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>算第一个散伙饭吧,</title>
   <link href="http://baotiao.github.io//2011/06/goobye-dinner/"/>
   <updated>2011-06-24T00:25:24+08:00</updated>
   <id>http://baotiao.github.io//2011/06/goobye-dinner</id>
   <content type="html">&lt;p&gt;2011.6.24  安哥请大家吃饭,哇咔咔.本来可以去唱歌的,后来没去.
后来玩杀人还是比较爽的,把安安单挑死了,哇咔咔..
今天还顺便把成绩的事情给解决了,终于可以拿到毕业证了,为了这毕业证哥累死累活了啊.哎...
不过还好弄好了这事...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The first post</title>
   <link href="http://baotiao.github.io//2011/06/first-post/"/>
   <updated>2011-06-14T00:00:00+08:00</updated>
   <id>http://baotiao.github.io//2011/06/first-post</id>
   <content type="html">&lt;p&gt;今天照毕业照,不过昨晚刚通宵完.&lt;/p&gt;
</content>
 </entry>
 
 
</feed>
