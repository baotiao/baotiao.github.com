<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>xfs kmalloc failure problem &#8211; baotiao</title>
    <link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com">
    <link rel="dns-prefetch" href="//cdn.mathjax.org">
    <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="做有积累的事情">
    <meta name="robots" content="all">
    <meta name="author" content="baotiao">
    <meta name="keywords" content="">
    <link rel="canonical" href="http://baotiao.github.io//2016/05/27/xfs-kmalloc-failure/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for baotiao" href="/feed.xml" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?201701051307" type="text/css">

    <!-- Fonts -->
    
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
    

    <!-- MathJax -->
    

    <!-- Verifications -->
    
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="xfs kmalloc failure problem">
    <meta property="og:description" content="做有积累的事情">
    <meta property="og:url" content="http://baotiao.github.io//2016/05/27/xfs-kmalloc-failure/">
    <meta property="og:site_name" content="baotiao">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    
        <meta name="twitter:site" content="@baotiao" />
    
    <meta name="twitter:title" content="xfs kmalloc failure problem" />
    <meta name="twitter:description" content="做有积累的事情" />
    <meta name="twitter:url" content="http://baotiao.github.io//2016/05/27/xfs-kmalloc-failure/" />

    <!-- Icons -->
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/favicon-160x160.png" sizes="160x160">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">

    
</head>

<body class="site">
  
	

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="http://baotiao.github.io/" class="site-title">baotiao</a>
      <nav class="site-nav">
        
    

    
        <a href="/about/">About</a>
    

    

    

    

    

    

    

    

    

    

    

    

    

    


    

    

    

    

    
        <a href="/links/">Links</a>
    

    

    

    

    

    

    

    

    

    

    


    

    

    

    

    

    
        <a href="/paper/">Paper</a>
    

    

    

    

    

    

    

    

    

    


      </nav>
      <div class="clearfix"></div>
      
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        


<div class="post-header mb2">
  <h1>xfs kmalloc failure problem</h1>
  <span class="post-meta">May 27 2016</span><br>
  
  <span class="post-meta small">
  
    3 minute read
  
  </span>
</div>

<article class="post-content">
  <p>记录一次线上实体机的xfs kmem_alloc 操作一直失败排查</p>

<h3 id="section">问题现象</h3>

<p>线上有些实体机dmesg出现xfs 报错</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Apr 29 21:54:31 w-openstack86 kernel: XFS: possible memory allocation deadlock <span class="k">in </span>kmem_alloc <span class="o">(</span>mode:0x250<span class="o">)</span>
Apr 29 21:54:33 w-openstack86 kernel: XFS: possible memory allocation deadlock <span class="k">in </span>kmem_alloc <span class="o">(</span>mode:0x250<span class="o">)</span>
Apr 29 21:54:34 w-openstack86 kernel: INFO: task qemu-system-x86:6902 blocked <span class="k">for </span>more than 120 seconds.
Apr 29 21:54:34 w-openstack86 kernel: <span class="s2">"echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs"</span> disables this message.
Apr 29 21:54:34 w-openstack86 kernel: qemu-system-x86 D ffff88105065e800     0  6902      1 0x00000080
Apr 29 21:54:34 w-openstack86 kernel: ffff880155c63778 0000000000000086 ffff88099719d080 ffff880155c63fd8
Apr 29 21:54:34 w-openstack86 kernel: ffff880155c63fd8 ffff880155c63fd8 ffff88099719d080 ffff88099719d080
Apr 29 21:54:34 w-openstack86 kernel: ffff88081018de10 ffffffffffffffff ffff88081018de18 ffff88105065e800
Apr 29 21:54:34 w-openstack86 kernel: Call Trace:
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff8163a879&gt;] schedule+0x29/0x70
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff8163c235&gt;] rwsem_down_read_failed+0xf5/0x170
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffffa0256a30&gt;] ? xfs_ilock_data_map_shared+0x30/0x40 <span class="o">[</span>xfs]
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff81301764&gt;] call_rwsem_down_read_failed+0x14/0x30
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff81639a90&gt;] ? down_read+0x20/0x30
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffffa02569bc&gt;] xfs_ilock+0xdc/0x120 <span class="o">[</span>xfs]
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffffa0256a30&gt;] xfs_ilock_data_map_shared+0x30/0x40 <span class="o">[</span>xfs]
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffffa023f4f4&gt;] __xfs_get_blocks+0x94/0x4b0 <span class="o">[</span>xfs]
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff810654f2&gt;] ? get_user_pages_fast+0x122/0x1a0
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffffa023f944&gt;] xfs_get_blocks_direct+0x14/0x20 <span class="o">[</span>xfs]
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff8121d704&gt;] do_blockdev_direct_IO+0x13f4/0x2620
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffffa023f930&gt;] ? xfs_get_blocks+0x20/0x20 <span class="o">[</span>xfs]
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff8121e985&gt;] __blockdev_direct_IO+0x55/0x60
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffffa023f930&gt;] ? xfs_get_blocks+0x20/0x20 <span class="o">[</span>xfs]
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffffa023f210&gt;] ? xfs_finish_ioend_sync+0x30/0x30 <span class="o">[</span>xfs]
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffffa023e5ca&gt;] xfs_vm_direct_IO+0xda/0x180 <span class="o">[</span>xfs]
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffffa023f930&gt;] ? xfs_get_blocks+0x20/0x20 <span class="o">[</span>xfs]
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffffa023f210&gt;] ? xfs_finish_ioend_sync+0x30/0x30 <span class="o">[</span>xfs]
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff8116afed&gt;] generic_file_direct_write+0xcd/0x190
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffffa027f1fc&gt;] xfs_file_dio_aio_write+0x1f3/0x232 <span class="o">[</span>xfs]
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffffa024bd2d&gt;] xfs_file_aio_write+0x13d/0x150 <span class="o">[</span>xfs]
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff811ddde9&gt;] do_sync_readv_writev+0x79/0xd0
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff811df3be&gt;] do_readv_writev+0xce/0x260
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffffa024bbf0&gt;] ? xfs_file_buffered_aio_write+0x260/0x260 <span class="o">[</span>xfs]
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff811ddca0&gt;] ? do_sync_read+0xd0/0xd0
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff810e506e&gt;] ? do_futex+0xfe/0x5b0
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff811df5e5&gt;] vfs_writev+0x35/0x60
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff811df9e2&gt;] SyS_pwritev+0xc2/0xf0
Apr 29 21:54:34 w-openstack86 kernel: <span class="o">[</span>&lt;ffffffff816458c9&gt;] system_call_fastpath+0x16/0x1b
</code></pre>
</div>

<p>进而导致虚拟机因为文件系统的内存申请操作出现了问题, 导致这个虚拟机挂掉</p>

<p>解决办法:</p>

<p>sync &amp;&amp; echo 3 &gt; /proc/sys/vm/drop_caches</p>

<p>将page cache 里面的内容清空, 那么就不再报错. 但是为什么简单的清空page cache 就可以解决这个问题, 如果系统被page cache 占用着难道不应该申请内存操作的时候将一部分page cache 里面的内存刷回, 然后让出部分空闲空间么?</p>

<h3 id="section-1">问题分析</h3>

<p>看了一下代码, 出现这个报错在xfs module里面, 这个错误是在kmalloc 失败以后就会报出来, 并且会重试100次, 如果100 次以后还是失败, 就直接return error. 那么为什么kmalloc 会失败呢?</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kt">void</span> <span class="o">*</span>
<span class="nf">kmem_alloc</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">size</span><span class="p">,</span> <span class="n">xfs_km_flags_t</span> <span class="n">flags</span><span class="p">)</span>
<span class="p">{</span>
  <span class="kt">int</span> <span class="n">retries</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="n">gfp_t</span> <span class="n">lflags</span> <span class="o">=</span> <span class="n">kmem_flags_convert</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
  <span class="kt">void</span>  <span class="o">*</span><span class="n">ptr</span><span class="p">;</span>

  <span class="k">do</span> <span class="p">{</span>
    <span class="n">ptr</span> <span class="o">=</span> <span class="n">kmalloc</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">lflags</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">ptr</span> <span class="o">||</span> <span class="p">(</span><span class="n">flags</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">KM_MAYFAIL</span><span class="o">|</span><span class="n">KM_NOSLEEP</span><span class="p">)))</span>
      <span class="k">return</span> <span class="n">ptr</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="o">++</span><span class="n">retries</span> <span class="o">%</span> <span class="mi">100</span><span class="p">))</span>
      <span class="n">xfs_err</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span>
    <span class="s">"possible memory allocation deadlock in %s (mode:0x%x)"</span><span class="p">,</span>
          <span class="n">__func__</span><span class="p">,</span> <span class="n">lflags</span><span class="p">);</span>
    <span class="n">congestion_wait</span><span class="p">(</span><span class="n">BLK_RW_ASYNC</span><span class="p">,</span> <span class="n">HZ</span><span class="o">/</span><span class="mi">50</span><span class="p">);</span>
  <span class="p">}</span> <span class="k">while</span> <span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
</code></pre>
</div>

<p>这里首先我们必须知道如果一个操作kmalloc() 是向slab allocator 申请具体的小块的内存, 而slab allocator 是想buddy system 通过<code class="highlighter-rouge">__alloc_pages()</code> 去申请连续的内存. 那么肯定就是在<code class="highlighter-rouge">__alloc_pages()</code> 申请内存的时候失败了, 那为什么进行<code class="highlighter-rouge">__alloc_pages()</code> 操作的时候会失败呢? 即使实际物理内存里面还有page cache页以及swap 空间还没占满</p>

<p>从出现问题的机器上面我们可以看到, 机器的状态大概是这个样子</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="o">[</span>chenzongzhi@w-openstack86 ~]<span class="nv">$ </span>free -m
              total        used        free      shared  buff/cache   available
Mem:          64272       26298        4379         129       33595       37051
Swap:         32255           0       32255
</code></pre>
</div>

<p>这里我们同时看一下机器的内存碎片状态</p>

<div class="highlighter-rouge"><pre class="highlight"><code>这里并不是现场的机器, 只是另一台线上内存用的差不多的机器 
<span class="o">[</span>chenzongzhi@w-openstack81 ~]<span class="nv">$ </span>cat /proc/buddyinfo
Node 0, zone      DMA      0      0      0      1      2      1      1      0      1      1      3
Node 0, zone    DMA32   2983   2230   1037    290    121     63     47     61     16      0      0
Node 0, zone   Normal  13707   1126    285    268    291    160     64     21     11      0      0
Node 1, zone   Normal  10678   5041   1167    705    316    158     61     22      0      0      0
</code></pre>
</div>

<p>我们可以看到这里比较大块的连续的page 是基本没有的. 因为在xfs 的申请内存操作里面我们看到有这种连续的大块的内存申请的操作的请求,  比如:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>6000:   map <span class="o">=</span> kmem_alloc<span class="o">(</span>subnex <span class="k">*</span> sizeof<span class="o">(</span><span class="k">*</span>map<span class="o">)</span>, KM_MAYFAIL | KM_NOFS<span class="o">)</span>;
</code></pre>
</div>

<p>因此比较大的可能是线上虽然有少量的空闲内存, 但是这些内存非常的碎片, 因此只要有一个稍微大的的连续内存的请求都无法满足. 但是为什么不使用page cache呢?</p>

<p>到这里. 我们可以看到整个系统的内存基本全部被使用, 有少量的空闲. 但是这里面包含了大量的page cache. 理论上page cache 里面只会包含几M的需要刷回磁盘的内容, 大量的page cache 只是为了加快读, 里面的内容应该可以随时清空掉. 所以在kmalloc申请内存的时候应该能够把page cache 里面的内容清空, 然后给kernel 留出空闲的连续的大内存空间才是. 那么为什么这次申请内存操作<code class="highlighter-rouge">__alloc_pages()</code>的时候不把page cache 里面的内容清空一部分, 然后给这次<code class="highlighter-rouge">__alloc_pages()</code> 预留出来空间呢?</p>

<p>这里xfs 的申请内存操作因为是文件系统的申请内存操作, 所以一般带上GFP_NOFS 这个参数, 这个参数的意思是</p>

<p>GFP_NOFS</p>

<blockquote>
  <p>These flags function like GFP_KERNEL, but they add restrictions on what the kernel can do to satisfy the request. A GFP_NOFS allocation is not allowed to perform any filesystem calls, while GFP_NOIO disallows the initiation of any I/O at all. They are used primarily in the filesystem and virtual memory code where an allocation may be allowed to sleep, but recursive filesystem calls would be a bad idea.</p>
</blockquote>

<p>也就是说如果带上这个GFP_NOFS的flag, 那么本次 <code class="highlighter-rouge">__alloc_pages()</code> 是不允许有任何的filesystem calls的操作的, 那么如果物理内存不够了, 也就是不能触发这个page_reclaim 的操作. 具体的实现是在page reclaim 的 do_try_to_free_pages 里面shrink_page_list 的时候会判断这次的scan_control 里面有没有这个 __GFP_FS flag, 如果是没有GFP_FS flag, 就不会就行这个page_reclaim</p>

<p>为什么要这样, 因为如果在文件系统申请内存的时候, 你又触发了一次文件系统相关的操作, 比如把page cache 里面的内容刷会到文件, 那么刷会到文件这个操作必然又会有内存申请相关的操作, 这样就进入是循环了. kernel 为了避免这样的死循环尝试, 所以在文件系统相关的内存申请就不允许有任何filesystem calls. 也就是这个原因导致kernel 本身kmalloc 一直失败</p>

<p>那么接下来 sync &amp;&amp; echo 3 &gt; /proc/sys/vm/drop_caches 操作为什么能够成功, 并且后续就不会有报错了呢?</p>

<p>因为drop_caches 这个操作属于外部操作, 不属于文件系统本身的操作, 因此没有GFP_NOFS这个flag, 因此可以很轻松的就把page cache 里面的内容清空, 让Kernel 有足够多的连续的大内存. 线上自然就不报错了</p>


</article>






  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname  = 'baotiao';
    var disqus_identifier = '/2016/05/27/xfs-kmalloc-failure';
    var disqus_title      = 'xfs kmalloc failure problem';

    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>






      </div>
    </div>
  </div>

  <footer class="center">
</footer>


</body>
</html>
